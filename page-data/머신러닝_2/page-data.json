{"componentChunkName":"component---src-templates-post-tsx","path":"/머신러닝_2/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"머신러닝ml\" style=\"position:relative;\"><a href=\"#%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9Dml\" aria-label=\"머신러닝ml permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>머신러닝(ML)</h1>\n<ul>\n<li>K-Means 클러스터링</li>\n<li>H-clustering</li>\n<li>DBSAN</li>\n<li>앙상블</li>\n<li>\n<p>연관규칙 분석</p>\n<br>\n</li>\n</ul>\n<br>\n<h2 id=\"k-means-클러스터링\" style=\"position:relative;\"><a href=\"#k-means-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81\" aria-label=\"k means 클러스터링 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>k-means</strong> 클러스터링</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Grid search<span class=\"token punctuation\">:</span> \n    Init <span class=\"token operator\">=</span> ‘k<span class=\"token operator\">-</span>means<span class=\"token operator\">+</span><span class=\"token operator\">+</span>’\n    n_init<span class=\"token operator\">=</span><span class=\"token number\">3</span>\n    max_iter<span class=\"token operator\">=</span><span class=\"token number\">300</span>\n    tol<span class=\"token operator\">=</span>le<span class=\"token operator\">-</span><span class=\"token number\">04</span>\n    random_state<span class=\"token operator\">=</span><span class=\"token number\">0</span></code></pre></div>\n<br>\n<ul>\n<li>비지도학습</li>\n<li>비계층적 군집분석</li>\n<li>\n<p>k-means 클러스터링은 데이터를 k개의 클러스터(cluster, 무리)로 분류</p>\n<br>\n</li>\n<li><code class=\"language-text\">EM알고리즘</code>: 중점을 할당한 후, 각 중점까지의 거리의 합을 최소화하는 알고리즘</li>\n<li>\n<p>알고리즘(작동 원리):</p>\n<ol>\n<li>사용자로부터 입력받은 k의 값에 따라, 임의로 클러스터 중심(centroid) k개를 설정해준다.</li>\n<li>k개의 클러스터 중심으로부터 모든 데이터가 얼마나 떨어져 있는지 계산한 후에, 가장 가까운 클러스터 중심을 각 데이터의 클러스터로 정해준다. </li>\n<li>각 클러스터에 속하는 데이터들의 <strong>평균</strong>을 계산함으로 클러스터 중심을 <strong>옮겨준다</strong>. </li>\n<li>보정된 클러스터 중심을 기준으로 2, 3단계를 반복한다.</li>\n<li>더이상 클러스터 중심이 이동하지 않으면 알고리즘을 종료한다. </li>\n</ol>\n <br>\n</li>\n<li>new data 입력(발생) 시엔 각 중점과의 거리만 비교해서 가장 가까운 곳에 있는 군집에 속한다고 파악</li>\n<li>초기값에 따라 전역이 아닌 지역 최소 값을 찾을 수 있음</li>\n<li>r에 따라 {0,1} 이면 명목형, 확률이면 연속형(GMM 모델)</li>\n</ul>\n<br>\n<ul>\n<li>적합한 k 개수를 찾고 검증하는 모델들</li>\n</ul>\n<h3 id=\"k-means-elbow-method\" style=\"position:relative;\"><a href=\"#k-means-elbow-method\" aria-label=\"k means elbow method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>K-Means Elbow Method</h3>\n<ul>\n<li>“k는 얼마가 적합할까?” 적합한 k개수 찾는 성격인 듯하다.</li>\n<li>\n<p>Grid search: </p>\n<ol>\n<li>n_clusters 조절</li>\n<li>error: 군집 속 중점과의 거리의 합 &#x3C;- 이라고 개념을 설정해두고(왜냐면 k-means는 비지도학습이라 정답이 없어서 label이나 target, class 등이 없어서 확인 못함) 군집화가 잘 된 경우라면 error가 작을 것.</li>\n</ol>\n</li>\n<li>따라서 k가 증가할 때 줄어드는 ‘폭’이 작아지는 지점의 k값이 최적 군집 개수</li>\n<li>거리의 합 != 거리가 줄어드는 폭.</li>\n<li>엘보우는 거리가 줄어드는 폭을 봄</li>\n</ul>\n <br>\n<h3 id=\"실루엣silhouette\" style=\"position:relative;\"><a href=\"#%EC%8B%A4%EB%A3%A8%EC%97%A3silhouette\" aria-label=\"실루엣silhouette permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>실루엣(Silhouette)</h3>\n<ul>\n<li>“군집화가 잘 됐나?” &#x3C;- 약간 검증하는 성격인 듯하다.</li>\n<li>\n<p>잘 된 군집화: </p>\n<ol>\n<li>군집 간 거리(b) > 군집 내 거리(a) </li>\n<li>cohesion(응집도:군집 내) &#x3C; separation(분리도:군집 간) </li>\n</ol>\n</li>\n<li>실루엣 계수는 원형 군집이 아닌 경우 잘 맞지 않음</li>\n<li>0~1값을 가짐</li>\n</ul>\n <br>\n<h2 id=\"k-means군집clustering\" style=\"position:relative;\"><a href=\"#k-means%EA%B5%B0%EC%A7%91clustering\" aria-label=\"k means군집clustering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>K-Means++군집(clustering)</h2>\n<ul>\n<li>local optimum 해결 위해 초기 중점을 좀 더 합리적으로 설정하는 방법</li>\n</ul>\n <br>\n <br>\n<hr>\n<br>\n<br>\n<h2 id=\"h-clustering\" style=\"position:relative;\"><a href=\"#h-clustering\" aria-label=\"h clustering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>H-clustering</h2>\n<ul>\n<li>계층적 군집분석</li>\n<li>덴드로그램</li>\n<li>\n<p>k-menas 와의 차이점:</p>\n<ul>\n<li>k-means는 <strong>사전에 그룹수(k)</strong> 결정, </li>\n<li>H-clutering은 한 개의 그룹이 남을 때까지 <strong>그룹을 다 나눈 후</strong> 몇 개 선택할지 두 개의 feature를 갖는 2차원의 덴드로그램으로 결정</li>\n</ul>\n</li>\n</ul>\n<br>\n <br>\n<hr>\n<br>\n<br>\n<h2 id=\"dbscan\" style=\"position:relative;\"><a href=\"#dbscan\" aria-label=\"dbscan permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DBSCAN</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Grid search<span class=\"token punctuation\">:</span>\n    eps <span class=\"token operator\">=</span> <span class=\"token number\">0.2</span> <span class=\"token comment\"># 앱실론: 특정 ‘반경’</span>\n    min_samples<span class=\"token operator\">=</span><span class=\"token number\">5</span> <span class=\"token comment\"># 0.2 반경에 샘플 5개가 있어야 함</span>\n    metric <span class=\"token operator\">=</span> ‘euclidean’</code></pre></div>\n<br>\n<ul>\n<li>계층적 군집분석</li>\n<li>밀집도 기반의 군집 알고리즘: core point(핵심 샘플), border point(경계 샘플), noise point(잡음 샘플)</li>\n<li>noise point는 분류하지 않는다</li>\n<li>K-means or 다른 군집분석과의 차이점: 모든 샘플을 클러스터에 할당하지 않고 잡음 샘플을 구분하는 능력이 있다</li>\n</ul>\n<br>\n<br>\n<hr>\n<br>\n<br>\n<h2 id=\"앙상블-기법\" style=\"position:relative;\"><a href=\"#%EC%95%99%EC%83%81%EB%B8%94-%EA%B8%B0%EB%B2%95\" aria-label=\"앙상블 기법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>앙상블 기법</h2>\n<ul>\n<li>다수의 결과를 예측하여 종합하고 분류함</li>\n<li>여러 알고리즘을 사용한 후 결과를 종합하여 정확도, 일반화 특성을 증가시킴</li>\n<li>classification_report(~~)</li>\n</ul>\n<br>\n<h3 id=\"배깅bagging-or-bootstrap-aggregation\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85bagging-or-bootstrap-aggregation\" aria-label=\"배깅bagging or bootstrap aggregation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅(Bagging or Bootstrap Aggregation)</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">BaggingClassifier<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">*</span> Hyper parameter <span class=\"token punctuation\">:</span>\n    base_estimatior <span class=\"token operator\">=</span> m\n    n_estimators <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\n    bootstrap <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n<span class=\"token operator\">*</span> prob <span class=\"token operator\">+=</span> bag<span class=\"token punctuation\">.</span>predict_proba<span class=\"token punctuation\">(</span>testX<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">*</span> predY <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>prob<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># axis=1 하는 이유: 안 하면 하나의 값인 int가 나와서 밑에 testY랑 mean하려면 array 형태로 나와야 함</span>\n<span class=\"token operator\">*</span> accuracy <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>testY <span class=\"token operator\">==</span> predY<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<ul>\n<li>\n<p>BootStrap(단순복원 임의추출)을 통해 샘플 뽑아내 서브 데이터 만듦</p>\n<ul>\n<li>각각의 서브 데이터 크기 = 원본 훈련 데이터 크기 </li>\n<li>why? 데이터 중복 허용해서 분산(변동)이 감소하고 overfitting 방지</li>\n</ul>\n</li>\n</ul>\n <br>\n<h3 id=\"부스팅boosting-증폭-가속\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85boosting-%EC%A6%9D%ED%8F%AD-%EA%B0%80%EC%86%8D\" aria-label=\"부스팅boosting 증폭 가속 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅(Boosting): 증폭, 가속.</h3>\n<ul>\n<li>\n<p>잘못 분류된 데이터에 가중치 두어 다시 뽑고 다시 분류하는 알고리즘</p>\n<ol>\n<li>서브 훈련 샘플 만듦</li>\n<li>처음 샘플링은 가중치를 두어 샘플링함 -> 분류 잘못된 데이터는 가중치 높이고 다시 샘플링. 이때 잘못 분류된 패턴이 선택될 확률이 높음. </li>\n</ol>\n<p> → 분류가 어려운 패턴에 더욱 집중하여 정확도를 높이는 방법</p>\n <br>\n</li>\n</ul>\n<h4 id=\"adaboostadaptive-boosting\" style=\"position:relative;\"><a href=\"#adaboostadaptive-boosting\" aria-label=\"adaboostadaptive boosting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AdaBoost(Adaptive Boosting)</h4>\n<ul>\n<li><strong>약한 분류기</strong> 사용. 잘못 분류한 데이터 샘플에 가중치를 두어 더 많이 샘플링하여 정확도 높임</li>\n<li>\n<p>서브 데이터로만 만듦(잔차 등으로 만드는 거 아님)</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">AdaBoostClassifier<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\nHyper parameter<span class=\"token punctuation\">:</span>\n  base_estimator<span class=\"token operator\">=</span>svm\n  n_estimators<span class=\"token operator\">=</span><span class=\"token number\">100</span> <span class=\"token comment\">#100번 재조합한단 뜻(오분류한 거에 가중치둬서)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<h4 id=\"gradient-boostingfor-regression\" style=\"position:relative;\"><a href=\"#gradient-boostingfor-regression\" aria-label=\"gradient boostingfor regression permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gradient Boosting(for regression)</h4>\n<ul>\n<li>target 데이터의 <strong>잔차</strong>를 줄이도록 학습. 학습할수록 residual(잔차)가 계속 작아짐. 잔차가 더 이상 줄어들지 않을 떄까지 tree 생성하며 학습+추정치 업데이트</li>\n<li>residual 계산법 : 변수-(평균+학습률(알파. 0~1사이 값. 아무렇게나 줘도 됨. 보통 0.1)*tree의 leaf 평균</li>\n<li>선형회귀라 dataset도 연속형 변수인 boston 집값을 보도록 한다.</li>\n<li>선형회귀라 MSE 대신 r2 사용해도 OK</li>\n<li>MSE: 선형, 로지스틱 회귀 둘다 쓰여도 OK. 다만 선형에선 R2를, 로지스틱에선 BCE(바이너리일 떄) 더 잘 쓰임..(?)</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">GradientBoostingRegressor<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\nHyper parameter<span class=\"token punctuation\">:</span>\n    Loss <span class=\"token operator\">=</span> ‘ls’           \t<span class=\"token comment\"># lest square = MSE 사용</span>\n    Learning_rate <span class=\"token operator\">=</span> <span class=\"token number\">0.1</span>     <span class=\"token comment\"># 알파. 가중치</span>\n    n_estimators <span class=\"token operator\">=</span> <span class=\"token number\">100</span>      <span class=\"token comment\"># 잔차(tree) 100개 만들라</span>\n    max_depth<span class=\"token operator\">=</span><span class=\"token number\">3</span> \t\t\t<span class=\"token comment\"># 얕은 depth</span></code></pre></div>\n<br>\n<h4 id=\"gradient-boostingfor-classification\" style=\"position:relative;\"><a href=\"#gradient-boostingfor-classification\" aria-label=\"gradient boostingfor classification permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gradient Boosting(for classification)</h4>\n<ul>\n<li>Regression과 동일하나, 추정치를 위해 odds, logs(odds), probability 개념 사용</li>\n<li>binary cross entropy(BCE)를 loss함수로 사용</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">GradientBoostingClassifier<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\nHyper parameter<span class=\"token punctuation\">:</span>\n    loss <span class=\"token operator\">=</span> ‘devianve’<span class=\"token punctuation\">,</span>      <span class=\"token comment\">#로지스틱 함수 + CE 쓰라는 뜻</span>\n    learning_rate <span class=\"token operator\">=</span> <span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span>     <span class=\"token comment\">#학습률, 가중치, 알파값</span>\n    n_estimators<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">,</span>      <span class=\"token comment\">#잔차(tree) 수</span>\n    max_depth<span class=\"token operator\">=</span><span class=\"token number\">3</span> </code></pre></div>\n<br>\n<h4 id=\"xgboostextreme-gradient-boostingfor-regression\" style=\"position:relative;\"><a href=\"#xgboostextreme-gradient-boostingfor-regression\" aria-label=\"xgboostextreme gradient boostingfor regression permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>XGBoost(Extreme Gradient Boosting)(for regression)</h4>\n<ul>\n<li>\n<p>정규화와 가지치기를 통해 </p>\n<ol>\n<li>overfitting을 줄이고 </li>\n<li>일반화 특성을 좋게 만듦</li>\n<li>특히 대용량 data의 경우에 속도도 SOSO</li>\n</ol>\n</li>\n<li>Similarity, output 값 사용: Similarity를 사용해서 잔차와 IG 계산하고 마지막에 output 계산해서 마지막 잔차 계산</li>\n<li>데이터 大 ~ similarity(유사도) 小 why? 상쇄되는 값이 많아서.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">XGBRegressor<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\nHyper parameter<span class=\"token punctuation\">:</span> \n\tObjective <span class=\"token operator\">=</span>’reg<span class=\"token punctuation\">:</span>squarederror’     <span class=\"token comment\">#regression 사용하고 MSE 사용한단 뜻. regression이니 r2 사용</span></code></pre></div>\n<br>\n<h4 id=\"xgboostextreme-gradient-boostingfor-classification\" style=\"position:relative;\"><a href=\"#xgboostextreme-gradient-boostingfor-classification\" aria-label=\"xgboostextreme gradient boostingfor classification permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>XGBoost(Extreme Gradient Boosting)(for classification)</strong></h4>\n<ul>\n<li>잔차 계산 시, output value를 사용한다는 데서 Gradient Boost랑 차이가 있음</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Hyper parameter<span class=\"token punctuation\">:</span>\n<span class=\"token punctuation\">(</span>chapter <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> XGBClassifier<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">*</span> Objective <span class=\"token operator\">=</span>’binary<span class=\"token punctuation\">:</span>logistic’        <span class=\"token comment\">#바이너리 변수고 logistic함수(sigmoid) 사용</span>\n    \n<span class=\"token punctuation\">(</span>chapter <span class=\"token number\">2</span><span class=\"token punctuation\">)</span> XGBClassifier<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">*</span> Param – <span class=\"token punctuation\">{</span>‘eta’ <span class=\"token punctuation\">:</span> <span class=\"token number\">0.3</span><span class=\"token punctuation\">,</span> \n  ‘max_depth’ <span class=\"token punctuation\">:</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> \n  ‘objective’ <span class=\"token punctuation\">:</span> ‘multi<span class=\"token punctuation\">:</span>softprob’      <span class=\"token comment\">#softmas 사용한단 뜻</span>\n  ‘num_class’ <span class=\"token punctuation\">:</span> <span class=\"token number\">3</span> <span class=\"token punctuation\">}</span>            \t\t  <span class=\"token comment\">#클래스 개수</span></code></pre></div>\n<br>\n<h3 id=\"랜덤포레스트random-forest\" style=\"position:relative;\"><a href=\"#%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8random-forest\" aria-label=\"랜덤포레스트random forest permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>랜덤포레스트(Random Forest)</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">RandomForestClassifier<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\nHyper parameter<span class=\"token punctuation\">:</span>\n    max_depth<span class=\"token operator\">=</span><span class=\"token number\">5</span>\n    estimaors<span class=\"token operator\">=</span><span class=\"token number\">100</span></code></pre></div>\n<br>\n<ul>\n<li>\n<p>DT(Decision Tree)를 앙상블함 </p>\n<ul>\n<li>트리마다 서로 다른 feature 사용</li>\n</ul>\n</li>\n<li>샘플링 함</li>\n</ul>\n<br>\n<h3 id=\"다수결-알고리즘majority-voting\" style=\"position:relative;\"><a href=\"#%EB%8B%A4%EC%88%98%EA%B2%B0-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98majority-voting\" aria-label=\"다수결 알고리즘majority voting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>다수결 알고리즘(Majority Voting)</h3>\n<ul>\n<li>\n<p>배깅/부스팅과의 차이점: 학습데이터를 서브data에 sampling 하느냐/안 하느냐</p>\n<ul>\n<li>다수결 알고리즘은 배깅/부스팅처럼 서브데이터로 나눈 게 아니라 그 자체를 쓴다.</li>\n</ul>\n</li>\n</ul>\n<br>\n<h3 id=\"isolation-forestiforest\" style=\"position:relative;\"><a href=\"#isolation-forestiforest\" aria-label=\"isolation forestiforest permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Isolation Forest(iForest):</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Model <span class=\"token operator\">=</span> IsolationForest\nHyper parameter<span class=\"token punctuation\">:</span>\n    n_estimators <span class=\"token operator\">=</span> <span class=\"token number\">100</span>   <span class=\"token comment\">#100개의 트리</span></code></pre></div>\n<br>\n<ul>\n<li>이상데이터 검출하는 알고리즘 (ex: 카드 불법 사용에 사용)</li>\n<li>\n<p>Keyword: </p>\n<ul>\n<li>이진검색트리</li>\n<li>Anomaly score(이상치 수치)</li>\n<li>recall, precison 사용</li>\n<li>특히 recall 써서 실제 정상(T)인데 비정상(F)으로 예측했다던가, 실제 비정상(F)인데 정상(T)로 예측한 비율 찾아냄</li>\n</ul>\n</li>\n</ul>\n <br>\n <br>\n<hr>\n<br>\n<br>\n<h2 id=\"연관규칙-분석\" style=\"position:relative;\"><a href=\"#%EC%97%B0%EA%B4%80%EA%B7%9C%EC%B9%99-%EB%B6%84%EC%84%9D\" aria-label=\"연관규칙 분석 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>연관규칙 분석</h2>\n<ul>\n<li>\n<p>장바구니 분석. 고객들의 구매 성향을 분석할 수 있음</p>\n<ol>\n<li>지지도: y가 독립변수인지 종속변수인지 불명확함</li>\n<li>신뢰도: y에 대한 영향을 무시한단 단점이 있음</li>\n<li>향상도: 1에 가까우면 X와 Y는 서로 독립적. 1보다 크면 양의 상관성, 1보다 작으면 음의 상관성. 리프트가 1보다 클수록 X→Y 규칙의 의미가 커짐</li>\n</ol>\n</li>\n<li>\n<p>지지도/신뢰도/향상도 특징:</p>\n<ol>\n<li>인과관계가 아닌 상관관계</li>\n<li>얼마나 빈번하게 나타나는지 측정</li>\n</ol>\n</li>\n<li>이진행렬 구성</li>\n<li>지/신/향 모두 임계치 이상인 모든 규칙을 찾기엔 Brute Force 방식을 써서 조합이 너무 많아짐. 따라서 이 조합을 줄일 수 있는 알고리즘이 Apriori 알고리즘.</li>\n<li>항목을 줄이는 게 관건</li>\n<li>한 항목 집합이 반발하면, 그것의 모든 부분 집합이 반발한단 뜻에서 지지도 기반 가지치기</li>\n<li>연관성이 높다 = lift가 높다</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Hyper parameter<span class=\"token punctuation\">:</span>\n    Frequent_itemsets <span class=\"token operator\">=</span> apriori<span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">,</span> min_support <span class=\"token operator\">=</span> <span class=\"token number\">0.6</span><span class=\"token punctuation\">,</span> use_columname<span class=\"token operator\">==</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># Item sparse matrix생성</span>\n    rules <span class=\"token operator\">=</span> association_rules<span class=\"token punctuation\">(</span>frequent_itemsets<span class=\"token punctuation\">,</span> metric<span class=\"token operator\">=</span>”lift”<span class=\"token punctuation\">,</span> min_threshold<span class=\"token operator\">=</span><span class=\"token number\">0.7</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 모델 생성</span>\n    Rules <span class=\"token operator\">=</span> association_rules<span class=\"token punctuation\">(</span>by<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>‘lift<span class=\"token punctuation\">]</span>’<span class=\"token punctuation\">,</span> axis <span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> ascendin<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># Lift가 작은 것부터 sort</span></code></pre></div>\n<br>\n<br>\n<br>\n<br>\n<blockquote>\n<p>참고: 심교훈. 2019. 10. 9. “가장 간단한 군집 알고리즘, k-means 클러스터링\". <a href=\"https://bskyvision.com/564\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://bskyvision.com/564</a>. b스카이비전</p>\n</blockquote>","excerpt":"머신러닝(ML) K-Means 클러스터링 H-clustering DBSAN 앙상블 연관규칙 분석 k-means 클러스터링 비지도학습 비계층적 군집분석 k-means 클러스터링은 데이터를 k개의 클러스터(cluster…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9Dml\">머신러닝(ML)</a></p>\n<ul>\n<li>\n<p><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#k-means-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81\"><strong>k-means</strong> 클러스터링</a></p>\n<ul>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#k-means-elbow-method\">K-Means Elbow Method</a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#%EC%8B%A4%EB%A3%A8%EC%97%A3silhouette\">실루엣(Silhouette)</a></li>\n</ul>\n</li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#k-means%EA%B5%B0%EC%A7%91clustering\">K-Means++군집(clustering)</a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#h-clustering\">H-clustering</a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#dbscan\">DBSCAN</a></li>\n<li>\n<p><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#%EC%95%99%EC%83%81%EB%B8%94-%EA%B8%B0%EB%B2%95\">앙상블 기법</a></p>\n<ul>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#%EB%B0%B0%EA%B9%85bagging-or-bootstrap-aggregation\">배깅(Bagging or Bootstrap Aggregation)</a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#%EB%B6%80%EC%8A%A4%ED%8C%85boosting-%EC%A6%9D%ED%8F%AD-%EA%B0%80%EC%86%8D\">부스팅(Boosting): 증폭, 가속.</a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8random-forest\">랜덤포레스트(Random Forest)</a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#%EB%8B%A4%EC%88%98%EA%B2%B0-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98majority-voting\">다수결 알고리즘(Majority Voting)</a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#isolation-forestiforest\">Isolation Forest(iForest):</a></li>\n</ul>\n</li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_2/#%EC%97%B0%EA%B4%80%EA%B7%9C%EC%B9%99-%EB%B6%84%EC%84%9D\">연관규칙 분석</a></li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/머신러닝_2/"},"frontmatter":{"title":"머신러닝 분석 방법들, 두 번째","date":"Jun 30, 2020","tags":["ML","XGBoost"],"keywords":["JyneeEarth","jynee"],"update":"Aug 16, 2020"}}},"pageContext":{"slug":"/머신러닝_2/","series":[{"slug":"/머신러닝_1/","title":"머신러닝 분석 방법들, 첫 번째","num":1},{"slug":"/머신러닝_2/","title":"머신러닝 분석 방법들, 두 번째","num":2}],"lastmod":"2020-08-16"}},"staticQueryHashes":["3649515864","694178885"]}