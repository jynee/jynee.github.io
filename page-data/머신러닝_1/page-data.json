{"componentChunkName":"component---src-templates-post-tsx","path":"/머신러닝_1/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"머신러닝ml\" style=\"position:relative;\"><a href=\"#%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9Dml\" aria-label=\"머신러닝ml permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>머신러닝(ML)</h1>\n<ul>\n<li>KNN</li>\n<li>Decision Tree</li>\n<li>SVM</li>\n<li>선형회귀분석</li>\n<li>로지스틱회귀분석</li>\n<li>나이브베이지안</li>\n</ul>\n<br>\n<br>\n<h2 id=\"code-classlanguage-text차원의-저주code\" style=\"position:relative;\"><a href=\"#code-classlanguage-text%EC%B0%A8%EC%9B%90%EC%9D%98-%EC%A0%80%EC%A3%BCcode\" aria-label=\"code classlanguage text차원의 저주code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong><code class=\"language-text\">차원의 저주</code></strong></h2>\n<ul>\n<li>\n<p>차원(feature)이 증가하면 성능이 저하된다</p>\n<ul>\n<li>차원이 증가할수록 말 그대로 ‘근접 이웃’에 한정하기 어려워 멀리 떨어진 데이터를 참조한다</li>\n</ul>\n<p>→ 따라서 데이터 양에 비해 feature의 수가 많으면 차원의 저주 문제를 생각해봐야 한다.</p>\n</li>\n<li>\n<p>차원의 저주를 벗어날 수 있는 방법:</p>\n<ol>\n<li>차원 축소(feature 축소)</li>\n<li>데이터 양을 늘림</li>\n</ol>\n</li>\n<li>\n<p>차원의 저주를 벗어날 수 있는 모델</p>\n<ol>\n<li><code class=\"language-text\">중요도 분석</code>(의사결정나무 – 사전 가지치기)</li>\n<li>PCA 주성분 분석</li>\n</ol>\n</li>\n</ul>\n<br>\n<br>\n<h2 id=\"code-classlanguage-textknnk-nearest-neighborcode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textknnk-nearest-neighborcode\" aria-label=\"code classlanguage textknnk nearest neighborcode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong><code class=\"language-text\">kNN(k-Nearest Neighbor)</code></strong></h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Model <span class=\"token operator\">=</span> `KNeighborsClassifier<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>`\nGrid search<span class=\"token punctuation\">:</span>\n    n_neighbors<span class=\"token operator\">=</span><span class=\"token number\">5</span>\n    p<span class=\"token operator\">=</span><span class=\"token number\">2</span>\n    meric <span class=\"token operator\">=</span> ‘minkowski’</code></pre></div>\n<ul>\n<li>지도학습</li>\n<li>\n<p>테스트 데이터에서 k개의 가장 가까운 이웃을 찾고, 그 이웃들 중 다수가 속한 클래스가 테스트 데이터의 클래스가 되게 한다</p>\n<ul>\n<li>iris 패키지: “새로운 꽃이 발견됐을 때, 어느 종류(클래스)에 넣는 것이 좋을까?</li>\n</ul>\n</li>\n<li><code class=\"language-text\">레이지 러닝(lazy learning)</code>: 미리 학습해 두는 방식이 아니라, test data 추정할 때마다 학습하기 때문에 시간이 오래 걸림</li>\n</ul>\n<p> <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ff8f151393663889633470d0077aac0f/32056/image-20200721074726342.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.78378378378378%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAABaUlEQVQoz12RWW+cQBCE+f//ycpLFMkvibw2dizYBZZz1zBcM9zwpZfN3VJLoyl1VVeXZcqSqWn4u9Z1o7r9ty3LMPyDDeNIU1XMMrN2HUzTbWDHtm3D0qpAfX9jiCPmYSS4NJh+oiwVOolR9gtbU6PbjiTXdH1PqRT12yu15wnZwse14lq0O6ll4oypqFh9j9fDiW/HDxluiJOU/hyzXHOM5/P4FBBdG/IiJ49ThnPCJpte3IDPh4imNnfCMDkQ5geWWeMnNZUZdyAIXeL0WZTfGVuD7aRMy0ZnOvyzTRx9pdUJqux5d9Ob4TuhF9k8OQ+i7gjpitYtXdeTZCHB9Rnb+8Q4mZ+3XekFi7ITx/gR1/8C88wqd1yWZW+rl42MqHr+iaJQ8jbUdS3d0OmB7JJwOh2pxF4pQd26qTVG9+LCI4zErgSk5K6thGj9Si/LLqRpwv819AOu6+6Ef+puT6kSx3Ek6Ok38gNsshT7lN4ElQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200721074726342\"\n        title=\"image-20200721074726342\"\n        src=\"/static/ff8f151393663889633470d0077aac0f/fcda8/image-20200721074726342.png\"\n        srcset=\"/static/ff8f151393663889633470d0077aac0f/12f09/image-20200721074726342.png 148w,\n/static/ff8f151393663889633470d0077aac0f/e4a3f/image-20200721074726342.png 295w,\n/static/ff8f151393663889633470d0077aac0f/fcda8/image-20200721074726342.png 590w,\n/static/ff8f151393663889633470d0077aac0f/32056/image-20200721074726342.png 602w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<blockquote>\n<p>그림 출처: 심교훈. 2019. 10. 8. \"유유상종의 진리를 이용한 분류 모델, kNN(k-Nearest Neighbor)\". <a href=\"https://bskyvision.com/563\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://bskyvision.com/563</a>. b스카이비전</p>\n</blockquote>\n<ul>\n<li>\n<p>K 가 작을수록 복잡도 높아짐</p>\n<p>→ 과잉적합</p>\n</li>\n<li>K가 증가할수록 정확도가 떨어진다</li>\n<li>\n<p>거리 측정 방법:</p>\n<ol>\n<li>맨하튼 거리: 가장 심플</li>\n<li>유클리디안 거리: 최단 거리</li>\n<li>민코우스키 거리: 많이 떨어진 성분 부각</li>\n</ol>\n</li>\n</ul>\n<br>\n<br>\n<h2 id=\"code-classlanguage-text결정-트리decision-treecode\" style=\"position:relative;\"><a href=\"#code-classlanguage-text%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%ACdecision-treecode\" aria-label=\"code classlanguage text결정 트리decision treecode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong><code class=\"language-text\">결정 트리(Decision Tree)</code></strong></h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Model <span class=\"token operator\">=</span> DecisionTreeClassifier<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">-</span> grid search<span class=\"token punctuation\">:</span> \n    criterion<span class=\"token operator\">=</span>’gini’\n    알파<span class=\"token punctuation\">(</span>가중치<span class=\"token punctuation\">)</span>\n    max_depth<span class=\"token operator\">=</span>k\n    pd<span class=\"token punctuation\">.</span>Categorical<span class=\"token punctuation\">(</span>income<span class=\"token punctuation\">[</span>c<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>codes\n    dt<span class=\"token punctuation\">.</span>feature_importances ← 중요도 분석\n    path <span class=\"token operator\">=</span> `DecisionTreeClassifier<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>cost_complexity_pruning_path<span class=\"token punctuation\">(</span>trainX<span class=\"token punctuation\">,</span> trainY<span class=\"token punctuation\">)</span>\n    ccp_alpha <span class=\"token operator\">=</span> ccp_alpha\n    <span class=\"token operator\">*</span> 이때 먼저<span class=\"token punctuation\">,</span> ccp_alpha <span class=\"token operator\">=</span> `ccp_alphas<span class=\"token punctuation\">[</span>np<span class=\"token punctuation\">.</span>where<span class=\"token punctuation\">(</span>ccp_alphas <span class=\"token operator\">></span> <span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>`\n    clfs<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>tree_<span class=\"token punctuation\">.</span>node_count <span class=\"token operator\">/</span> clfs<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>tree_<span class=\"token punctuation\">.</span>max_depth</code></pre></div>\n<br>\n<ul>\n<li>지도학습, 비지도학습</li>\n<li>알파값(가중치 조정) 높이기 ~ 오분류율(불순도) 증가</li>\n<li>\n<p>핵심: 변별력이 좋은 질문을 위에서부터 하나하나 세팅</p>\n<br>\n</li>\n<li>Feature가 3개 이상이면 초평면으로 구분</li>\n<li>Featrue(차원)이 많아져도 덜 중요한 feature는 분류 기준에서 제외되어 feature 선정에 크게 신경 쓸 필요 X</li>\n<li>\n<p><strong>중요한 Feature 확인 가능</strong></p>\n<ul>\n<li>중요도 분석</li>\n</ul>\n</li>\n<li>\n<p>Tree 과도하게 분할( = 과잉 적합 = 트리가 복잡) → 아래 노드에 데이터 小 → 데이터 단편화 → 유의미한 결정 내리기 어려움</p>\n<ul>\n<li>즉, Depth 높 ~ 트리 길어짐 ~ 과도하게 분할 ~ overfitting ~ 정확도 낮음</li>\n<li>해결 위해 정지기준 or 사전/사후 가지치기 사용</li>\n<li>\n<p><code class=\"language-text\">정지기준</code>: depth 지정 or 마지막 노드의 데이터 수가 임계치 이하로 떨어지지 않도록 지정</p>\n<ul>\n<li>알파(가중치): depth 조정</li>\n</ul>\n</li>\n<li>\n<p><code class=\"language-text\">가지치기</code>: 트리 단순화하여 일반화 특성 향상시키기.</p>\n<ol>\n<li><code class=\"language-text\">사전 가지치기</code>: depth, 마지막 노드의 최소 데이터 수, 불순척도(criterion)</li>\n<li><code class=\"language-text\">사후 가지치기</code>: 오분류율, 패널티항, 알파(가중치)</li>\n</ol>\n <br>\n</li>\n</ul>\n</li>\n<li><code class=\"language-text\">ID3 알고리즘</code>: 정보량과 엔트로피 개념 활용</li>\n<li>\n<p>알고리즘 <strong>순서</strong>: </p>\n<ol>\n<li>엔트로피 계산량에 의해 엔트로피(E)가 낮고, </li>\n<li>정보획득량(IG)가 높은 선택지를 선택함</li>\n<li>온도 속성은 끝내 결정 노드에 쓰이지 않았다. 4개의 속성 중에 가장 변별력이 낮은 속성이었던 것이다.</li>\n</ol>\n</li>\n<li>불순척도(지니지수, 엔트로피) 작아지도록 분할 기준 선택. 분할 전 부모노드 보다 분할 후 자식노드의 불순척도가 작아지는 게 좋음(IG)</li>\n<li><code class=\"language-text\">지니지수</code>: 0~0.5값</li>\n<li><strong><code class=\"language-text\">정보량</code></strong>: 어떤 사건이 가지고 있는 정보의 양. 드물게 발생하는 일일수록 정보량이 크다.</li>\n<li>\n<p><strong><code class=\"language-text\">엔트로피</code></strong>(E): 0~1값 정보량의 기댓값(평균). 발생한 사건들의 정보량을 모두 구해서 (가중)평균</p>\n<ul>\n<li>엔트로피가 크다는 것은 평균정보량이 크다는 것</li>\n<li>대개 사건들이 일어날 확률이 비슷한 경우에 엔트로피가 크다.</li>\n<li>따라서 두 사건이 0.5, 0.5 확률로 일어날 때의 엔트로피가 가장 크다</li>\n</ul>\n<p>→ 즉, <strong>불확실성이</strong> 클수록 엔트로피가 크다</p>\n<p>→ 불확실성이 크면 클수록 분류하기는 어려워짐</p>\n<p>→ 엔트로피가 가장 작은 것을 상위 의사결정 노드에 위치시켜야 함.</p>\n<p>→ 이를 위해 <strong><code class=\"language-text\">정보획득량</code></strong>(IG)이란 개념이 필요</p>\n<p>\"어떤 속성을 가지고 분류했을 때 가장 엔트로피(불확실성)가 작은지, 정보획득량이 큰지\"</p>\n</li>\n</ul>\n<br>\n<br>\n<h2 id=\"code-classlanguage-text서포트-벡터-머신svmcode\" style=\"position:relative;\"><a href=\"#code-classlanguage-text%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0svmcode\" aria-label=\"code classlanguage text서포트 벡터 머신svmcode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong><code class=\"language-text\">서포트 벡터 머신(SVM)</code></strong></h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Model <span class=\"token operator\">=</span> SVC<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\ngrid search <span class=\"token punctuation\">:</span> \n    kernel <span class=\"token operator\">=</span> ‘linear’\n    C\n    gamma\n    <span class=\"token punctuation\">(</span>비선형<span class=\"token operator\">+</span>multiclassification<span class=\"token punctuation\">)</span> kernel <span class=\"token operator\">=</span> `‘rbf’\n     <span class=\"token operator\">*</span> 파라미터 C는 이상치 또는 오류를 얼마나 허용하는 가<span class=\"token punctuation\">(</span>복잡도 조절<span class=\"token punctuation\">)</span>를 정해주고<span class=\"token punctuation\">,</span> gamma는 결정 경계의 곡률을 결정\n        <span class=\"token operator\">*</span> C<span class=\"token punctuation\">:</span> <span class=\"token builtin\">max</span> margin <span class=\"token operator\">+</span> <span class=\"token builtin\">min</span> 섞임 조절\n        <span class=\"token operator\">*</span> C<span class=\"token punctuation\">:</span> 大 <span class=\"token operator\">~</span> 패널티항 大 <span class=\"token operator\">~</span> 거리 짧음</code></pre></div>\n</br>\n<ul>\n<li>가장 많이 사용되는 SVM은 radial(방사형) basis function (RBF) 커널을 사용한 SVM</li>\n<li>SVM은 일반화 특성이 우수</li>\n<li>\n<blockquote>\n<p> 모델 생성시 특정 옵션을 주어야 <code class=\"language-text\">predict_proba</code>가 사용 가능</p>\n<p>predict_proba 함수를 가지고 있지만, 모델 생성시 <code class=\"language-text\">probalility=True</code>를 설정하지 않으면 사용할 수 없다.</p>\n<p>이때, 레이블이 3개 이상인 경우, LinearSVC보단 SVC를 사용하고, probability=True 옵션 주는 것을 추천한다. </p>\n<p>출처: <a href=\"https://m.blog.naver.com/cjh226/221358912619\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://m.blog.naver.com/cjh226/221358912619</a>)</p>\n</blockquote>\n</li>\n</ul>\n<br>\n<h3 id=\"선형-svm\" style=\"position:relative;\"><a href=\"#%EC%84%A0%ED%98%95-svm\" aria-label=\"선형 svm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>선형 SVM</strong></h3>\n<ul>\n<li>\n<p>데이터를 선형으로 분리하는 최적의 선형 결정 경계를 찾는 알고리즘</p>\n<ul>\n<li>그 중 가장 간단한 것이 선형 SVM(linear SVM)</li>\n</ul>\n</li>\n<li>\n<p>SVM 알고리즘의 목표: 클래스가 다른 데이터들을 가장 큰 마진(margin)으로 분리해내는 선 또는 면(결정 경계 또는 분리 초평면)을 찾아내는 것</p>\n<ul>\n<li>마진: 두 데이터 군과 결정 경계와 떨어져있는 정도</li>\n<li>서포트 벡터: 결정 경계와 가장 먼저 만나는 데이터</li>\n</ul>\n</li>\n<li>SVM의 기본 매개변수인 C</li>\n<li>\n<p>cost(C): C는 얼마나 많은 데이터 샘플이 다른 클래스에 놓이는 것을 허용하는지를 결정</p>\n<ul>\n<li>ex: 작을 수록 많이 허용, 클 수록 적게 허용</li>\n<li>ex: C값을 낮게 설정하면 이상치들이 있을 가능성을 크게 잡아 일반적인 결정 경계를 찾아내고, 높게 설정하면 반대로 이상치의 존재 가능성을 작게 봐서 좀 더 세심하게 결정 경계를 찾아낸다.</li>\n<li>ex: \"난 데이터 샘플하나도 잘못 분류할 수 없어!\" : C를 높여야</li>\n<li>ex: \"몇 개는 놓쳐도 괜찮아, 이상치들이 꽤 있을 수도 있으니까\" : C를 낮춰야</li>\n</ul>\n</li>\n<li>C가 너무 낮으면 과소적합(underfitting)</li>\n<li>\n<p>C가 너무 높으면 과대적합(overfitting)</p>\n<p>→ 적합한 C값을 찾아내는 것이 중요</p>\n</li>\n<li>하드마진(hard-margin) SVM</li>\n<li>소프트마진(soft-margin) SVM</li>\n</ul>\n </br>\n<h3 id=\"rbf-커널-svm\" style=\"position:relative;\"><a href=\"#rbf-%EC%BB%A4%EB%84%90-svm\" aria-label=\"rbf 커널 svm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>RBF 커널 SVM</strong></h3>\n<ul>\n<li>커널 기법은 주어진 데이터를 고차원 특징 공간으로 사상해주는 것이다.</li>\n<li>3차원 공간에서 분류된 것을 다시 2차원 공간으로 매핑해서 보면 결정 경계가 둥그렇게 보일 것</li>\n<li>\n<p>RBF 커널의 경우 gamma라는 매개변수를 사용자가 조정해야 한다.</p>\n<ul>\n<li><code class=\"language-text\">gamma</code> : 하나의 데이터 샘플이 영향력을 행사하는 거리를 결정</li>\n<li>ex: gamma가 클수록 한 데이터 포인터들이 영향력을 행사하는 거리가 짧아지는 반면, 낮을수록 커진다</li>\n<li>\n<p>ex: gamma는 가우시안 함수의 표준편차와 관련되어 있는데, 클수록 작은 표준편차를 의미</p>\n<ul>\n<li>'편차가 크다': 어떤 자료는 평균보다 엄청 크고 어떤 자료는 평균보다 엄청 작다</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>gamma 매개변수는 결정 경계의 곡률을 조정한다고 말할 수도 있다.</p>\n<ul>\n<li>gamma의 값이 높아짐에 따라 공간이 점점 작아지는데, 위에서 언급한 것과 같이 각각의 데이터 포인터가 영향력을 행사하는 거리가 짧아졌기 때문 &#x3C;- 아마 정확도가 높아질 듯하다.</li>\n</ul>\n</li>\n<li>\n<p>매개변수 C와 마찬가지로 너무 낮으면 과소적합될 가능성이 크고, 너무 높으면 과대적합의 위험이 있다.</p>\n<ul>\n<li>두 값 모두 커질수록 알고리즘의 복잡도는 증가하고, 작아질수록 복잡도는 낮아진다.</li>\n</ul>\n<br>\n</li>\n</ul>\n<br>\n<h2 id=\"선형-회귀linear-regression\" style=\"position:relative;\"><a href=\"#%EC%84%A0%ED%98%95-%ED%9A%8C%EA%B7%80linear-regression\" aria-label=\"선형 회귀linear regression permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>선형 회귀(linear regression)</strong></h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Model <span class=\"token operator\">=</span> LinearRegression<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>예측값(Y햇)이 실수(연속형)일 떈, ‘.mean’, ‘.score’ 말고 R2(*0~1값을 가짐) 사용(값의 범위가 MSE보다 작기 때문)</li>\n<li>\n<p>선형회귀는 사용되는 '특성(feature)의 개수'에 따라 </p>\n<ol>\n<li>단순 선형 회귀(simple linear regression): 단 하나의 특징(feature)을 가지고 라벨값(label) 또는 타깃(target)을 예측하기 위한 회귀 모델을 찾는다.</li>\n<li>다중 선형 회귀(multiple linear regression): 하나의 특성이 아닌 여러 개의 특성을 활용해서 회귀모델을 만듦</li>\n</ol>\n</li>\n<li><strong>선형 회귀는 y와 y햇 사이의 평균제곱오차(mean squared error, MSE)를 최소화하는 파라미터(w, b)</strong>를 찾는다. y와 y햇의 차이가 작으면 작을 수록 예측 성능이 좋기 때문</li>\n<li><strong>라쏘(Lasso):</strong> 선형 회귀의 단점을 극복하기 위해 개발된 방법</li>\n<li>linear regression에선 predict_proba 함수를 제공하지 않는다</li>\n<li>\n<p>y햇 = w[0]+x[0]+b</p>\n<ul>\n<li>w: 가중치(weight), 계수(coefficient)</li>\n<li>b: 편항(offset)</li>\n<li>y햇: 예측값</li>\n<li>x[0]: 특징</li>\n<li>\"feature와 lable 사이의 관계를 잘 설명해낼 수 있는 최적(가장 적합한)의 w와 b를 찾는 것\"</li>\n</ul>\n</li>\n</ul>\n<br>\n<h3 id=\"code-classlanguage-text라쏘l1code\" style=\"position:relative;\"><a href=\"#code-classlanguage-text%EB%9D%BC%EC%8F%98l1code\" aria-label=\"code classlanguage text라쏘l1code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong><code class=\"language-text\">라쏘(L1)</code></strong></h3>\n<ul>\n<li>추가 제약조건이자 grid search</li>\n<li>선형 회귀에 <strong>L1</strong> 규제를 줘서 과대적합을 피하는 방법이다.</li>\n<li>상관성이 있을 수도 있는 feature의 영향력을 줄일 수 있음</li>\n<li>\n<p>동작 원리:</p>\n<ul>\n<li>MSE가 최소가 되게 하는 w, b 찾기 + w의 모든 원소가 0이 되거나 0에 가깝게</li>\n<li>MSE와 penalty 항의 합이 최소가 되게 하는 w와 b를 찾는 것이 라쏘의 목적</li>\n</ul>\n</li>\n<li>\n<p>L1-norm(벡터의 요소들의 절대값들의 합) 패널티를 조정하는 건 알파값(선형회귀 분석의 grid serch)</p>\n<ul>\n<li><strong>알파 너무 작으면 과대적합(복잡도 큼), 너무 크면 과소 적합(복잡도 넘 작음)</strong></li>\n</ul>\n</li>\n<li>\n<p>라쏘의 장점:</p>\n<ol>\n<li>제약 조건을 통해 일반화된 모형을 찾는다.</li>\n<li>모델 해석력이 good (모델에서 가장 중요한 특성이 무엇인지 아는)</li>\n</ol>\n</li>\n</ul>\n<blockquote>\n<p>총 105개의 특성을 라쏘 회귀 모델을 만들기 위해 사용했다. <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 36px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/de1e4e5f483c13ccd70931eaeed07ced/b501b/image-20200721081736446.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAABKklEQVQoz4VSu6qDUBDMN/oDqfIDaUQwKVPHTiwEg11Io0hSBFJaWIgkihGSVisfnfiae/eAYmJy78B6luXM7O4cZ/hF13Xoz/9ijKZpMOYTZu9ihLZtJ+SxqOu6WK/XeDwew/1B8L3DOO+FKXpSkiSwbRuLxQJBEEwFe4GqqnA6nXA4HHA+n7Hf72GaJjabDbbbLTuv1+tAXK1W8H1/stGsV9c0DcfjEVmWYT6fw3EcVq/rmhHIr/5uWZbgeR63220qSJ88z7FcLlnBMAxIksTyy+UCWZax2+2gKAqiKBomFEXxu4fUURAEWJYFXddZTpPGcczWDMOQTUM1Ak3PcRxUVUWapi/eDx6S2ff7neUkUBQFPoFWfz6f8DzvpcnLyn+98jjef6VPnB+lqpvwRtKYcAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200721081736446\"\n        title=\"image-20200721081736446\"\n        src=\"/static/de1e4e5f483c13ccd70931eaeed07ced/b501b/image-20200721081736446.png\"\n        srcset=\"/static/de1e4e5f483c13ccd70931eaeed07ced/b501b/image-20200721081736446.png 36w\"\n        sizes=\"(max-width: 36px) 100vw, 36px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>로 설정했더니 105개의 가중치 중에서 101개가 0이 되면서 특성은 단 4개만 사용되었다. 훈련셋에서의 점수와 테스트셋에서의 점수를 보니 과소적합이었다. 따라서 복잡도를 높이기 위해서 <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 73px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/342cd0d147b9f829aaa817345be1c855/2bc29/image-20200721081835369.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 23.287671232876715%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsSAAALEgHS3X78AAAA9klEQVQY021QyYqDUBDM/3+AJyF3QTEuwagHb+KCBESIghAQRC+K4oKg1tAdJsxh6vDorep11+U8T/yHqqowzzPH0zRxTmiaBn3fc/x+v3EcB9Z1/dYu9AzDgHEcUdc1N16vF0zThG3bPOS6Lu73O3zf59hxHARBAF3XEccx0jSFJEkfQUoMw4BlWYiiCEmSMJnqmqZh2zaoqoo8zyEIAp7PJzzPw/V6RVEUPLssC/NZkDaRZZkFaCDLMiYrioIwDFGWJZMejwdvfrvdeDv6mHjUp9NFUfwIkjqdSz78BZ1O/pB/tCXZ8msP5YSu674et22Lfd/xAzbWZmyVUan7AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200721081835369\"\n        title=\"image-20200721081835369\"\n        src=\"/static/342cd0d147b9f829aaa817345be1c855/2bc29/image-20200721081835369.png\"\n        srcset=\"/static/342cd0d147b9f829aaa817345be1c855/2bc29/image-20200721081835369.png 73w\"\n        sizes=\"(max-width: 73px) 100vw, 73px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>로 설정했더니 가중치 중에서 7개만 0이 되면서 94개의 특성이 사용되었다. 훈련셋과 테스트셋에서의 점수를 보니 훈련셋에서는 좋은데 테스트셋에서는 많이 떨어졌다. 즉, 과대적합이었다. 따라서 다시 복잡도를 낮추기 위해 <img src=\"https://t1.daumcdn.net/cfile/tistory/99E9B0335A0547A912\" alt=\"img\">을 사용했다. 105개의 가중치 중에서 72개가 0이 되면서 33개의 특성이 사용되었다. 훈련셋에서의 점수와 테스트셋에서의 점수가 모두 괜찮았다. </p>\n</blockquote>\n<br>\n<h3 id=\"code-classlanguage-text릿지l2code\" style=\"position:relative;\"><a href=\"#code-classlanguage-text%EB%A6%BF%EC%A7%80l2code\" aria-label=\"code classlanguage text릿지l2code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">릿지(L2)</code></h3>\n<ul>\n<li><code class=\"language-text\">L2-norm</code></li>\n<li>릿지 원의 크기와 라쏘의 마름모 크기는 정규화 역할하는 람다 혹은 C로 조절</li>\n<li>특성이 다수일 경우에는 릿지 회귀가 좀 더 잘 작동. 선형 회귀와 달리 모델의 복잡도를 조정할 수 있기 때문. 복잡도를 조정할 수 있다는 말은 사용자가 설정 가능한 파라미터가 있다는 뜻</li>\n<li><strong>알파값(가중치)</strong>: (feature가 생각보다 덜 쓰였다던가의)과소적합: 1보다 작은 α 값(ex: 0.1, 0.01, 0.001)로 조정</li>\n<li>\n<p>즉, alpha 값을 크게 설정 ~ 기울기가 줄어듦 ~ 특성들이 출력에 미치는 영향력이 줄어듦(현재 특성들에 덜 의존)</p>\n<ul>\n<li>alpha = 0 ← 선형회귀</li>\n<li>default : alpha = 1</li>\n</ul>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>정리</p>\n<ul>\n<li>특성이 많은데 그중 <strong>일부분만 중요하다면 라쏘</strong></li>\n<li>특성의 중요도가 전체적으로 <strong>비슷하다면 릿지</strong></li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<h2 id=\"code-classlanguage-text로지스틱-회귀-분석code\" style=\"position:relative;\"><a href=\"#code-classlanguage-text%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80-%EB%B6%84%EC%84%9Dcode\" aria-label=\"code classlanguage text로지스틱 회귀 분석code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">로지스틱 회귀 분석</code></h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> Logisticregression<span class=\"token punctuation\">(</span><span class=\"token operator\">~</span><span class=\"token operator\">~</span><span class=\"token punctuation\">)</span>\ngrid search<span class=\"token punctuation\">:</span>\n    penaly <span class=\"token operator\">=</span> ‘l2’\n    C <span class=\"token operator\">=</span> c\n    max_iter <span class=\"token operator\">=</span> <span class=\"token number\">500</span></code></pre></div>\n<br>\n<ul>\n<li>선형회귀분석에서 쓰던 MSE 말고 Cross Entropy(CE) 사용</li>\n<li><strong>오즈</strong>(0.9가 나왔으면 1이라 보는 것)</li>\n<li>\n<p>계산 시 유의 사항:</p>\n<ul>\n<li>예측값(y햇)이 실제값(y)과에 가깝게 나오게 하기 위해 CE, MSE 최소화</li>\n<li>출력에 1이 여러 개인(sigmoid 출력) 이진분류일 경우 <strong>BCE</strong>(바이너리 CE) 사용: 개별 출력이라 1이 여러 개</li>\n<li>출력에 1이 한 개일 경우(one-shot 형태로 softmax가 출력) <strong>CCE</strong>(카테고리컬 CE) 사용: 그룹(?) 출력이라 1이 하나</li>\n</ul>\n<p>→ BCE, CCE는 결과가 다르고 정확도 측정도 달라지므로 주의해서 선택</p>\n<br>\n</li>\n<li>\n<p>sklearn 패키지에서는 sigmoid 안 거치고 바로 softmax로 감</p>\n<br>\n</li>\n<li>\n<p>보통의 계산 순서: </p>\n<ol>\n<li>sigmoid 함수로 계산</li>\n<li>softmax함수로 계산(이때 CE 사용)</li>\n</ol>\n</li>\n</ul>\n <br>\n<ul>\n<li>\n<p>로지스틱함수의 yHat 계산법(step):</p>\n<ol>\n<li>시그모이드 계산</li>\n<li>시그모이드(sigmoid) 함수: 가중치와 바이어스는 시그모이드의 비활성도를 조절해준다.</li>\n<li>CE 계산</li>\n</ol>\n</li>\n</ul>\n<br>\n <br>\n<h2 id=\"code-classlanguage-text나이브-베이지안code\" style=\"position:relative;\"><a href=\"#code-classlanguage-text%EB%82%98%EC%9D%B4%EB%B8%8C-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88code\" aria-label=\"code classlanguage text나이브 베이지안code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">나이브 베이지안</code></h2>\n<ul>\n<li>model = GaussianNB()</li>\n<li><strong>Feature들이 서로 독립이라 가정</strong>하고 <strong>조건부 확률 계산</strong>해서 데이터 분류</li>\n<li>\n<p><strong>명목형</strong>(1, 0), <strong>연속형</strong>(정규분포 사용) 변수 모두 사용 가능</p>\n<br>\n</li>\n<li>\n<p>계산 순서:</p>\n<ol>\n<li>결합 확률은 너무 복잡해서 두 feature를 독립이라 가정하여,</li>\n<li>베이지안 식에 의해 각각을 곱해 계산한 후,</li>\n<li>큰 확률의 값을 선택</li>\n</ol>\n<p> Ex: yes 일 확률 : 1 , no일 확률: 0 => (계산 후) no로 분류</p>\n <br>\n</li>\n<li><code class=\"language-text\">m 추정치(m-Estimates)</code>: 비교하려는 두 샘플의 data 중에 특정 데이터가 없을 땐 두 확률이 모두 0으로 나와서 분류할 수 없으므로, m-Estimates라는 m과 p를 사용해서 조건부 확률 계산식을 조정함. </li>\n<li>분모의 m은 임의의 확률, mp는 분자가 0 나오는 거 방지</li>\n<li>\n<p>명목형, 연속형 모두 섞여 있는 data 일 땐,</p>\n<ol>\n<li>명목형, 연속형 각각 model 학습</li>\n<li>각각의 model 정확도 추정</li>\n<li>'2'의 그 정확도(확률)을 곱함</li>\n<li>'3'의 확률의 곱으로 정확도 측정</li>\n</ol>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<br>\n<br>\n<blockquote>\n<p> 참고: </p>\n<ul>\n<li>심교훈. 019. 10. 24. \"결정 트리(Decision Tree) 알고리즘, ID3 소개\". <a href=\"https://bskyvision.com/598\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://bskyvision.com/598</a>. b스카이비전.</li>\n<li>심교훈. 2020. 1. 20. \"[ubuntu+python] 선형 회귀의 업그레이드 버전1, 릿지 회귀\". <a href=\"https://bskyvision.com/687\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://bskyvision.com/687</a>. b스카이비전</li>\n<li>심교훈. 2017. 10. 21. \"서포트 벡터 머신(SVM)의 사용자로서 꼭 알아야할 것들 - 매개변수 C와 gamma\". <a href=\"https://bskyvision.com/163\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://bskyvision.com/163</a>. b스카이비전.</li>\n<li>머신러닝. 2020.01.14. \"서포트 벡터 머신(Support Vector Machine) 쉽게 이해하기\". <a href=\"http://hleecaster.com/ml-svm-concept/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">http://hleecaster.com/ml-svm-concept/</a>. 아무튼워라밸</li>\n</ul>\n</blockquote>","excerpt":"머신러닝(ML) KNN Decision Tree SVM 선형회귀분석 로지스틱회귀분석 나이브베이지안  차원(feature…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9Dml\">머신러닝(ML)</a></p>\n<ul>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#code-classlanguage-text%EC%B0%A8%EC%9B%90%EC%9D%98-%EC%A0%80%EC%A3%BCcode\"><strong><code class=\"language-text\">차원의 저주</code></strong></a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#code-classlanguage-textknnk-nearest-neighborcode\"><strong><code class=\"language-text\">kNN(k-Nearest Neighbor)</code></strong></a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#code-classlanguage-text%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%ACdecision-treecode\"><strong><code class=\"language-text\">결정 트리(Decision Tree)</code></strong></a></li>\n<li>\n<p><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#code-classlanguage-text%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0svmcode\"><strong><code class=\"language-text\">서포트 벡터 머신(SVM)</code></strong></a></p>\n<ul>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#%EC%84%A0%ED%98%95-svm\"><strong>선형 SVM</strong></a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#rbf-%EC%BB%A4%EB%84%90-svm\"><strong>RBF 커널 SVM</strong></a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#%EC%84%A0%ED%98%95-%ED%9A%8C%EA%B7%80linear-regression\"><strong>선형 회귀(linear regression)</strong></a></p>\n<ul>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#code-classlanguage-text%EB%9D%BC%EC%8F%98l1code\"><strong><code class=\"language-text\">라쏘(L1)</code></strong></a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#code-classlanguage-text%EB%A6%BF%EC%A7%80l2code\"><code class=\"language-text\">릿지(L2)</code></a></li>\n</ul>\n</li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#code-classlanguage-text%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80-%EB%B6%84%EC%84%9Dcode\"><code class=\"language-text\">로지스틱 회귀 분석</code></a></li>\n<li><a href=\"/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_1/#code-classlanguage-text%EB%82%98%EC%9D%B4%EB%B8%8C-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88code\"><code class=\"language-text\">나이브 베이지안</code></a></li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/머신러닝_1/"},"frontmatter":{"title":"머신러닝 분석 방법들, 첫 번째","date":"Jun 23, 2020","tags":["ML"],"keywords":["JyneeEarth","jynee"],"update":"Aug 16, 2020"}}},"pageContext":{"slug":"/머신러닝_1/","series":[{"slug":"/머신러닝_1/","title":"머신러닝 분석 방법들, 첫 번째","num":1},{"slug":"/머신러닝_2/","title":"머신러닝 분석 방법들, 두 번째","num":2}],"lastmod":"2020-08-16"}},"staticQueryHashes":["3649515864","694178885"]}