{"componentChunkName":"component---src-templates-post-tsx","path":"/NLP한글_4/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"nlp\" style=\"position:relative;\"><a href=\"#nlp\" aria-label=\"nlp permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NLP</h1>\n<ul>\n<li>Quora  : 질문 간 텍스트 유사도 분석</li>\n<li>maLSTM : 맨하탄 거리 사용한 LSTM</li>\n<li>GloVe : 빈도 + 맥락(Embedding) 고려한 워드 패키지</li>\n<li>FastText : hash 값을 사용해 어딘가에 저장해두므로, 사전에 없는 단어를 입력해도 비슷한 걸 찾아 vector 값으로 나온다.</li>\n</ul>\n<br>\n<br>\n<h2 id=\"glove--malstm\" style=\"position:relative;\"><a href=\"#glove--malstm\" aria-label=\"glove  malstm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GloVe &#x26; maLSTM</h2>\n<ul>\n<li>maLSTM = 맨하탄 거리 사용한 LSTM</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Question-1, 2 입력용</span>\nK<span class=\"token punctuation\">.</span>clear_session<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ninputQ1 <span class=\"token operator\">=</span> Input<span class=\"token punctuation\">(</span>batch_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> trainQ1<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\ninputQ2 <span class=\"token operator\">=</span> Input<span class=\"token punctuation\">(</span>batch_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> trainQ2<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Question-1 처리용 LSTM</span>\nembQ1 <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>VOCAB_SIZE<span class=\"token punctuation\">,</span> output_dim<span class=\"token operator\">=</span>EMB_SIZE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>inputQ1<span class=\"token punctuation\">)</span>\nembQ1 <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span>rate<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>embQ1<span class=\"token punctuation\">)</span>\nlstmQ1 <span class=\"token operator\">=</span> LSTM<span class=\"token punctuation\">(</span>HIDDEN_DIM<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>embQ1<span class=\"token punctuation\">)</span>\nlstmQ1 <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span>FEATURE_DIM<span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> kernel_regularizer<span class=\"token operator\">=</span>regularizers<span class=\"token punctuation\">.</span>l2<span class=\"token punctuation\">(</span>REGULARIZER<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>lstmQ1<span class=\"token punctuation\">)</span>\nlstmQ1 <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span>rate<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>lstmQ1<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Question-2 처리용 LSTM</span>\nembQ2 <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>VOCAB_SIZE<span class=\"token punctuation\">,</span> output_dim<span class=\"token operator\">=</span>EMB_SIZE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>inputQ2<span class=\"token punctuation\">)</span>\nembQ2 <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span>rate<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>embQ2<span class=\"token punctuation\">)</span>\nlstmQ2 <span class=\"token operator\">=</span> LSTM<span class=\"token punctuation\">(</span>HIDDEN_DIM<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>embQ2<span class=\"token punctuation\">)</span>\nlstmQ2 <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span>FEATURE_DIM<span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> kernel_regularizer<span class=\"token operator\">=</span>regularizers<span class=\"token punctuation\">.</span>l2<span class=\"token punctuation\">(</span>REGULARIZER<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>lstmQ2<span class=\"token punctuation\">)</span>\nlstmQ2 <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span>rate<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>lstmQ2<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>위 코드처럼 Embedding을 각각 해줄 땐, </p>\n<ol>\n<li>한글 사전 / 영어 사전 속 단어의 Vector 값을 각각 확인하는 것 처럼 사전의 근본이 다를 때</li>\n<li>\n<p>챗봇 Q&#x26;A : Q는 형태소 분석을 하고, A는 형태소 분석을 하지 않을 때 처럼 결과값이 달라야 할 때.</p>\n<p>\"달라야 할 필요가 있을 때만\" 이렇게 사용한다.</p>\n</li>\n</ol>\n</blockquote>\n<br>\n<ul>\n<li>\n<p>각각 embedding 값에서 맨하탄 거리 측정</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token comment\"># Question-1, 2의 출력으로 맨하탄 거리를 측정한다.</span>\n<span class=\"token comment\"># lstmQ1 = lstmQ2 --> mDist = 1</span>\n<span class=\"token comment\"># lstmQ1 - lstmQ2 = inf --> mDist = 0</span>\n<span class=\"token comment\"># mDist = 0 ~ 1 사잇값이므로, trainY = [0, 1]과 mse를 측정할 수 있다.</span>\nmDist <span class=\"token operator\">=</span> K<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span>K<span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span>K<span class=\"token punctuation\">.</span><span class=\"token builtin\">abs</span><span class=\"token punctuation\">(</span>lstmQ1 <span class=\"token operator\">-</span> lstmQ2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> keepdims<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<ul>\n<li>ex:\na=np.array(np.arange(20).reshape(4,5)\nnp.sum(a,axis=0, keepdims=true)\n*keepdims=true쓰면 원래 1차원으로 나와야 할 것이 원래 a의 차원대로 2차원으로 나옴</li>\n<li>Backend as k:네트워크 출력 결과에 어떤 연산을 취할 때\nkeepdims=true:원래 a값 구조대로 줌</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>compile</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">model <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>inputQ1<span class=\"token punctuation\">,</span> inputQ2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> mDist<span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>loss<span class=\"token operator\">=</span><span class=\"token string\">'mean_squared_error'</span><span class=\"token punctuation\">,</span> optimizer<span class=\"token operator\">=</span>Adam<span class=\"token punctuation\">(</span>lr<span class=\"token operator\">=</span><span class=\"token number\">0.0005</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<p><br><br></p>\n<h2 id=\"glove--malstm--quora\" style=\"position:relative;\"><a href=\"#glove--malstm--quora\" aria-label=\"glove  malstm  quora permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GloVe &#x26; maLSTM &#x26; Quora</h2>\n<ul>\n<li>동시발생 확률 고려<br></li>\n<li>빈도 기반 문장 , 맥락(context) 고려<br></li>\n<li>\n<p>순서</p>\n<ol>\n<li>전처리가 완료된 학습 데이터를 읽어온다.</li>\n<li>Quora 데이터의 Vocabulary를 읽어온다.</li>\n<li>maLSTM 모델을 빌드한다.</li>\n<li>저장된 WE를 읽어온다.</li>\n<li>\n<p>Pre-trained GloVe 파일을 읽어와서 GloVe dictionary를 생성한다</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">    GloVe <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span> <span class=\"token comment\">#Vocabulary</span>\n    <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> <span class=\"token builtin\">file</span><span class=\"token punctuation\">:</span> <span class=\"token comment\"># 1라인씩 읽음. # line=['love','~','~'..]. vector가 300개 들어있다.</span>\n        wv <span class=\"token operator\">=</span> line<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        word <span class=\"token operator\">=</span> <span class=\"token string\">''</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>wv<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token operator\">-</span><span class=\"token number\">300</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 앞부분 # word=wv[0] 워드만.</span>\n        GloVe<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>asarray<span class=\"token punctuation\">(</span>wv<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">300</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 뒷부분 vector만.</span>\n    <span class=\"token builtin\">file</span><span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 뒷부분 300개 </span></code></pre></div>\n<blockquote>\n<p>GloVe['love']을  실행하면 아래 vector가 출력된다. shape = (300,)\narray([ 1.3949e-01,  5.3453e-01, -2.5247e-01, -1.2565e-01,  4.8748e-02,\n1.5244e-01,  1.9906e-01, -6.5970e-02,  1.2883e-01,  2.0559e+00, ...</p>\n</blockquote>\n<br>\n</li>\n<li>\n<p>WE = np.zeros((VOCAB<em>SIZE, EMB</em>SIZE))</p>\n<br>\n</li>\n<li>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">    <span class=\"token keyword\">for</span> word<span class=\"token punctuation\">,</span> i <span class=\"token keyword\">in</span> word2idx<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token comment\">#여기서 word2idx는 quora 용으로 우리가 만든 사전 </span>\n      vec <span class=\"token operator\">=</span> GloVe<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span> <span class=\"token comment\">#300개짜리 vector # GloVe.get(word) : 있으면 나오고 없으면 null 값이 나와서 프로그램이 멈추지 않음(get)을 쓰면. # glove[word]라고만 쓸 땐, 있으면 나오고 없으면 error 뜸 </span>\n        <span class=\"token keyword\">if</span> vec <span class=\"token keyword\">is</span> <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n          WE<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> vec</code></pre></div>\n<br>\n</li>\n<li>\n<p>결과를 저장한다.</p>\n<br>\n</li>\n<li>학습 데이터와 시험 데이터로 나눈다.<br></li>\n<li>Question-1, 2 입력용(input)<br></li>\n<li>공통으로 사용(범용)할 Embedding layer 빌드<br></li>\n<li>Question-1 처리용 LSTM model 빌드<br></li>\n<li>Question-2 처리용 LSTM model 빌드<br></li>\n<li>Question-1, 2의 출력으로 맨하탄 거리를 측정한다.</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">  mDist <span class=\"token operator\">=</span> K<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span>K<span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span>K<span class=\"token punctuation\">.</span><span class=\"token builtin\">abs</span><span class=\"token punctuation\">(</span>lstmQ1 <span class=\"token operator\">-</span> lstmQ2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> keepdims<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>Question-1, 2의 출력으로 맨하탄 거리를 측정한다.\nlstmQ1 = lstmQ2 --> mDist = 1\nlstmQ1 - lstmQ2 = inf --> mDist = 0\nmDist = 0 ~ 1 사잇값이므로, trainY = [0, 1]과 mse를 측정할 수 있다.</p>\n</blockquote>\n<br>\n<ol start=\"11\">\n<li>학습</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>inputQ1<span class=\"token punctuation\">,</span> inputQ2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> mDist<span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>loss<span class=\"token operator\">=</span><span class=\"token string\">'mean_squared_error'</span><span class=\"token punctuation\">,</span> optimizer<span class=\"token operator\">=</span>Adam<span class=\"token punctuation\">(</span>lr<span class=\"token operator\">=</span><span class=\"token number\">0.0005</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n  <br>\n<ol start=\"12\">\n<li>예측</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">trainY <span class=\"token operator\">=</span> trainY<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\ntestY <span class=\"token operator\">=</span> testY<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nhist <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>trainQ1<span class=\"token punctuation\">,</span> trainQ2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> trainY<span class=\"token punctuation\">,</span>\n               validation_data <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>testQ1<span class=\"token punctuation\">,</span> testQ2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> testY<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                 batch_size <span class=\"token operator\">=</span> <span class=\"token number\">1000</span><span class=\"token punctuation\">,</span> epochs <span class=\"token operator\">=</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span></code></pre></div>\n  <br>\n<ol start=\"13\">\n<li>시험 데이터로 학습 성능을 평가</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">predicted <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>testQ1<span class=\"token punctuation\">,</span> testQ2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\npredY <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>where<span class=\"token punctuation\">(</span>predicted <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\naccuracy <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>testY <span class=\"token operator\">==</span> predY<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nAccuracy = %.2f %s\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>accuracy <span class=\"token operator\">*</span> <span class=\"token number\">100</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'%'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n  <br>\n</li>\n</ul>\n<br>\n<hr>\n<p><br><br></p>\n<h2 id=\"fasttext--malstm--quora\" style=\"position:relative;\"><a href=\"#fasttext--malstm--quora\" aria-label=\"fasttext  malstm  quora permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>FastText &#x26; maLSTM &#x26; Quora</h2>\n<ul>\n<li>\n<p>subword 덕분에 어떤 단어라도 vector 형태로 만들어준다</p>\n<blockquote>\n<p>model.wv.vocab.keys()</p>\n</blockquote>\n<br>\n</li>\n<li>\n<p>한글 hash도 있다</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> gensim<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> fasttext</code></pre></div>\n</li>\n<li>\n<p>순서</p>\n<ol>\n<li>전처리가 완료된 학습 데이터를 읽어온다.<br></li>\n<li>Quora 데이터의 Vocabulary를 읽어온다.<br></li>\n<li>maLSTM 모델을 빌드한다.</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 이때, </span>\nEMB_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">300</span>\n<span class=\"token comment\"># pre-trained FastText의 vector size = 300이므로, 이와 동일하게 맞춘다.</span></code></pre></div>\n<ol>\n<li>저장된 WE를 읽어온다</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">savedWeightEmbedding <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span></code></pre></div>\n<ol start=\"2\">\n<li>Pre-trained FastText 파일을 읽어와서 dictionary를 생성한다.</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> fasttext<span class=\"token punctuation\">.</span>load_facebook_vectors<span class=\"token punctuation\">(</span><span class=\"token string\">'./dataset/wiki.en.bin'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ol start=\"3\">\n<li>빈(zero) 가중치 파일 생성</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">WE <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>VOCAB_SIZE<span class=\"token punctuation\">,</span> EMB_SIZE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>\n<p>Embedding의 w 값을 뽑아낼 수 있게 되었다. </p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">for</span> word<span class=\"token punctuation\">,</span> i <span class=\"token keyword\">in</span> word2idx<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\nWE<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>wv<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span></code></pre></div>\n</li>\n<li>결과 저장 <br></li>\n<li>학습 데이터와 시험 데이터로 나눈다.<br></li>\n<li>Question-1, 2 입력용(input)<br></li>\n<li>공통으로 사용할 Embedding layer 빌드<br></li>\n<li>Question-1 처리용 LSTM model 빌드<br></li>\n<li>Question-2 처리용 LSTM model 빌드<br></li>\n<li>Question-1, 2의 출력으로 맨하탄 거리를 측정</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">mDist <span class=\"token operator\">=</span> K<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span>K<span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span>K<span class=\"token punctuation\">.</span><span class=\"token builtin\">abs</span><span class=\"token punctuation\">(</span>lstmQ1 <span class=\"token operator\">-</span> lstmQ2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> keepdims<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>lstmQ1 = lstmQ2 --> mDist = 1\nlstmQ1 - lstmQ2 = inf --> mDist = 0\nmDist = 0 ~ 1 사잇값이므로, trainY = [0, 1]과 mse를 측정할 수 있다.</p>\n</blockquote>\n <br>\n<ol start=\"10\">\n<li>학습   </li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>inputQ1<span class=\"token punctuation\">,</span> inputQ2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> mDist<span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>loss<span class=\"token operator\">=</span><span class=\"token string\">'mean_squared_error'</span><span class=\"token punctuation\">,</span> optimizer<span class=\"token operator\">=</span>Adam<span class=\"token punctuation\">(</span>lr<span class=\"token operator\">=</span><span class=\"token number\">0.0005</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n  <br>\n<ol start=\"11\">\n<li>\n<p>예측</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">trainY <span class=\"token operator\">=</span> trainY<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\ntestY <span class=\"token operator\">=</span> testY<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nhist <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>trainQ1<span class=\"token punctuation\">,</span> trainQ2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> trainY<span class=\"token punctuation\">,</span> validation_data <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>testQ1<span class=\"token punctuation\">,</span> testQ2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> testY<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> batch_size <span class=\"token operator\">=</span> <span class=\"token number\">1000</span><span class=\"token punctuation\">,</span> epochs <span class=\"token operator\">=</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n</li>\n<li>\n<p>시험 데이터로 학습 성능을 평가</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">predicted <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>testQ1<span class=\"token punctuation\">,</span> testQ2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\npredY <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>where<span class=\"token punctuation\">(</span>predicted <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\naccuracy <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>testY <span class=\"token operator\">==</span> predY<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nAccuracy = %.2f %s\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>accuracy <span class=\"token operator\">*</span> <span class=\"token number\">100</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'%'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ol>\n</li>\n</ul>\n<br>\n <br>\n<h3 id=\"fasttext-연습\" style=\"position:relative;\"><a href=\"#fasttext-%EC%97%B0%EC%8A%B5\" aria-label=\"fasttext 연습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>FastText 연습</h3>\n<ul>\n<li>\n<p>패키지</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> gensim<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> FastText\n<span class=\"token keyword\">from</span> gensim<span class=\"token punctuation\">.</span>test<span class=\"token punctuation\">.</span>utils <span class=\"token keyword\">import</span> common_texts <span class=\"token comment\"># 9개 문장</span></code></pre></div>\n<blockquote>\n<p>[['human', 'interface', 'computer'],\n['survey', 'user', 'computer', 'system', 'response', 'time'],\n['eps', 'user', 'interface', 'system'],\n['system', 'human', 'system', 'eps'],\n['user', 'response', 'time'],\n['trees'],\n['graph', 'trees'],\n['graph', 'minors', 'trees'],\n['graph', 'minors', 'survey']]</p>\n</blockquote>\n<br>\n</li>\n<li>\n<p>모델 빌드</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> FastText<span class=\"token punctuation\">(</span>size<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> window<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> min_count<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> \nmodel<span class=\"token punctuation\">.</span>build_vocab<span class=\"token punctuation\">(</span>sentences<span class=\"token operator\">=</span>common_texts<span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span>sentences<span class=\"token operator\">=</span>common_texts<span class=\"token punctuation\">,</span> total_examples<span class=\"token operator\">=</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>common_texts<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>bucket=hash table. 따라서 100이라 주면, model.wv.vectors<em>ngrams.shape가 100, n이 됨. hashing trick을 통과시키고 나오는 값들이 들어가는 곳으로, bucket 값이 크면 collision을 방지한다\nsize = EMB</em>SIZE\nwindow = 좌우 context 개수\nmin_count = 빈도 1개 이상(그니까 전부 다)</p>\n</blockquote>\n<br>\n</li>\n<li>\n<p>word2vec 확인</p>\n<ul>\n<li>어떤 단어라도 Hashing되어 vector값을 준다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model<span class=\"token punctuation\">.</span>wv<span class=\"token punctuation\">[</span><span class=\"token string\">'human'</span><span class=\"token punctuation\">]</span>\nmodel<span class=\"token punctuation\">.</span>wv<span class=\"token punctuation\">[</span><span class=\"token string\">'klakasdfsdjf'</span><span class=\"token punctuation\">]</span></code></pre></div>\n<blockquote>\n<p>model.wv['human']\nOut[16]:\narray([-0.00893843, -0.03338013, -0.0210454 ,  0.03005639, -0.03349178], dtype=float32)</p>\n<p>model.wv['klakasdfsdjf']\nOut[17]:\narray([ 0.00078089, -0.01366953, -0.00929362,  0.00640602, -0.00282957], dtype=float32)</p>\n</blockquote>\n<br>\n</li>\n<li>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model<span class=\"token punctuation\">.</span>wv<span class=\"token punctuation\">.</span>vocab\nmodel<span class=\"token punctuation\">.</span>wv<span class=\"token punctuation\">.</span>vocab<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n<li>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model<span class=\"token punctuation\">.</span>wv<span class=\"token punctuation\">.</span>vectors_ngrams<span class=\"token punctuation\">.</span>shape</code></pre></div>\n<blockquote>\n<p>(2000000, 5)</p>\n</blockquote>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<br>\n<ul>\n<li>\n<p>참고: </p>\n<blockquote>\n<ul>\n<li>아마추어 퀀트, blog.naver.com/chunjein</li>\n<li>전창욱, 최태균, 조중현. 2019.02.15. 텐서플로와 머신러닝으로 시작하는 자연어 처리 - 로지스틱 회귀부터 트랜스포머 챗봇까지. 위키북스</li>\n</ul>\n</blockquote>\n</li>\n</ul>","excerpt":"NLP Quora  : 질문 간 텍스트 유사도 분석 maLSTM : 맨하탄 거리 사용한 LSTM GloVe : 빈도 + 맥락(Embedding) 고려한 워드 패키지 FastText : hash…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/NLP%ED%95%9C%EA%B8%80_4/#nlp\">NLP</a></p>\n<ul>\n<li><a href=\"/NLP%ED%95%9C%EA%B8%80_4/#glove--malstm\">GloVe &#x26; maLSTM</a></li>\n<li><a href=\"/NLP%ED%95%9C%EA%B8%80_4/#glove--malstm--quora\">GloVe &#x26; maLSTM &#x26; Quora</a></li>\n<li>\n<p><a href=\"/NLP%ED%95%9C%EA%B8%80_4/#fasttext--malstm--quora\">FastText &#x26; maLSTM &#x26; Quora</a></p>\n<ul>\n<li><a href=\"/NLP%ED%95%9C%EA%B8%80_4/#fasttext-%EC%97%B0%EC%8A%B5\">FastText 연습</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/NLP한글_4/"},"frontmatter":{"title":"NLP maLSTM","date":"Aug 07, 2020","tags":["NLP","maLSTM","Quora"],"keywords":["JyneeEarth","jynee"],"update":"Aug 19, 2020"}}},"pageContext":{"slug":"/NLP한글_4/","series":[{"slug":"/NLP한글_1/","title":"NLP 카운트 기반 방법의 텍스트 유사도 측정","num":1},{"slug":"/NLP한글_2/","title":"NLP Doc2Vec","num":2},{"slug":"/NLP한글_3/","title":"NLP Kaggle competition 우승자가 제안한 새로운 접근방법을 배워보자","num":3},{"slug":"/NLP한글_4/","title":"NLP maLSTM","num":4}],"lastmod":"2020-08-19"}},"staticQueryHashes":["3649515864","694178885"]}