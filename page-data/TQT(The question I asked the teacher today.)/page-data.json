{"componentChunkName":"component---src-templates-post-tsx","path":"/TQT(The question I asked the teacher today.)/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"wweights\" style=\"position:relative;\"><a href=\"#wweights\" aria-label=\"wweights permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>W(weights)</h1>\n<ul>\n<li>네트워크 및 model build까지 완성해서 실행되어 역전파 되었을 때 형성된다.</li>\n<li><code class=\"language-text\">compile에서 w</code>: 네트워크 만들고 난 후 model build하는 과정. optimizer &#x26; loss 값을 정의해주는 부분임. w는 만들어져있지 않다.</li>\n<li><code class=\"language-text\">fit에서 w</code>: fit은 train data 사용. A 다음 B가 <strong>나온다고 저장함</strong></li>\n<li><code class=\"language-text\">predict에서 w</code>: predict(예측)는 test data 사용. 예측 모델 기준으로 A를 넣으면 B가 <strong>나오게 하는</strong> 어떤 것(Thing)</li>\n</ul>\n<br>\n<br>\n<br>\n<h1 id=\"hyper-parameter\" style=\"position:relative;\"><a href=\"#hyper-parameter\" aria-label=\"hyper parameter permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Hyper parameter</h1>\n<ul>\n<li><code class=\"language-text\">weight decay</code>(annealing): epoch(alpha)를 처음에는 적당히 높게 했다가, 점차 줄여 나가는 방법</li>\n</ul>\n<br>\n<br>\n<br>\n<h1 id=\"dense\" style=\"position:relative;\"><a href=\"#dense\" aria-label=\"dense permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Dense</h1>\n<ul>\n<li><code class=\"language-text\">Dense</code>: fully connected</li>\n<li>\n<p><code class=\"language-text\">ANN(FNN)</code>에서는 여러 <code class=\"language-text\">Dense</code>를 써도 되지만, <code class=\"language-text\">RNN(LSTM)</code>에선 마지막 층에서만 <code class=\"language-text\">Dense</code>를 써야함.</p>\n<ul>\n<li><code class=\"language-text\">CNN</code>에서는 여러 <code class=\"language-text\">Dense</code> 써도 될까?</li>\n<li>=> 일단... <code class=\"language-text\">lstm</code>에서 <code class=\"language-text\">lstm() → Dense → lstm()</code>은 <code class=\"language-text\">lstm 네트워크가 2개</code> 만들어진다고 보면 된다. <code class=\"language-text\">lstm() → Dense</code> 했을 때, <code class=\"language-text\">1개의 네트워크</code>가 형성된 것</li>\n<li>=> 그리고 <code class=\"language-text\">CNN</code>은 일종의 잘 짜여진 레시피라서 <code class=\"language-text\">con1D → pooling → Dense → con1D → pooling</code>은 위 <code class=\"language-text\">lstm</code>처럼 좀 이상한 네트워크 구조가 되는 거라 생각함...</li>\n</ul>\n</li>\n<li>Dense(1, activation='sigmoid')</li>\n<li><strong>LSTM에서 FNN으로 보내는 마지막 Dense에선 relu 쓰면 안됨</strong></li>\n</ul>\n<br>\n<br>\n<br>\n<h1 id=\"fnn순방향-신경망\" style=\"position:relative;\"><a href=\"#fnn%EC%88%9C%EB%B0%A9%ED%96%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D\" aria-label=\"fnn순방향 신경망 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>FNN(순방향 신경망)</h1>\n<ul>\n<li>↔ RNN</li>\n<li>\n<p>hidden 층에서</p>\n<ul>\n<li>Dense(4, <code class=\"language-text\">activation</code> = 'sigmoid', <code class=\"language-text\">kernel_regularizer</code>=regularizers.l2(0.0001), activation='relu')</li>\n<li><code class=\"language-text\">Dropout</code>(rate=0.5)</li>\n</ul>\n<br>\n</li>\n<li><code class=\"language-text\">BatchNormalization</code>(momentum=0.9, epsilon=0.005, center=True, scale=True, moving<em>variance</em>initializer='ones')</li>\n</ul>\n<br>\n<ul>\n<li>\n<p><code class=\"language-text\">predict</code>까지 끝낸 <strong>연속형</strong> <code class=\"language-text\">yHat</code> 값을, <code class=\"language-text\">np.where</code> 써줘서 <strong>바이너리 형태</strong>로 변환 </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">np<span class=\"token punctuation\">.</span>where<span class=\"token punctuation\">(</span>yHat <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 딥러닝_파일: 4-4.ANN(Credit_Keras)_직접 해보기_커스텀loss.py</span></code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p><code class=\"language-text\">history</code> 활용</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">hist<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span>\nhist<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># 딥러닝_파일: 4-4.ANN(Credit_Keras)_직접 해보기.py</span></code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>학습/평가/예측용 model로 나누었을 때 <strong>평가 데이터 활용</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>trainX<span class=\"token punctuation\">,</span> trainY<span class=\"token punctuation\">,</span> validation_data<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>evlX<span class=\"token punctuation\">,</span> evlY<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">200</span><span class=\"token punctuation\">,</span> batch_size<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<h1 id=\"lstm\" style=\"position:relative;\"><a href=\"#lstm\" aria-label=\"lstm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LSTM</h1>\n<ul>\n<li>long term(장기기억, 전체적 흐름), short term(단기기억, 최근의 흐름)</li>\n<li>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>설명</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2층</td>\n<td><code class=\"language-text\">lstm()</code>을 2번 써준다</td>\n</tr>\n<tr>\n<td>양방향</td>\n<td><code class=\"language-text\">bidirectional</code> + <code class=\"language-text\">merge_mode = ‘concat’</code> <br />FNN, BFN 값을 merge_mode 형태로 합쳐서 list형으로 되돌려줌<br />단방향(FBN)은 ‘이후’만 기억, 양방향(FBN+BFN)은 ‘이전’+’이후’ 모두 기억</td>\n</tr>\n<tr>\n<td>many-to-many</td>\n<td><code class=\"language-text\">return-sequences = True</code><br />LSTM 뉴런 <strong>각각의 중간 스텝에서 나오는 각각의 출력(h)</strong>을 (바로 위 뉴런으로도) 사용(전파)한다는 뜻</td>\n</tr>\n<tr>\n<td>timedistributed</td>\n<td><code class=\"language-text\">timedistributed()</code><br /> <strong>FFN으로 가기 전</strong> LSTM 마지막 층에서 각 뉴런의 각 지점에서 계산한 오류를 다음 층으로 전파</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n<br>\n<ul>\n<li>LSTM이 many-to-many 상태에서 FNN으로 가면 각각의 Output 값이 나온다</li>\n<li>\n<p>NLP의 챗봇, 기계번역 등에서 사용함.</p>\n<ul>\n<li>Input > 안녕 만나서 반가워</li>\n<li>Output > 저도 반갑습니다</li>\n<li>3개의 출력층. 비어 있는 1개는 padding </li>\n<li>Q. ... padding은 어디로?</li>\n</ul>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>LSTM에서 사용되는 h와 c</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>역할</th>\n<th>특징</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>h</td>\n<td><strong>위, 왼쪽</strong> 전파</td>\n<td>LSTM이 1층일 땐 c랑 똑같이 왼쪽으로 밖에 전파 못한다.</td>\n</tr>\n<tr>\n<td>c</td>\n<td><strong>왼쪽</strong> 전파</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>h와 c 둘다 처음엔 0으로 시작한다.</p>\n</blockquote>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<h1 id=\"cnn\" style=\"position:relative;\"><a href=\"#cnn\" aria-label=\"cnn permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>CNN</h1>\n<ul>\n<li>이미지를 대표할 수 있는 특성들을 도출해서 FNN에 넣어줌</li>\n<li>\n<table>\n<thead>\n<tr>\n<th>code</th>\n<th>설명</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code class=\"language-text\">Input</code>(batch_shape = (None, nStep, nFeature, nChannel))</td>\n<td></td>\n</tr>\n<tr>\n<td><code class=\"language-text\">Conv2D</code>(filters=30, kernel_size=(8,3), strides=1, padding = 'same', activation='relu')</td>\n<td></td>\n</tr>\n<tr>\n<td><code class=\"language-text\">MaxPooling2D</code>(pool_size=(2,1), strides=1, padding='valid')</td>\n<td>- 경우에 따라 conv2D, pooling 더 써줄 수 있음<br />- <code class=\"language-text\">GlobalMaxPooling1D()</code>도 있음</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">Flatten()</code></td>\n<td>2D는 4차원이라 shape 맞추려고 보통 flatten을 써줌<br />1d는 안 써도 되는 듯(?)</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">Dense</code>(nOutput, activation='linear')</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<h2 id=\"lstm과-cnn의-차이\" style=\"position:relative;\"><a href=\"#lstm%EA%B3%BC-cnn%EC%9D%98-%EC%B0%A8%EC%9D%B4\" aria-label=\"lstm과 cnn의 차이 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LSTM과 CNN의 차이</h2>\n<ul>\n<li>둘다 (흐름을 보는)시계열 데이터에 사용할 수 있다.</li>\n<li>LSTM과 CNN1D는 기능은 비슷하지만 CNN1D는 table 中 <strong>(colum 전체+n row)아래 방향으로의 흐름</strong>을 보는 거고, </li>\n<li>LSTM은 bidirectional 을 사용해서 table 中 <strong>위/아래로 흐름</strong>을 이동시켜서 볼 수 있다.</li>\n<li>CNN2D는 kernel_size, pooling 등을 통해 tabel 中 <strong>(n colum(일부분) + n row(일부분) = 내가 focus를 맞춰 보고 싶은 부분)에 따라 그 흐름을 볼 수 있다는 데</strong>서 차이가 있다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/05265afca2b82133a555e66d16254a73/108f8/image-20200805122723206.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 63.51351351351351%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAABmUlEQVQ4y5WTW1ODMBCF+///W1860xkdb22tCpYCuZMQOO4GUlHbBxmYwMnm25PNshpB1zjS/feZpv6nrxIM168c9B99lR2azxPa93eIokBzPMILkYLsqULz9gbJOo2ubpIelELz9Az58Ij29RW6mfQJOMQk+nONUUooApuqSgHySBBKxrotPyE/iqSbooR6fkG8u4cvy2TmG6gryLrCMAyTK2PgOk/OeyjRIoSQdO89NM3RBGxbJ/fQGlFpyLZdAD0LNWIGWgvrOnLuoUj3ob8AlSbgGGElAedEkUomaQcLoCFgswCaGRgI2BAw/ASCgIKAzk3Avv8FDDZZjjFO9aFt2Y6A0UG3Z4R+0nnrOgEpqTjDcVI+AjLyE6hOEFVxaR/n5i33BuL0kd65JbgUeaEp96m+2XlzOWWmDFR8KSi7TovqukbXTdn5EFpyz3oeE8SZFCeovRj27XBuUra93W6x2+3+dP9+v8dms0lOcmx2tl6vcTgcFo3Ni+bF7Cq3yBLKOjvLoKzzN0N7OpSsra79SsvvW3O39C+NMPfX+fR5GQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200805122723206\"\n        title=\"image-20200805122723206\"\n        src=\"/static/05265afca2b82133a555e66d16254a73/fcda8/image-20200805122723206.png\"\n        srcset=\"/static/05265afca2b82133a555e66d16254a73/12f09/image-20200805122723206.png 148w,\n/static/05265afca2b82133a555e66d16254a73/e4a3f/image-20200805122723206.png 295w,\n/static/05265afca2b82133a555e66d16254a73/fcda8/image-20200805122723206.png 590w,\n/static/05265afca2b82133a555e66d16254a73/108f8/image-20200805122723206.png 777w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<br>\n<p><br><br></p>\n<h1 id=\"activation\" style=\"position:relative;\"><a href=\"#activation\" aria-label=\"activation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>activation</h1>\n<ul>\n<li>\n<table>\n<thead>\n<tr>\n<th>activation(비선형 함수)</th>\n<th>loss</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code class=\"language-text\">softmax</code></td>\n<td><code class=\"language-text\">sparse_categorical_crossentropy</code></td>\n</tr>\n<tr>\n<td><code class=\"language-text\">sigmoid</code></td>\n<td><code class=\"language-text\">binary_crossentropy</code></td>\n</tr>\n<tr>\n<td><code class=\"language-text\">linear</code></td>\n<td><code class=\"language-text\">mse</code></td>\n</tr>\n<tr>\n<td><code class=\"language-text\">relu</code></td>\n<td>← Hidden layer에 씀. 기울기가 0이기 때문에 뉴런이 죽을 수 있는 단점 有</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Leakly ReLU</td>\n<td>뉴런이 죽을 수 있는 현상 해결</td>\n</tr>\n<tr>\n<td>PReLU</td>\n<td>x&#x3C;0 에서 학습 가능</td>\n</tr>\n<tr>\n<td>granger causality</td>\n<td>통제된 상황에서 인과관계가 가능하다고 말할 수 있음. 시계열 데이터에서 쓰일 수 있음</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<ul>\n<li>sparse<em>categorical</em>crossentropy</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>encoderX<span class=\"token punctuation\">,</span> decoderX<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> outputY<span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span>optimizers<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span>lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> loss<span class=\"token operator\">=</span><span class=\"token string\">'sparse_categorical_crossentropy'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>sparse 안 쓸 거면 위에 'outputY'를 to<em>categorical()로 변형 후, loss 함수로 \"categorical</em>crossentropy\" 사용</li>\n<li>target이 one-hot encoding되어 있으면 categorical<em>crossentropy,\ntarget이 integer로 되어 있으면 sparse</em>categorical<em>crossentropy를 쓴다.\nsparse</em>categorical<em>entropy는 integer인 target을 one-hot으로 바꾼 후에 categorical</em>entropy를 수행한다.</li>\n</ul>\n</blockquote>\n<br>\n</li>\n<li>\n<p>딥러닝 네트워크(DN)의 노드는 입력값을 전부 더한 후, 활성화 함수(Activation function)를 통과시켜 다음 노드에 전달한다.</p>\n<ul>\n<li>이때 사용하는 활성화 함수는 비선형 함수를 쓴다. </li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<h2 id=\"softmax---sigmoid\" style=\"position:relative;\"><a href=\"#softmax---sigmoid\" aria-label=\"softmax   sigmoid permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>softmax - sigmoid</h2>\n<table>\n<thead>\n<tr>\n<th>구분</th>\n<th>함수</th>\n<th>code</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>회귀</td>\n<td>항등함수(출력값을 그대로 반환)</td>\n<td></td>\n</tr>\n<tr>\n<td>분류(0/1)</td>\n<td>sigmoid</td>\n<td># 시험 데이터로 학습 성능을 평가한다<br/>predicted = model.predict(test<em>input)<br/>test</em>pred = np.where(predicted > 0.5, 1, 0)<br/>accuracy = (test<em>label == test</em>pred).mean()</td>\n</tr>\n<tr>\n<td>분류(multiple)</td>\n<td>softmax</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p> Cross-Entropy : 예측한 값과 실제값의 차를 계산. entropy 값이 감소하는 방향으로 진행하다 보면 최저 값을 찾을 수 있다. </p>\n<ul>\n<li>출처: sshkim Sh.TK. 2017. 8. 23. \"[모두의딥러닝] Softmax Regression (Multinomial Logistic Regression)\". \"<a href=\"https://sshkim.tistory.com/146\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://sshkim.tistory.com/146</a>\"</li>\n</ul>\n</blockquote>\n<blockquote>\n<p> argmax 을 사용하면 2라는 값이 나온다. 가장 큰 값의 위치가 2번째에 있는 1이기 때문</p>\n<ul>\n<li>출처: JINSOL KIM. 2017. 12. 24. \"Softmax vs Sigmoid\". <a href=\"https://blog.naver.com/infoefficien/221170205067\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://blog.naver.com/infoefficien/221170205067</a></li>\n</ul>\n</blockquote>\n<br>\n<br>\n<br>\n<h2 id=\"relu\" style=\"position:relative;\"><a href=\"#relu\" aria-label=\"relu permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ReLu</h2>\n<ul>\n<li>히든층에 자주 쓰임</li>\n<li>\n<p>그냥 CNN이든 LSTM이든 출력층 Dense에 Relu 쓰지 말자</p>\n<ul>\n<li>LSTM에선 Relu 안 쓰는 게 좋음. 특히 출력층엔 쓰면 안 됨.</li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<h1 id=\"학습compile-예측predict\" style=\"position:relative;\"><a href=\"#%ED%95%99%EC%8A%B5compile-%EC%98%88%EC%B8%A1predict\" aria-label=\"학습compile 예측predict permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>학습(compile), 예측(predict)</h1>\n<h2 id=\"optimizer\" style=\"position:relative;\"><a href=\"#optimizer\" aria-label=\"optimizer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>optimizer</h2>\n<ul>\n<li>\n<table>\n<thead>\n<tr>\n<th>종류(빈도순)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code class=\"language-text\">adam</code></td>\n</tr>\n<tr>\n<td>Adadelta, RMSprop, Adagrad</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">momentum</code></td>\n</tr>\n<tr>\n<td>GD, NAG</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>최적화가 잘 안 되면 글로벌 minmun을 찾지 못하고 로컬 minimum에 빠진다. 이때 로컬 minimum을 <strong>어떻게 빨리</strong> 탈출할 수 있을지 U턴 메소드를 쓸지, 다른 1차 미분방법(GD)를 쓸 지 결정하게 된다. </li>\n</ul>\n<br>\n<br>\n<br>\n<h2 id=\"epoch\" style=\"position:relative;\"><a href=\"#epoch\" aria-label=\"epoch permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>epoch</h2>\n<ul>\n<li><code class=\"language-text\">epoch</code> 수치가 커지면 <code class=\"language-text\">optimizer</code>가 일을 해서 local이 아닌 global을 찾아간다.</li>\n<li>그런데 너무 크면 overfitting</li>\n<li>따라서 적당한 <code class=\"language-text\">epoch</code> 설정이 필요 </li>\n</ul>\n<br>\n<br>\n<br>\n<h2 id=\"batch_size\" style=\"position:relative;\"><a href=\"#batch_size\" aria-label=\"batch_size permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Batch_size</h2>\n<ul>\n<li>\n<p>data가 크면 <code class=\"language-text\">batch_size</code>도 크게</p>\n<ul>\n<li>25,000개의 raw data라면 <code class=\"language-text\">batch_size</code> = 20 보다 300 이 정도로 설정</li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<hr>\n<br>\n<br>\n<br>\n<h1 id=\"nlp--dl\" style=\"position:relative;\"><a href=\"#nlp--dl\" aria-label=\"nlp  dl permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NLP &#x26; DL</h1>\n<h2 id=\"sgns\" style=\"position:relative;\"><a href=\"#sgns\" aria-label=\"sgns permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SGNS</h2>\n<table>\n<thead>\n<tr>\n<th>용어</th>\n<th>설명</th>\n<th>CODE</th>\n<th>참고</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>pre-trained</td>\n<td>SGNS에서 학습한 We를 적용</td>\n<td>model.layers[1]<strong>.set_weights</strong>(We)</td>\n<td>해당 code 적용 후 model fit 진행</td>\n</tr>\n<tr>\n<td>fine-training</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<br>\n<ul>\n<li>\n<p>SGNS에 모델 학습(fit) 시, 학습을 따로 시키는 이유?</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 학습</span>\nhist <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>X<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> X<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> X<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> \n               batch_size<span class=\"token operator\">=</span>BATCH_SIZE<span class=\"token punctuation\">,</span>\n               epochs<span class=\"token operator\">=</span>NUM_EPOCHS<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<ul>\n<li>각기 연결된 가중치 선이 구분되어 있기 때문에</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>SGNS 모델 만들 때 dot을 한다면, </p>\n<ol>\n<li><strong>axis=2</strong>    <em>@2</em></li>\n</ol>\n<p> → 후에</p>\n<ol start=\"2\">\n<li>reshape<strong>(())</strong>    <em>@괄호 두 개</em></li>\n</ol>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>SGNS로 만든 Embedding의 w(가중치)를 basic한 word data에 적용할 때, load_weights 사용하는 방법도 있다.</p>\n<ul>\n<li>근데 이땐 shape을 맞춰줘야 한다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">w <span class=\"token operator\">=</span> encoder<span class=\"token punctuation\">.</span>load_weights<span class=\"token punctuation\">(</span><span class=\"token string\">'model_w.h5'</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 가중치(w) 불러온 후,</span>\nemb <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>max_features<span class=\"token punctuation\">,</span> embedding_dims<span class=\"token punctuation\">,</span> load_weights <span class=\"token operator\">=</span> w<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>xInput<span class=\"token punctuation\">)</span> <span class=\"token comment\"># embedding layer에 바로 적용</span></code></pre></div>\n<ul>\n<li>보통 이런 느낌으로 씀</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">weights <span class=\"token operator\">=</span> load_weights<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nembedding_layer <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>V<span class=\"token punctuation\">,</span>\n                            output_dim<span class=\"token operator\">=</span>embedding_dim<span class=\"token punctuation\">,</span>\n                            input_length<span class=\"token operator\">=</span>input_length<span class=\"token punctuation\">,</span>\n                            trainable<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n                            weights<span class=\"token operator\">=</span>weights<span class=\"token punctuation\">,</span>\n                            name<span class=\"token operator\">=</span><span class=\"token string\">'embedding'</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<h2 id=\"embedding--pad_sequences\" style=\"position:relative;\"><a href=\"#embedding--pad_sequences\" aria-label=\"embedding  pad_sequences permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Embedding &#x26; pad_sequences</h2>\n<br>\n<h3 id=\"word2vec-기준\" style=\"position:relative;\"><a href=\"#word2vec-%EA%B8%B0%EC%A4%80\" aria-label=\"word2vec 기준 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>word2vec 기준</h3>\n<table>\n<thead>\n<tr>\n<th>word2vec</th>\n<th>code</th>\n<th>input</th>\n<th>output</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>tokenizer</td>\n<td>tokenizer = Tokenizer()<br />tokenizer.fit<em>on</em>texts(clean<em>train</em>review)<br />train<em>sequences = tokenizer.texts</em>to<em>sequences(clean</em>train_review)</td>\n<td>[안녕, 만나서, 반가워]</td>\n<td>[13, 4, 3]</td>\n</tr>\n<tr>\n<td>pad_sequences</td>\n<td>train<em>inputs = pad</em>sequences(train<em>sequences, maxlen=MAX</em>SEQUENCE_LENGTH, padding='post')</td>\n<td>[13, 4, 3]</td>\n<td>([0,0...1,..],<br />[0,0,0,1,0,...],<br />[0,0,1,...])</td>\n</tr>\n<tr>\n<td>Embedding</td>\n<td>embedding<em>layer = Embedding(input</em>dim=VOCAB<em>SIZE, output</em>dim=EMB_SIZE)</td>\n<td>([0,0...1,..],<br />[0,0,0,1,0,...],<br />[0,0,1,...])</td>\n<td>[0,0...1,..] -> ANN layer</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>embedding<em>layer 는 결국 pad</em>sequence된 단어들끼리 모임. 즉, 1개 문장에 대한 임베딩 행렬이 됨 </p>\n<p>1개 단어 = 1개 임베딩 레이어=벡터값</p>\n</blockquote>\n<br>\n<br>\n<h3 id=\"doc2vec-기준\" style=\"position:relative;\"><a href=\"#doc2vec-%EA%B8%B0%EC%A4%80\" aria-label=\"doc2vec 기준 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>doc2vec 기준</h3>\n<table>\n<thead>\n<tr>\n<th>doc2vec</th>\n<th>code</th>\n<th>input</th>\n<th>output</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>TaggedDocument</td>\n<td>documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)]</td>\n<td>[...,'laughabl',<br/>  'horror'],<br/> ...]</td>\n<td>TaggedDocument(words=['move', 'last', ... 'horror'], tags=[999])</td>\n</tr>\n<tr>\n<td>Embedding</td>\n<td>model = Doc2Vec(vector<em>size=300, alpha=0.025, min</em>alpha=0.00025, min_count=10, workers=4, dm =1)</td>\n<td>TaggedDocument(words=['move', 'last', ... 'horror'], tags=[999])</td>\n<td>[벡터값]</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>tags=[999] : 999번 째 문장</p>\n<p>Embedding은 model.build_vocab, model.train 거치면 한 문장에 대한 하나의 벡터가 나온다.</p>\n<p>(word2vec의 경우 한 문장에 있는 각각의 단어 수만큼 벡터가 나온다.)</p>\n<p>1개 문장 = 1개 임베딩 레이어 = 1개 벡터값</p>\n</blockquote>\n<br>\n<br>\n<br>\n<h1 id=\"chatbot\" style=\"position:relative;\"><a href=\"#chatbot\" aria-label=\"chatbot permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ChatBot</h1>\n<br>\n<h2 id=\"sequence-to-sequence\" style=\"position:relative;\"><a href=\"#sequence-to-sequence\" aria-label=\"sequence to sequence permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Sequence to Sequence</h2>\n<table>\n<thead>\n<tr>\n<th>encoder</th>\n<th>decoder</th>\n<th>가능/불가능</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1층</td>\n<td>2층</td>\n<td><em>불가능</em></td>\n</tr>\n<tr>\n<td>1층</td>\n<td>1층</td>\n<td>가능</td>\n</tr>\n<tr>\n<td>2층</td>\n<td>1층</td>\n<td>가능</td>\n</tr>\n<tr>\n<td>2층</td>\n<td>2층</td>\n<td>가능</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>\"굳이 2층으로 할 필요가 있는가?\"</p>\n<p>→ 1층으로 하는 건 선형의 개념. 2층은 비선형의 개념이다.</p>\n<p>비선형이 분류를 더 잘해낼 수도 있지만, overfitting의 위험이 있다. </p>\n</blockquote>\n<br>\n<br>\n<br>\n<hr>\n<br>\n<h1 id=\"기타\" style=\"position:relative;\"><a href=\"#%EA%B8%B0%ED%83%80\" aria-label=\"기타 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>기타</h1>\n<br>\n<h2 id=\"유클리디안-거리\" style=\"position:relative;\"><a href=\"#%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%94%94%EC%95%88-%EA%B1%B0%EB%A6%AC\" aria-label=\"유클리디안 거리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>유클리디안 거리</h2>\n<ul>\n<li>\n<p>거리 계산할 때, 비교하고 싶은 건 <code class=\"language-text\">[]</code>를 쳐서 넣어주기  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">euclidean_distances<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>father<span class=\"token punctuation\">,</span> mother<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<h2 id=\"가중치-저장save\" style=\"position:relative;\"><a href=\"#%EA%B0%80%EC%A4%91%EC%B9%98-%EC%A0%80%EC%9E%A5save\" aria-label=\"가중치 저장save permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>가중치 저장(Save)</h2>\n<ul>\n<li>\n<p>Embedding (left side) layer의 W를 저장할 때, [2]를 저장한단 사실 알아두기</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'data/embedding_W.pickle'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'wb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n  pickle<span class=\"token punctuation\">.</span>dump<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>get_weights<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> f<span class=\"token punctuation\">,</span> pickle<span class=\"token punctuation\">.</span>HIGHEST_PROTOCOL<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<h2 id=\"영역별-code--논문-참고하기-좋은-site\" style=\"position:relative;\"><a href=\"#%EC%98%81%EC%97%AD%EB%B3%84-code--%EB%85%BC%EB%AC%B8-%EC%B0%B8%EA%B3%A0%ED%95%98%EA%B8%B0-%EC%A2%8B%EC%9D%80-site\" aria-label=\"영역별 code  논문 참고하기 좋은 site permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>영역별 code &#x26; 논문 참고하기 좋은 site</h2>\n<ul>\n<li>\n<p>SOTA site</p>\n<p><a href=\"https://paperswithcode.com/sota\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://paperswithcode.com/sota</a></p>\n</li>\n</ul>\n<br>\n<br>\n<br>","excerpt":"W(weights) 네트워크 및 model build까지 완성해서 실행되어 역전파 되었을 때 형성된다. : 네트워크 만들고 난 후 model build하는 과정. optimizer & loss 값을 정의해주는 부분임. w…","tableOfContents":"<ul>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#wweights\">W(weights)</a></li>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#hyper-parameter\">Hyper parameter</a></li>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#dense\">Dense</a></li>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#fnn%EC%88%9C%EB%B0%A9%ED%96%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D\">FNN(순방향 신경망)</a></li>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#lstm\">LSTM</a></li>\n<li>\n<p><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#cnn\">CNN</a></p>\n<ul>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#lstm%EA%B3%BC-cnn%EC%9D%98-%EC%B0%A8%EC%9D%B4\">LSTM과 CNN의 차이</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#activation\">activation</a></p>\n<ul>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#softmax---sigmoid\">softmax - sigmoid</a></li>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#relu\">ReLu</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#%ED%95%99%EC%8A%B5compile-%EC%98%88%EC%B8%A1predict\">학습(compile), 예측(predict)</a></p>\n<ul>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#optimizer\">optimizer</a></li>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#epoch\">epoch</a></li>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#batch_size\">Batch_size</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#nlp--dl\">NLP &#x26; DL</a></p>\n<ul>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#sgns\">SGNS</a></li>\n<li>\n<p><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#embedding--pad_sequences\">Embedding &#x26; pad_sequences</a></p>\n<ul>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#word2vec-%EA%B8%B0%EC%A4%80\">word2vec 기준</a></li>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#doc2vec-%EA%B8%B0%EC%A4%80\">doc2vec 기준</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#chatbot\">ChatBot</a></p>\n<ul>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#sequence-to-sequence\">Sequence to Sequence</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#%EA%B8%B0%ED%83%80\">기타</a></p>\n<ul>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%94%94%EC%95%88-%EA%B1%B0%EB%A6%AC\">유클리디안 거리</a></li>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#%EA%B0%80%EC%A4%91%EC%B9%98-%EC%A0%80%EC%9E%A5save\">가중치 저장(Save)</a></li>\n<li><a href=\"/TQT(The%20question%20I%20asked%20the%20teacher%20today.)/#%EC%98%81%EC%97%AD%EB%B3%84-code--%EB%85%BC%EB%AC%B8-%EC%B0%B8%EA%B3%A0%ED%95%98%EA%B8%B0-%EC%A2%8B%EC%9D%80-site\">영역별 code &#x26; 논문 참고하기 좋은 site</a></li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/TQT(The question I asked the teacher today.)/"},"frontmatter":{"title":"TQT(The question I asked the teacher today)","date":"Aug 01, 2020","tags":["TQT"],"keywords":["JyneeEarth","jynee"],"update":"Aug 06, 2020"}}},"pageContext":{"slug":"/TQT(The question I asked the teacher today.)/","series":[],"lastmod":"2020-08-06"}},"staticQueryHashes":["3649515864","694178885"]}