{"componentChunkName":"component---src-pages-index-tsx","path":"/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"excerpt":"NLP  Bidirectional Encoder Representations Form Transformer transformer의 encoder만 사용한다.  Pre-traning과 fine-tuning으로 활용한다. 즉, transformer…","fields":{"slug":"/NLP_BERT_1/"},"frontmatter":{"date":"Aug 18, 2020","update":"Aug 20, 2020","title":"NLP BERT","tags":["NLP","BERT","XLNet"]}}},{"node":{"excerpt":"NLP  CNN, RNN 대신 Self-Attention을 사용하는 모델 Transformer는 RNN, LSTM없이 time 시퀀스 역할을 하는 모델입니다. RNN, LSTM 셀을 일체 사용하지 않았으나, 자체만으로 time…","fields":{"slug":"/NLP_Transfomer/"},"frontmatter":{"date":"Aug 15, 2020","update":"Aug 20, 2020","title":"NLP Transformer & SentecePiece","tags":["NLP","transformer","SentencePiece"]}}},{"node":{"excerpt":"NLP  NLP, Time Serise/Sin data 예측에 사용한다. 기법 종류: seq2seq (RNN) > SL(fine Tuning) → USL Attention: RNN 기반 > SL(fine Tuning) → USL Self…","fields":{"slug":"/NLP_Seq2Seq/"},"frontmatter":{"date":"Aug 15, 2020","update":"Aug 18, 2020","title":"NLP Sequence to Sequence","tags":["NLP","seq2seq"]}}},{"node":{"excerpt":"GAN 1D 정규분포에서 샘플링한 데이터를 모방하여, fake data를 생성한다. fake data는 정규분포의 특성을 갖는다. (KL divergence, 평균, 분산, 왜도, 첨도 등) Discrimi의 loss는 maxlog(Dx) + log…","fields":{"slug":"/GAN_2/"},"frontmatter":{"date":"Aug 08, 2020","update":"Aug 08, 2020","title":"GAN 실전 응용","tags":["DL","GAN"]}}},{"node":{"excerpt":"GAN 비지도학습(UL) 방식의 이미지, 문서, 음성 등의 데이터를 생성(모방)하는 알고리즘 비모수적방법으로도 비교적 정확한 sampling이 가능함  위조 데이터 생성 및 판별에 사용 \nEX…","fields":{"slug":"/GAN_1/"},"frontmatter":{"date":"Aug 08, 2020","update":"Aug 08, 2020","title":"GAN 이론","tags":["DL","GAN"]}}},{"node":{"excerpt":"NLP Quora  : 질문 간 텍스트 유사도 분석 maLSTM : 맨하탄 거리 사용한 LSTM GloVe : 빈도 + 맥락(Embedding) 고려한 워드 패키지 FastText : hash…","fields":{"slug":"/NLP한글_4/"},"frontmatter":{"date":"Aug 07, 2020","update":"Aug 19, 2020","title":"NLP maLSTM","tags":["NLP","maLSTM","Quora"]}}},{"node":{"excerpt":"AUC Area under the roc curve (AUC) Confusion matix(Binaray classification)  predictionP N actual P TP FP N FN TN TPR  FPR  Thes…","fields":{"slug":"/NLP한글_3/"},"frontmatter":{"date":"Aug 06, 2020","update":"Aug 19, 2020","title":"NLP Kaggle competition 우승자가 제안한 새로운 접근방법을 배워보자","tags":["NLP","논문 분석"]}}},{"node":{"excerpt":"NLP 분야에서 딥러닝의 고급 응용 DMN Ask Me Anything attention score layer story layer episodic memory layer answer layer 텍스트 자동 생성 예제 문장: I love you…","fields":{"slug":"/NLP응용_4/"},"frontmatter":{"date":"Aug 05, 2020","update":"Aug 18, 2020","title":"NLP Ask Me Anything","tags":["NLP","attention","ask_me_anything"]}}},{"node":{"excerpt":"one-hot 인코딩 categorical 변환 방법  Keras를 이용한 one-hot encoding '남자' '여자' '아빠' '엄마' '삼촌' '이모'\n'남자', '삼촌', '아빠', '엄마', '여자', '이모' [1. 0. 0. 0.…","fields":{"slug":"/NLP한글_1/"},"frontmatter":{"date":"Aug 05, 2020","update":"Aug 18, 2020","title":"NLP 카운트 기반 방법의 텍스트 유사도 측정","tags":["NLP","SVD","거리 측정"]}}},{"node":{"excerpt":"텍스트 분류 Skip-Gram SGNS Hirarchical softmax  연산이 많아진단 단점의 softmax를 개선하여 Binary Tree 사용 Binary Tree iForest 알고리즘, DB indexing…","fields":{"slug":"/NLP한글_2/"},"frontmatter":{"date":"Aug 05, 2020","update":"Aug 18, 2020","title":"NLP Doc2Vec","tags":["NLP","Doc2Vec"]}}},{"node":{"excerpt":"안녕하세요. 이곳엔 다양한 포스팅이 올라옵니다. 태그로 동일 관심분야를 검색해주세요. 안하면 모를테고, 하면 늘겠지 싶은 마음으로 일단 파고들었던 모든 것들을 이곳에 올릴 예정입니다. 이런 것까지...?  이런 것까지... 올라올 거예요. ML/DL…","fields":{"slug":"/main/"},"frontmatter":{"date":"Aug 02, 2020","update":"Aug 22, 2020","title":"처음 방문했다면 블로그 소개를","tags":["1st"]}}},{"node":{"excerpt":"W(weights) 네트워크 및 model build까지 완성해서 실행되어 역전파 되었을 때 형성된다. : 네트워크 만들고 난 후 model build하는 과정. optimizer & loss 값을 정의해주는 부분임. w…","fields":{"slug":"/TQT(The question I asked the teacher today.)/"},"frontmatter":{"date":"Aug 01, 2020","update":"Aug 06, 2020","title":"TQT(The question I asked the teacher today)","tags":["TQT"]}}},{"node":{"excerpt":"NLP & DL 특수 목적이 아닌, 범용적(일반적)으로 쓰일 Word Embedding을 만든다. embedding의 방법 따라서 문장 속 단어의 맥락(의미)를 파악할 줄 안다. 즉, semantic…","fields":{"slug":"/NLP응용_3/"},"frontmatter":{"date":"Jul 29, 2020","update":"Aug 16, 2020","title":"NLP Word2Vec/SGNS","tags":["NLP","Word2Vec","SGNS"]}}},{"node":{"excerpt":"NLP & 딥러닝 핵심 문제: \"단어를 어떻게 수치화할 것인가?\" Email - Classification 딥러닝을 이용하여 20개의 카테고리로 분류된 이메일 데이터를 학습하고, 시험 이메일을 20개 카테고리 중 하나로 분류한다.  Email…","fields":{"slug":"/NLP응용_2/"},"frontmatter":{"date":"Jul 22, 2020","update":"Aug 16, 2020","title":"NLP Embedding","tags":["NLP","Embedding"]}}},{"node":{"excerpt":"고급 NLP 레시피 자연어 기초 용어 편집거리 주제식별 감성분석  자연어 관련 용어 (문서) (말뭉치): 텍스트(문서)의 집합 (토큰): 단어처럼 의미를 가지는 요소 (형태소): 의미를 가지는 언어에서 최소 단위 (품사): ex) Nouns…","fields":{"slug":"/NLP응용_1/"},"frontmatter":{"date":"Jul 20, 2020","update":"Aug 16, 2020","title":"NLP 편집거리/주제식별/자연어분석","tags":["NLP","LDA","PageRank"]}}},{"node":{"excerpt":"NLP 형식언어 이론 Context-free Grammar Context-sensitive Grammar Natural Language 문장 구조 분석 Word-salad…","fields":{"slug":"/NLP기초_4/"},"frontmatter":{"date":"Jul 17, 2020","update":"Aug 16, 2020","title":"(NLP 기초) 문장 구조 분석","tags":["NLP","기초"]}}},{"node":{"excerpt":"NLP 정규표현식 청킹 칭킹  문서 정보 추출  정해진 패턴을 사용해서 패턴에 일치하는 데이터 검색을 지원하는 표현식 정규표현식에 쓰이는 특수문자  : 아무 문자나 여러 개   : } { 안의 내용 제외     =  읽어보기 DEVHolic…","fields":{"slug":"/NLP기초_3/"},"frontmatter":{"date":"Jul 17, 2020","update":"Aug 16, 2020","title":"(NLP 기초) 문서 정보 추출","tags":["NLP","기초"]}}},{"node":{"excerpt":"NLP 품사 태깅 원리 HMM 품사 태깅 : 문장의 N, V, ad, av 판별 문장만 보고 품사를 붙여주는 기계:   문맥 = '문장 내' 주변 단어 =  현재 NLP 상에선 문장 간, 절 간 Context는 불가 \"NLP…","fields":{"slug":"/NLP기초_2/"},"frontmatter":{"date":"Jul 16, 2020","update":"Aug 16, 2020","title":"(NLP 기초) 품사 태깅","tags":["NLP","기초"]}}},{"node":{"excerpt":"딥러닝 DL RNN CNN 순환 신경망(RNN) hidden 층에서 서로 값을 기억해 순환한다. 지금까진 FNN(feed forward neword) + 순서가 필요 없는 data를 써서 모델이 기억할 필요가 없었지만, 문장 같은 data…","fields":{"slug":"/딥러닝_2/"},"frontmatter":{"date":"Jul 14, 2020","update":"Aug 16, 2020","title":"딥러닝 LSTM&CNN","tags":["DL","LSTM","CNN"]}}},{"node":{"excerpt":"딥러닝 DL optimaizer kerass optimizers 이차방정식 계수 추정 방법들: : 움직임  : 미분해서 움직임 : 지수이동평균법으로 움직임 : 관성 방향으로 이동 후 그 지점에서 GD 방향으로 움직임  가중치(알파 or lr…","fields":{"slug":"/딥러닝_1/"},"frontmatter":{"date":"Jul 03, 2020","update":"Aug 16, 2020","title":"딥러닝 기초","tags":["DL"]}}},{"node":{"excerpt":"머신러닝(ML) K-Means 클러스터링 H-clustering DBSAN 앙상블 연관규칙 분석 k-means 클러스터링 비지도학습 비계층적 군집분석 k-means 클러스터링은 데이터를 k개의 클러스터(cluster…","fields":{"slug":"/머신러닝_2/"},"frontmatter":{"date":"Jun 30, 2020","update":"Aug 16, 2020","title":"머신러닝 분석 방법들, 두 번째","tags":["ML","XGBoost"]}}},{"node":{"excerpt":"머신러닝(ML) KNN Decision Tree SVM 선형회귀분석 로지스틱회귀분석 나이브베이지안  차원(feature…","fields":{"slug":"/머신러닝_1/"},"frontmatter":{"date":"Jun 23, 2020","update":"Aug 16, 2020","title":"머신러닝 분석 방법들, 첫 번째","tags":["ML"]}}}]}},"pageContext":{}},"staticQueryHashes":["3649515864","694178885"]}