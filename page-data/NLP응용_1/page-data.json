{"componentChunkName":"component---src-templates-post-tsx","path":"/NLP응용_1/","result":{"data":{"markdownRemark":{"html":"<br>\n<h1 id=\"고급-nlp-레시피\" style=\"position:relative;\"><a href=\"#%EA%B3%A0%EA%B8%89-nlp-%EB%A0%88%EC%8B%9C%ED%94%BC\" aria-label=\"고급 nlp 레시피 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>고급 NLP 레시피</h1>\n<ul>\n<li>자연어 기초 용어</li>\n<li>편집거리</li>\n<li>주제식별</li>\n<li>감성분석</li>\n</ul>\n<p><br><br></p>\n<h2 id=\"자연어-관련-용어\" style=\"position:relative;\"><a href=\"#%EC%9E%90%EC%97%B0%EC%96%B4-%EA%B4%80%EB%A0%A8-%EC%9A%A9%EC%96%B4\" aria-label=\"자연어 관련 용어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>자연어 관련 용어</h2>\n<ul>\n<li><code class=\"language-text\">Document</code>(문서)</li>\n<li><code class=\"language-text\">Corpus</code>(말뭉치): 텍스트(문서)의 집합</li>\n<li><code class=\"language-text\">Token</code>(토큰): 단어처럼 의미를 가지는 요소</li>\n<li><code class=\"language-text\">Morphemes</code>(형태소): 의미를 가지는 언어에서 최소 단위</li>\n<li><code class=\"language-text\">POS</code>(품사): ex) Nouns, Verbs</li>\n<li><code class=\"language-text\">Stopword</code>(불용어): I, my, me, 조사, 접미사와 같이 자주 나타나지만 실제 의미에 큰 기여를 하지 못하는 단어들</li>\n<li><code class=\"language-text\">Stemming</code>(어간 추출): 어간만 추출하는 것을 의미(running, runs, run -> run)</li>\n<li>\n<p><code class=\"language-text\">Lemmatization</code>(음소표기법): 앞뒤 문맥을 보고 단어를 식별하는 것</p>\n<blockquote>\n<p>출처: hero4earth. 2018.01.17. \"자연어(NLP) 처리 기초 정리\". <a href=\"http://hero4earth.com/blog/learning/2018/01/17/NLP_Basics_01/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">http://hero4earth.com/blog/learning/2018/01/17/NLP_Basics_01/</a></p>\n</blockquote>\n</li>\n</ul>\n<br>\n<hr>\n<br>\n<h2 id=\"편집거리edit-distance\" style=\"position:relative;\"><a href=\"#%ED%8E%B8%EC%A7%91%EA%B1%B0%EB%A6%ACedit-distance\" aria-label=\"편집거리edit distance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>편집거리(Edit distance)</h2>\n<ul>\n<li>\n<p>두 단어 간의 <strong>형태적인 거리</strong></p>\n<ul>\n<li>의미적 거리 X</li>\n<li>의미적인 거리는 Word Embedding을 통해 구현</li>\n</ul>\n</li>\n<li>\n<p>문자 교정, 추천단어에 사용</p>\n<ul>\n<li>ex: 타자기에 appie를 쳤을 때, 기계 내에서 apple로 추천 단어를 보여주는 시스템</li>\n<li>이때 A와 B의 편집거리가 작은 순서로 추천을 해준다</li>\n<li>\n<p>그렇다면, \"A와 B 사이의 거리를 어떻게 측정할 것인가?\"</p>\n<p>→ Edit distance = Levenshtein distance</p>\n</li>\n</ul>\n</li>\n</ul>\n<br>\n<blockquote>\n<p> 이하 코드는 크리슈나 바브샤 외 2. 자연어 처리 쿡북 with 파이썬. 에이콘. 2019.01.31에 기반을 두고 있다.</p>\n</blockquote>\n<br>\n<ul>\n<li>\n<p>편집 거리를 계산하기 위해선,</p>\n<ol>\n<li>자체 알고리즘 작성<br></li>\n<li><code class=\"language-text\">nltk.metrics.distance.edit_distance()</code>와 비교하여 온전성 검사 수행</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>metrics<span class=\"token punctuation\">.</span>distance <span class=\"token keyword\">import</span> edit_distance\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">my_edit_distance</span><span class=\"token punctuation\">(</span>str1<span class=\"token punctuation\">,</span> str2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token comment\">#두 개의 문자열을 입력 받는다</span>\n    <span class=\"token comment\"># 두 문자열의 길이를 구한다</span>\n    m <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>str1<span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token number\">1</span> \n    n <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>str2<span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token number\">1</span>\n    \n    <span class=\"token comment\"># mXn을 할 테이블을 만들고 첫 번째 행과 열을 초기화한다.</span>\n    table <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> table<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> i\n    <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> table<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> j\n        \n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>m<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            cost <span class=\"token operator\">=</span> <span class=\"token number\">0</span> <span class=\"token keyword\">if</span> str1<span class=\"token punctuation\">[</span>i<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> str2<span class=\"token punctuation\">[</span>j<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">else</span> <span class=\"token number\">1</span>\n            table<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">,</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token builtin\">min</span><span class=\"token punctuation\">(</span>table<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">,</span> j<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> table<span class=\"token punctuation\">[</span>i<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> j<span class=\"token punctuation\">]</span><span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> table<span class=\"token punctuation\">[</span>i<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> j<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token operator\">+</span>cost<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> table<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">,</span>j<span class=\"token punctuation\">]</span>\n\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Our Alorithm:\"</span><span class=\"token punctuation\">,</span> my_edit_distance<span class=\"token punctuation\">(</span><span class=\"token string\">\"hand\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"and\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"NLTK Alorithm:\"</span><span class=\"token punctuation\">,</span> edit_distance<span class=\"token punctuation\">(</span><span class=\"token string\">\"hand\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"and\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<ul>\n<li>\n<p>print(\"Our Alorithm:\", my<em>edit</em>distance(\"hand\",\"and\"))</p>\n<p>Our Alorithm: 1</p>\n</li>\n<li>\n<p>print(\"NLTK Alorithm:\", edit_distance(\"hand\",\"and\"))</p>\n<p>NLTK Alorithm: 1  </p>\n</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>cost는 str1과 str2가 동일하거나 편집됐는지, 삭제 또는 삽입인지에 따라 계산됨</p>\n<p>다음 행의 수식은 행렬에 있는 셀의 값을 계산하고 첫 번째 두 개는 대체를 처리하고 세 번째는 대체를 위한 것</p>\n<p>이전 단계의 비용을 추가하고 최소 세 단계를 취한다</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>0</th>\n<th>H</th>\n<th>A</th>\n<th>N</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td>A</td>\n<td>1</td>\n<td><strong>1</strong></td>\n<td><strong>1</strong></td>\n<td>3</td>\n</tr>\n<tr>\n<td>N</td>\n<td>2</td>\n<td><code class=\"language-text\">2</code></td>\n<td><strong>2</strong></td>\n<td>1</td>\n</tr>\n<tr>\n<td>D</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n<td>2</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p> ex: <strong>AN(열)~H(행): 같게 만들기 위해</strong> </p>\n<ol>\n<li>A를 삭제 2. N→H 치환. 따라서 <strong>2단계</strong> 취해서 </li>\n<li>action : 두 문자를 같게 만들기 위한 문자 1개를 1. 삭제 2. 치환 3. 삽입. 이때 최소 action의 개수를 table에 써줌</li>\n</ol>\n</blockquote>\n <br>\n<ol start=\"3\">\n<li>위에 굵은 글씨 (1,1/2,2) 설명하기 위해 따로 뺌</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>C1</th>\n<th>C2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>C1</td>\n<td><code class=\"language-text\">A</code></td>\n<td>B</td>\n</tr>\n<tr>\n<td>C2</td>\n<td>C</td>\n<td><code class=\"language-text\">D</code></td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>알고리즘:</li>\n<li>\n<p>C2 == C2</p>\n<p>→ D = A</p>\n<p>따라서 이동할 필요 없다. cost = 0</p>\n<blockquote>\n<p>cost = 0 if str1[i-1] == str2[j-1] else 1</p>\n</blockquote>\n<ul>\n<li>C2 =/= C2</li>\n</ul>\n<p> → A+1, B+1, C+1</p>\n<blockquote>\n<p>table[i,j] = min(table[i, j-1]+1, table[i-1, j]+1, table[i-1, j-1]+cost)</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<h3 id=\"code-classlanguage-texttf-idfcode\" style=\"position:relative;\"><a href=\"#code-classlanguage-texttf-idfcode\" aria-label=\"code classlanguage texttf idfcode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">TF-IDF</code></h3>\n<ul>\n<li>\n<p>Text 문서를 수치로 표현할 때</p>\n<ol>\n<li>단어의 빈도 기반(카운트 기반): <code class=\"language-text\">TF-IDF</code></li>\n<li><code class=\"language-text\">Embedding</code> 기반</li>\n</ol>\n</li>\n<li>\n<p><code class=\"language-text\">TF-IDF</code> 용어: </p>\n<ul>\n<li><code class=\"language-text\">TF</code>: 단어 빈도, term frequency</li>\n<li><code class=\"language-text\">DF</code>: 문서 빈도, document frequency</li>\n</ul>\n</li>\n<li><code class=\"language-text\">IDF</code>: DF값의 역수. 역문서 빈도, inverse document frequency</li>\n<li>TF-IDF 생성 순서</li>\n<li>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2604c3262d2bbba33e536e9ef82ffa85/d3b46/image-20200722100557200.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 41.891891891891895%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsSAAALEgHS3X78AAABaklEQVQoz02RW0/CQBCF++MFH33QfyHwYkJiTPRFHjQRIxAKFHrZ7fZioZclRim0x+m2Vtp8OXNmmtntjMYYQ7SN4Ae+0jiJkaSJIk0zyEwiI6TcN0jKJ3WuqZ2jOQ7Hz1eObZggjqhB+oX8u8D/U55xnkfjS3qL9httun7Fm32Pl9WwZWw9YCIecTe7Qn/aQf+jWzPpYDi/xlQ8kd6gN7lA772L23EHg+klBpMuNH05g8kX8LeMcBDsOMEgIguGNcfa1hV1vIBhz7FhutKVOSet46peeW25WCLepchSqZApzSnbq/kJ12vhzFXqcoHPIGpzlT9XzTRNNehEDTpTyL1Uy+Gcw3VdRbW8v1h4AkJQA7eqE6KGc0a/rOsIw5BulCKOY6VV04Q0CAK1fd/34Xle7ZvYYQ4syyH1ahwPti2gVSfleY7D4dBS+dPphLIsFUVRtPHxeFSH+n5AN1rB3IxgGM/YkK7XI/wCroZMutySfNUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200722100557200\"\n        title=\"image-20200722100557200\"\n        src=\"/static/2604c3262d2bbba33e536e9ef82ffa85/fcda8/image-20200722100557200.png\"\n        srcset=\"/static/2604c3262d2bbba33e536e9ef82ffa85/12f09/image-20200722100557200.png 148w,\n/static/2604c3262d2bbba33e536e9ef82ffa85/e4a3f/image-20200722100557200.png 295w,\n/static/2604c3262d2bbba33e536e9ef82ffa85/fcda8/image-20200722100557200.png 590w,\n/static/2604c3262d2bbba33e536e9ef82ffa85/efc66/image-20200722100557200.png 885w,\n/static/2604c3262d2bbba33e536e9ef82ffa85/c83ae/image-20200722100557200.png 1180w,\n/static/2604c3262d2bbba33e536e9ef82ffa85/d3b46/image-20200722100557200.png 1690w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<ol>\n<li><code class=\"language-text\">Vocabulary</code> 생성</li>\n<li>Term - Document Matrix(<code class=\"language-text\">TDM</code>) 생성</li>\n<li>Term : 행 / Document : 열</li>\n<li>행열 반전시킨 Document - Term Matrix(DTM) 으로도 가능</li>\n<li><strong>Term이 각 Document에 몇 번 쓰였는지 카운트</strong></li>\n<li>Term Frequency(<code class=\"language-text\">TF</code>) 계산</li>\n<li>TDM에서 <strong>문서길이 표준화</strong></li>\n<li><code class=\"language-text\">DF</code> 계산: <strong>각 Vocabulary가 총 몇 개의 Document에 쓰였는지 카운트</strong></li>\n<li>이때, 검색문서는 카운트에서 제외하고</li>\n<li><strong>단어가 쓰인 횟수가 아닌, Document 개수로 작성</strong></li>\n<li>DF 大 ~ 여러 문서에 나타나는 General한 단어 → 중요도 小</li>\n<li>\n<p>따라서 단어의 중요성 </p>\n<ul>\n<li>=1/DF → 반비례 관계</li>\n<li>=TF → 비례 관계</li>\n</ul>\n</li>\n<li>Inverse DF(<code class=\"language-text\">IDF</code>) (즉, = 1/DF)  계산</li>\n<li>DF에 log 취해서 계산</li>\n<li><strong>단어의 중요도는 IF에 비례, IDF에 비례</strong></li>\n</ol>\n</li>\n<li>TF*IDF = <code class=\"language-text\">TF-IDF</code></li>\n<li>\n<p>TF-IDF 상의 검색문 Vector와 Document Vector 값을 </p>\n<ol>\n<li>\n<p><strong>norm</strong>(l1,l2 등) </p>\n<ul>\n<li>l1: 벡터의 요소에 대한 절댓값의 합</li>\n<li>l2: 해당 차원의 좌표평면에서 원점에서 벡터 좌표까지의 최단거리</li>\n</ul>\n</li>\n<li>검색문과 각 Document의 내적 </li>\n<li>cosin 거리 </li>\n</ol>\n<p> 등으로 <strong>유사도 측정</strong></p>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<h2 id=\"code\" style=\"position:relative;\"><a href=\"#code\" aria-label=\"code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>code</h2>\n</li>\n<li>\n<p>sklearn 패키지 활용</p>\n<ul>\n<li>대표적인 Python 머신러닝 라이브러리에서는 문서 전처리용 클래스를 제공한다.</li>\n<li>기능</li>\n<li><code class=\"language-text\">DicVectorizer</code> : 단어의 수를 세어놓은 사전에서 BOW 벡터를 만든다.</li>\n<li><code class=\"language-text\">CountVectorizer</code>: 문서 집합으로부터 단어의 수를 세어 BOW 벡터를 만든다.</li>\n<li><code class=\"language-text\">Tfidfvectorizer</code>: 문서 집합으로부터 단어의 수를 세고 TF-IDF 방식으로 단어의 가중치를 조정한 BOW 벡터를 만든다.(CounterVectorizer의 서브클래스로 CountVectorizer를 이용해 BOW를 만들고 TfidTransformer를 사용해 tf-idf로 변환)</li>\n<li><code class=\"language-text\">HashingVectorizer</code>: hashing trick을 사용하여 빠르게 BOW 벡터를 만든다.</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> TfidfVectorizer</code></pre></div>\n<br>\n<ul>\n<li>\n<p>TF-IDF matrix를 생성한다</p>\n<ul>\n<li>아래 Code 中 <code class=\"language-text\">tfidf_vect_simple.fit_transform(statements)</code> 참고</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">statements <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>문장<span class=\"token number\">1</span><span class=\"token punctuation\">,</span> 문장<span class=\"token number\">2</span><span class=\"token punctuation\">,</span> 문장<span class=\"token number\">3</span><span class=\"token punctuation\">]</span>\nTfidfVectorizer<span class=\"token punctuation\">(</span>max_features <span class=\"token operator\">=</span> <span class=\"token number\">500</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 빈도 높은 순으로 500개 단어만 # 변환된 행렬은 희소 행렬</span>\nwords <span class=\"token operator\">=</span> tfidf_vect_simple<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>statements<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 각 문서들에 대한 단어들이 나오는 빈도수를 log 처리 시켜서 table로 만듦(행렬로 만듦) # transform 써서 단어가 feature(열)로 바뀜 </span>\n\nvocab <span class=\"token operator\">=</span> tf_vector<span class=\"token punctuation\">.</span>get_feature_names<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># shape=(500,) 단어 500개.</span></code></pre></div>\n<br>\n<ul>\n<li>cosin 분석한다</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics<span class=\"token punctuation\">.</span>pairwise <span class=\"token keyword\">import</span> cosine_similarity\ncosine_similarity<span class=\"token punctuation\">(</span>words<span class=\"token punctuation\">,</span>words<span class=\"token punctuation\">)</span> <span class=\"token comment\"># tf-idf 처리된 행렬로 word 전체의 cosin-distance 구함</span></code></pre></div>\n<br>\n<br>\n<h3 id=\"code-classlanguage-text코사인-유사도code거리\" style=\"position:relative;\"><a href=\"#code-classlanguage-text%EC%BD%94%EC%82%AC%EC%9D%B8-%EC%9C%A0%EC%82%AC%EB%8F%84code%EA%B1%B0%EB%A6%AC\" aria-label=\"code classlanguage text코사인 유사도code거리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">코사인 유사도</code>(거리)</h3>\n<ul>\n<li>\n<p>벡터와 벡터 간의 유사도를 두 벡터 간의 각도로 나타낸 것</p>\n<ul>\n<li>\n<p>각도=<strong>방향</strong>인 셈.</p>\n<blockquote>\n<ol>\n<li>방향이 비슷할수록 두 벡터는 서로 유사하며, </li>\n<li>벡터 방향이 90도 일때는 두 벡터 간의 관련성이 없으며, </li>\n<li>벡터 방향이 반대가 될수록 두 벡터는 반대 관계</li>\n</ol>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://blog.kakaocdn.net/dn/cluYST/btqBUUjgX1Z/6j4dN9FjuoNhIvKhA9F19k/img.png\" alt=\"img\"></p>\n<blockquote>\n<p> 그림 출처: 딥 러닝을 이용한 자연어 처리 입문</p>\n</blockquote>\n<br>\n<ul>\n<li>\n<p>유클리디안 거리와 달리, cosin distance(유사도)는 방향만 고려하기 때문에 개인 맞춤 추천 솔루션(협업 필터링)에 활용된다.</p>\n<ul>\n<li>협업 필터링 기법에서는 한 '제품' 을 하나의 벡터로 취급한다.</li>\n<li>세부 특징 하나하나가 벡터가 되지 않는단 것이다</li>\n</ul>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<blockquote>\n<p>출처 및 참고:</p>\n<ul>\n<li>데이터 파수꾼 Baek Kyun Shin. 2020. 2. 17. \"NLP - 8. 코사인 유사도(Cosine Similarity)\". <a href=\"https://bkshin.tistory.com/entry/NLP-8-%EB%AC%B8%EC%84%9C-%EC%9C%A0%EC%82%AC%EB%8F%84-%EC%B8%A1%EC%A0%95-%EC%BD%94%EC%82%AC%EC%9D%B8-%EC%9C%A0%EC%82%AC%EB%8F%84\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://bkshin.tistory.com/entry/NLP-8-문서-유사도-측정-코사인-유사도</a></li>\n<li>데이타광 DNA구너. 2020. 5. 20. \"[스팀 2부 - 이론] 협업 필터링 - 스팀, 넷플릭스, 아마존이 당신을 사로잡기 위해 부리는 마법 이해하기\". <a href=\"https://dnagooner.tistory.com/51\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://dnagooner.tistory.com/51</a></li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<br>\n<br>\n<hr>\n<p><br><br></p>\n<ul>\n<li>\n<h3 id=\"text를-수치-vector로-표현할-때\" style=\"position:relative;\"><a href=\"#text%EB%A5%BC-%EC%88%98%EC%B9%98-vector%EB%A1%9C-%ED%91%9C%ED%98%84%ED%95%A0-%EB%95%8C\" aria-label=\"text를 수치 vector로 표현할 때 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Text를 수치 Vector로 표현할 때</h3>\n</li>\n<li>\n<p>통계적 기반, 빈도 기반, 카운트 기반:</p>\n<ul>\n<li><code class=\"language-text\">TF-IDF</code>: </li>\n<li>Vocabulary(Dictionary) 생성</li>\n<li>Term - Document Matrix(<code class=\"language-text\">TDM</code>) 생성</li>\n<li>Term Frequency(<code class=\"language-text\">TF</code>) 계산</li>\n<li><code class=\"language-text\">DF</code> 작성: <strong>각 Vocabulary가 총 몇 개의 Document에 쓰였는지 카운트</strong></li>\n<li>Inverse DF(<code class=\"language-text\">IDF</code>) (즉, = 1/DF) 작성</li>\n<li>\n<p>TF*IDF = <code class=\"language-text\">TF-IDF</code></p>\n<ul>\n<li>나아가, TF-IDF 상의 검색문 Vector와 Document Vector 값을 가공하여 유사도 측정</li>\n</ul>\n<br>\n</li>\n<li><strong><code class=\"language-text\">BOW(Bag of word)</code></strong>: </li>\n<li>쿡북 p. 235~</li>\n<li>방법: 수치로 형상화하여 추출 시 사용 </li>\n<li>\n<p><code class=\"language-text\">doc2bow</code> 생성</p>\n<p>ex: I love you very much, you love me too.</p>\n<ol>\n<li>Vocabulary(Dictionary) 생성</li>\n</ol>\n<blockquote>\n<p>여러 문서로 부터 생성함</p>\n<p>I : 0</p>\n<p>love : 1</p>\n<p>you : 2</p>\n</blockquote>\n<blockquote>\n<p>...(이하 생략)</p>\n</blockquote>\n<ol start=\"2\">\n<li>문서를 doc2bow로 생성</li>\n</ol>\n<blockquote>\n<ol>\n<li>\n<p>bow: 워드를 수치화(I love you~를 Vocabulary에서 정의한 숫자로 적어줌)</p>\n<p>[0, 1, 2, ... 2, 1, ...] ← vector</p>\n</li>\n<li>\n<p>Doc2Bow: 수치화된 워드별 빈도</p>\n<p>[(0,1), (1,2), (2,2), ...] ← vector</p>\n</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>  (0,1)에서 '1' = 빈도 </p>\n</blockquote>\n<ol start=\"3\">\n<li>LDA 모형 사용 </li>\n<li>문서별 Topic 번호 확인</li>\n<li>topic별 문서 확인 </li>\n</ol>\n  <br>\n</li>\n<li><code class=\"language-text\">TF-IDF</code>와 <code class=\"language-text\">BOW</code>의 차이점:</li>\n<li>\n<p>Vocabulary 만들고 시작하는 건 같지만, </p>\n<ul>\n<li>\n<ol>\n<li>BOW는 TF까지, TF-IDF는 IDF와 TF*IDF까지 구함</li>\n</ol>\n</li>\n<li>\n<p>BOW의 TF와 TF-IDF의 TF값은 다름</p>\n<ul>\n<li>\n<p>이유: TF-IDF의 TF는 표준화 시킨 값이고, BOW의 TF는 빈도만 세었기 때문</p>\n<blockquote>\n<p>TF-IDF:</p>\n<p><strong>어떤 단어가 하나의 문서에서도 많이 사용되었다고 하더라도, 다른 모든 문서에서 널리 쓰이는 흔해 빠진 단어라면 이 단어는 특정성(specificity)이 떨어지는 것이다.</strong></p>\n<p>그래서 단순 단어 빈도로 접근하는 게 아니라, 어떤 단어가 한 문서에서 많이 나타난 동시에 다른 문서에서는 잘 나타나지 않는 것까지 고려하기 위한 개념이 등장하는 데 이게 바로 <strong>TF-IDF(Term Frequency-Inverse Document Frequency)</strong>다. </p>\n<p>아무튼 TF-IDF는 단순한 단어 빈도가 아니라 일종의 가중치를 적용한 개념이라고 이해하면 된다. 그래서 이 TF-IDF를 활용해서 문서-단어 행렬을 만들고 분석을 하는 경우도 매우 많다.</p>\n<p>그래서 파이썬 라이브러리 scikit-learn에서는 아예 단순 빈도로 접근하는 <code class=\"language-text\">CountVectorizer</code>말고, TF-IDF로 접근하는 <code class=\"language-text\">TfidfVectorizer</code>클래스를 제공하기도 한다.</p>\n<ul>\n<li>출처: 아무튼워라밸. 2020.01.24. \"Bag-of-Words(BoW) 쉽게 이해하기\". <a href=\"http://hleecaster.com/nlp-bag-of-words-concept/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">http://hleecaster.com/nlp-bag-of-words-concept/</a></li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</li>\n<li>따라서 BOW와 TF-IDF의 쓰임새는 상황에 따라 다르다.</li>\n</ul>\n <br>\n</li>\n<li>\n<p><code class=\"language-text\">BOW</code>: 주어진 말뭉치에서 사용된 각각의 단어의 빈도를 센다</p>\n<blockquote>\n<p>주어진 말뭉치에서 사용된 각각의 단어에 인덱스를 매겨서 사전처럼 만든다. (==Vocabulary==)</p>\n<p>그리고 입력된 문서의 각 단어가 해당 단어 사전(단어 주머니)에서 해당 인덱스에 얼마나 나타나는지 표시하는 방식으로 입력된 문장을 분석한다. (==TF==)</p>\n</blockquote>\n<br>\n</li>\n<li>\n<p><code class=\"language-text\">TF-IDF</code>: <code class=\"language-text\">BOW</code>와 같이 각각의 문서에서 단어의 개수를 세는 것은 같다. 그치만 여기에 전체 말뭉치(Corpus)에서 단어의 갯수도 함께 센다.</p>\n<blockquote>\n<p>특정 단어가 문서 내에 얼마나 자주 등장하는 지를 나타내는 ==TF(단어 빈도)==와</p>\n<p>어떤 단어가 문서 전체 집합에서 얼마나 많이 나오는지를 나타내는 ==IDF(역문서 빈도)==로 </p>\n<p>이 둘의 곱으로 TF-IDF를 값을 구할 수 있다.</p>\n</blockquote>\n<br>\n</li>\n</ul>\n</li>\n<li>\n<p>Embedding 기반</p>\n<ul>\n<li>Word Embedding: 벡터화</li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<h3 id=\"code-classlanguage-textbowcode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textbowcode\" aria-label=\"code classlanguage textbowcode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Bow</code></h3>\n<ul>\n<li>code에 <code class=\"language-text\">TF-IDF</code> 없음</li>\n<li>\n<p>한계: </p>\n<blockquote>\n<p>Bow 모델은 그 빈도만 세기 때문에 맥락을 충분히 고려해야 하는 상황, 즉 <strong>텍스트를 생성한다거나 예측하는 등의 장면에는 활용이 어렵다.</strong></p>\n<p>게다가 학습된 단어 사전을 기반으로 하기 때문에 사전에 없는 새로운 단어가 나타났을 때 그걸 처리할 방법이 없다. 학습 데이터에 지나치게 의존하기 때문에 <strong>오버피팅(overfitting)</strong>이 발생하는 거다.</p>\n<p>출처: 아무튼워라밸. 2020.01.24. \"Bag-of-Words(BoW) 쉽게 이해하기\". <a href=\"http://hleecaster.com/nlp-bag-of-words-concept/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">http://hleecaster.com/nlp-bag-of-words-concept/</a></p>\n</blockquote>\n<br>\n</li>\n<li>\n<h2 id=\"code-1\" style=\"position:relative;\"><a href=\"#code-1\" aria-label=\"code 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Code:</h2>\n<ul>\n<li><code class=\"language-text\">gensim</code> 패키지 활용</li>\n<li><strong>문서 사이의 유사도 계산과 텍스트 분석을 돕는</strong> 라이브러리</li>\n<li>\n<p>기능</p>\n<ul>\n<li><strong>Topic Modeling</strong></li>\n<li><code class=\"language-text\">LDA(Latent Dirichlet Allocation)</code></li>\n<li>LSI(Latent Semantic Indexing)</li>\n<li>HDP(Hierarchical Dirichlet Process)</li>\n<li><strong>Word Embedding</strong></li>\n<li><code class=\"language-text\">word2Vec</code></li>\n</ul>\n</li>\n<li>Latent Dirichlet Allocation (<code class=\"language-text\">LDA</code>) using <code class=\"language-text\">gensim</code></li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> re\n<span class=\"token keyword\">import</span> pickle\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>corpus <span class=\"token keyword\">import</span> stopwords\n<span class=\"token operator\">**</span><span class=\"token keyword\">from</span> gensim <span class=\"token keyword\">import</span> corpora<span class=\"token operator\">**</span>\n<span class=\"token operator\">**</span><span class=\"token keyword\">from</span> gensim<span class=\"token punctuation\">.</span>models<span class=\"token punctuation\">.</span>ldamodel <span class=\"token keyword\">import</span> LdaModel <span class=\"token keyword\">as</span> LDA<span class=\"token operator\">**</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 1. 저장된 news data를 읽어온다.</span>\n<span class=\"token comment\"># 2. 첫 번째 news를 조회해 본다.</span>\n<span class=\"token comment\"># 3. news 별로 분류된 target을 확인해 본다.</span>\n  \n<span class=\"token comment\"># 4. preprocessing.</span>\n<span class=\"token comment\"># 4-1. 영문자가 아닌 문자를 모두 제거한다.</span>\n<span class=\"token comment\"># 4-2. 불용어를 제거하고, 모든 단어를 소문자로 변환하고, 길이가 3 이하인 단어를 제거한다</span></code></pre></div>\n<blockquote>\n<p>밑에 주제식별 > LSA , LDA code 참고</p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># doc2bow 생성</span>\n<span class=\"token comment\">## vocabulary </span>\nvocab <span class=\"token operator\">=</span> corpora<span class=\"token punctuation\">.</span>Dictionary<span class=\"token punctuation\">(</span>news2<span class=\"token punctuation\">)</span>\n<span class=\"token builtin\">dict</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># list-> dic 변환하는 거 복습 시 다시 확인하기</span></code></pre></div>\n<blockquote>\n<p>{0: 'acts',\n1: 'atrocities',\n2: 'austria',\n3: 'away',\n4: 'biased',\n5: 'blessing',\n6: 'clearly',\n7: 'commited',\n8: 'daily',\n9: 'degree'}</p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">## bow &amp; doc2bow</span>\nnews_bow <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>vocab<span class=\"token punctuation\">.</span>doc2bow<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> s <span class=\"token keyword\">in</span> news2<span class=\"token punctuation\">]</span> \n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>news_bow<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 4), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1)]</p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Latent Dirichlet Allocation (LDA)</span>\n<span class=\"token comment\"># ---------------------------------</span>\nmodel <span class=\"token operator\">=</span> LDA<span class=\"token punctuation\">(</span>news_bow<span class=\"token punctuation\">,</span>\n          num_topics <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>newsData<span class=\"token punctuation\">.</span>target_names<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># len(newsData.target_names) = 20</span>\n          id2word<span class=\"token operator\">=</span>vocab<span class=\"token punctuation\">)</span> <span class=\"token comment\"># vocab = vocabulary 만든 것 </span></code></pre></div>\n<br>\n<ul>\n<li>이하부턴 TF-IDF의 LDA 코드와 동일함</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 문서 별 Topic 번호를 확인한다. (문서 10개만 확인)</span>\ndoc_topic <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>get_document_topics<span class=\"token punctuation\">(</span>news_bow<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    dp <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>doc_topic<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    most_likely_topic <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>dp<span class=\"token punctuation\">[</span>np<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>dp<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'문서-{:d} : topic = {:d}'</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> most_likely_topic<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>문서-0 : topic = 3\n문서-1 : topic = 3\n문서-2 : topic = 3\n문서-3 : topic = 2\n문서-4 : topic = 3\n문서-5 : topic = 3\n문서-6 : topic = 14\n문서-7 : topic = 2\n문서-8 : topic = 2\n문서-9 : topic = 5</p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># topic_term 행렬에서 topic 별로 중요 단어를 표시한다</span>\ntopic_term <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>get_topic_terms<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> topn<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>newsData<span class=\"token punctuation\">.</span>target_names<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  topic_term <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>get_topic_terms<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> topn<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n  idx <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>idx <span class=\"token keyword\">for</span> idx<span class=\"token punctuation\">,</span> score <span class=\"token keyword\">in</span> topic_term<span class=\"token punctuation\">]</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'토픽-{:2d} : '</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> end<span class=\"token operator\">=</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">for</span> n <span class=\"token keyword\">in</span> idx<span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'{:s} '</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">[</span>n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> end<span class=\"token operator\">=</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>토픽- 1 : would keyboard program problem like number medical also health available\n토픽- 2 : available file program window information mail version server info thanks\n토픽- 3 : would people like think know time could well something make\n토픽- 4 : would people think know jesus even like said time believe\n토픽- 5 : state would pain many like people case could also court </p>\n<p>....</p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 문서별로 분류된 코드를 확인해 본다.</span>\n<span class=\"token comment\"># x, y : 문서 번호</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">checkTopic</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"문서 %d의 topic = %s\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> newsData<span class=\"token punctuation\">.</span>target_names<span class=\"token punctuation\">[</span>newsData<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"문서 %d의 topic = %s\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">,</span> newsData<span class=\"token punctuation\">.</span>target_names<span class=\"token punctuation\">[</span>newsData<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">[</span>y<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\ncheckTopic<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span>\ncheckTopic<span class=\"token punctuation\">(</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">9</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>문서 2의 topic = talk.politics.mideast\n문서 5의 topic = soc.religion.christian\n문서 7의 topic = talk.politics.mideast\n문서 9의 topic = sci.electronics</p>\n</blockquote>\n<br>\n<br>\n<hr>\n<br>\n<br>\n<h2 id=\"주제-식별-topic-model\" style=\"position:relative;\"><a href=\"#%EC%A3%BC%EC%A0%9C-%EC%8B%9D%EB%B3%84-topic-model\" aria-label=\"주제 식별 topic model permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>주제 식별 (Topic Model)</h2>\n<ul>\n<li>주제별 clustering 실행<br></li>\n<li>\n<p>수많은 text document 를 주제별로 군집화(clustering)</p>\n<ul>\n<li>ex: 문학/경제 ... 등의 주제별로 나눔<br></li>\n</ul>\n</li>\n<li>\n<p>대표 알고리즘: <code class=\"language-text\">LDA</code></p>\n<ul>\n<li>ML:</li>\n<li>SL(지도학습)</li>\n<li>UL(비지도학습)</li>\n<li>통계적 추론: <code class=\"language-text\">HMM</code>(<code class=\"language-text\">Baum welch</code> 등), <strong><code class=\"language-text\">LDA 모형</code></strong></li>\n<li>RL(강화학습)<br></li>\n</ul>\n</li>\n<li>Topic Model 발전 순서: SVD → LSA 모형 → LDA 알고리즘 </li>\n</ul>\n<br>\n<hr>\n<br>\n<h3 id=\"topic-modellsa-vs-lda\" style=\"position:relative;\"><a href=\"#topic-modellsa-vs-lda\" aria-label=\"topic modellsa vs lda permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Topic Model(LSA vs LDA)</h3>\n<ul>\n<li>\n<p><code class=\"language-text\">LSA</code></p>\n<ul>\n<li>SVD 원리</li>\n</ul>\n</li>\n<li>\n<p><code class=\"language-text\">LDA</code></p>\n<ul>\n<li><strong>결합확률 분포</strong> 취급</li>\n<li>문서, 단어 조합이 많아 결합확률 분포가 굉장히 복잡하기 때문에 <code class=\"language-text\">Sampling 기법</code>으로 <strong>근사적으로 계산</strong>함(특히 <code class=\"language-text\">Gibb&#39;s sampling 기법</code> 사용)</li>\n</ul>\n</li>\n<li>\n<blockquote>\n<p><code class=\"language-text\">LSA</code> : ==DTM을 <strong>차원 축소</strong> 하여 축소 차원에서 근접 단어들을 <strong>토픽으로 묶는다.</strong>==\n<code class=\"language-text\">LDA</code> : ==단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 <strong>결합확률</strong>로 추정하여 <strong>토픽을 추출한다.</strong>==</p>\n<ul>\n<li>출처: <a href=\"https://wikidocs.net/30708\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">딥 러닝을 이용한 자연어 처리 입문, \"2) 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)\"</a></li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<br>\n<h3 id=\"code-classlanguage-textsvdcode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textsvdcode\" aria-label=\"code classlanguage textsvdcode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">SVD</code></h3>\n<ul>\n<li>\n<p>차원축소</p>\n<ol>\n<li><code class=\"language-text\">PCA</code></li>\n<li>고유값 분해</li>\n<li><code class=\"language-text\">SVD</code></li>\n<li>특이값 분해, Topic Model에도 쓰임(특히 LSA: 잠재 의미 분석)</li>\n</ol>\n</li>\n<li>PCA, SVD 모두 sklearn에서 사용할 수 있음</li>\n</ul>\n<br>\n<ul>\n<li>\n<h2 id=\"code-2\" style=\"position:relative;\"><a href=\"#code-2\" aria-label=\"code 2 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Code:</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> TfidfVectorizer</code></pre></div>\n<br>\n```python\n# TF-IDF matrix를 SVD로 분해한다.\n# C = U.S.VT\nstatements = [\n          'ruled india',\n          'Chalukyas ruled Badami',\n          'So many kingdoms ruled India',\n          'Lalbagh is a botanical garden in India'\n      ]\n```\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># TF-IDF matrix를 생성한다.</span>\ntf_vector <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span>max_features <span class=\"token operator\">=</span> <span class=\"token number\">8</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># max_features = 8 최대 단어 개수(빈도 높은 8개 단어만 추출)</span>\ntfidf <span class=\"token operator\">=</span> tf_vector<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>statements<span class=\"token punctuation\">)</span> <span class=\"token comment\">#.toarray 써주면 배열 형태로 출력 </span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tfidf<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n```python\n# SVD (Singular Vector Decomposition)으로 TF-IDF를 분해한다.\n# U, S, VT 행렬의 의미 --> Latent Semantic Analysis (LSA)\n# U 행렬 ~ 차원 = (문서 개수 X topic 개수) : 문서당 topic 분포\n# S 행렬 ~ 차원 = (topic 개수 X topic 개수) : 대각성분. 나중에 행렬에 넣을 땐 대각성분만 빼면 0\n# VT 행렬. 차원 = (topic 개수 X 단어 개수) : topic 당 단어 빈도 (분포)\nU, S, VT = np.linalg.svd(tfidf.toarray(), full_matrices = True)\n<p>print(U.round(2), '\\n') # 모든 열벡터들의 내적 = 0\nprint(S.round (2), '\\n') # 왼->오로 갈 수록 수치가 작아짐. 따라서 성분이 큰 것(=수치가 큰 것)이 앞에 나옴\nprint(VT.round(2), '\\n')</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;br&gt;\n&gt; **print(U.round(2), &#39;\\n&#39;)**\n&gt;\n&gt; [[ 0.65 -0.   -0.02  0.76]\n&gt;  [ 0.33 -0.79 -0.42 -0.29]\n&gt;  [ 0.53 -0.    0.72 -0.44]\n&gt;  [ 0.43  0.61 -0.55 -0.38]] \n&lt;br&gt;\n&gt; **print(S.round (2), &#39;\\n&#39;)**\n&gt;\n&gt; [1.34 1.   0.88 0.66] \n&lt;br&gt;\n&gt; **print(VT.round(2), &#39;\\n&#39;)**\n&gt; [[ 0.16  0.16  0.65  0.2   0.27  0.2   0.57  0.2 ]\n&gt;  [-0.51 -0.51  0.33 -0.    0.51 -0.   -0.33 -0.  ]\n&gt;  [-0.31 -0.31 -0.08  0.42 -0.52  0.42  0.06  0.42]\n&gt;  [-0.28 -0.28  0.29 -0.34 -0.48 -0.34  0.42 -0.34]\n&gt;  [ 0.24 -0.45 -0.33  0.56  0.21 -0.28  0.33 -0.28]\n&gt;  [ 0.25 -0.35 -0.16 -0.41  0.1   0.71  0.16 -0.29]\n&gt;  [-0.6   0.3  -0.47 -0.12  0.3   0.06  0.47  0.06]\n&gt;  [ 0.25 -0.35 -0.16 -0.41  0.1  -0.29  0.16  0.71]] \n&lt;br&gt;\n\n```python\n# S를 행렬 형태로 변환한다.\ns = np.zeros(tfidf.shape)\ns[:S.shape[0], :S.shape[0]] = np.diag(S) # 대각선에 정방행렬(S)를 집어 넣어라 \nprint(s.round(2), &#39;\\n&#39;)</code></pre></div>\n<br>\n<blockquote>\n<p><strong>print(s.round(2), '\\n')</strong>\n[[1.34 0.   0.   0.   0.   0.   0.   0.  ]\n[0.   1.   0.   0.   0.   0.   0.   0.  ]\n[0.   0.   0.88 0.   0.   0.   0.   0.  ]\n[0.   0.   0.   0.66 0.   0.   0.   0.  ]] </p>\n</blockquote>\n</li>\n</ul>\n<br>\n<h3 id=\"code-classlanguage-textlsacode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textlsacode\" aria-label=\"code classlanguage textlsacode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">LSA</code></h3>\n<ul>\n<li>LSA : 잠재 의미 분석 </li>\n<li>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b9872c633b3e4a7f22f8c0e9e281b945/78958/image-20200722094819751.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 52.70270270270271%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAABl0lEQVQoz2WT6XLCMAyE/f5v2B8tpSFNgNzORRJQ93Njhk6Z0TiWVseujLtcLna9Xq0oCquqyuq6DmdZluGbs21bezweFn+v39M0Wdd11ve9zfNsDscwDP+A/O73+/PEiEfMtm22LMsfLDF3u90Mo/o4jgFEk+jHon9d1zA9rLDz+RzYcXIH47gAgjIOkqEIXSb33gdKNKQ4sSzLLEkSS9PUTqeTHY9Hy/M8TPmckGI4oMIk0cc9+jCmb5rGovZhQhVDa3COKVoBvO9CsFKAhFmJJDMp0zMZ/kHiv2pKk3mXLBRkklXB27KFouMoass+jUA0pDDJJLyubdU96K8mozCLctxZGlTHL+ta/7vpdbMqkU86+V1ouseNbmgqfyPt889Pq4WbxADfJDauVKBRwIvOvv8AmHQnaVYhlsKy7lAU9fzjw7Ld0vd3yw4HS97erJdkbpJmo0Bss+Axy3ptvd/PQV294hQMDGS9sFeejhqyEE42jzyu5onIWEj6/W0XWSkZCkxPI5gSaFbuxpI44z+q3J8di/sBVyJODi8AubUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200722094819751\"\n        title=\"image-20200722094819751\"\n        src=\"/static/b9872c633b3e4a7f22f8c0e9e281b945/fcda8/image-20200722094819751.png\"\n        srcset=\"/static/b9872c633b3e4a7f22f8c0e9e281b945/12f09/image-20200722094819751.png 148w,\n/static/b9872c633b3e4a7f22f8c0e9e281b945/e4a3f/image-20200722094819751.png 295w,\n/static/b9872c633b3e4a7f22f8c0e9e281b945/fcda8/image-20200722094819751.png 590w,\n/static/b9872c633b3e4a7f22f8c0e9e281b945/efc66/image-20200722094819751.png 885w,\n/static/b9872c633b3e4a7f22f8c0e9e281b945/c83ae/image-20200722094819751.png 1180w,\n/static/b9872c633b3e4a7f22f8c0e9e281b945/78958/image-20200722094819751.png 1320w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<br>\n</li>\n<li>\n<h2 id=\"code-3\" style=\"position:relative;\"><a href=\"#code-3\" aria-label=\"code 3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>code</h2>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> re\n<span class=\"token keyword\">import</span> pickle\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>corpus <span class=\"token keyword\">import</span> stopwords\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> TfidfVectorizer\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>decomposition <span class=\"token keyword\">import</span> TruncatedSVD</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 저장된 news data를 읽어온다.</span>\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'./dataset/news.data'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'rb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    newsData  <span class=\"token operator\">=</span> pickle<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 첫 번째 news를 조회해 본다.</span>\nnews <span class=\"token operator\">=</span> newsData<span class=\"token punctuation\">.</span>data\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>news<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> \n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>news<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>11314</p>\n<p>Well i'm not sure about the story nad it did seem biased. What...</p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># news 별로 분류된 target을 확인해 본다.</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>newsData<span class=\"token punctuation\">.</span>target_names<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>newsData<span class=\"token punctuation\">.</span>target_names<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', ...</p>\n<p>20</p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># preprocessing.</span>\n<span class=\"token comment\"># 1. 영문자가 아닌 문자를 모두 제거한다.</span>\nnews1 <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> doc <span class=\"token keyword\">in</span> news<span class=\"token punctuation\">:</span>\n    news1<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>re<span class=\"token punctuation\">.</span>sub<span class=\"token punctuation\">(</span><span class=\"token string\">\"[^a-zA-Z]\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">,</span> doc<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    \n<span class=\"token comment\"># 2. 불용어를 제거하고, 모든 단어를 소문자로 변환하고, 길이가 3 이하인 단어를 제거한다</span>\nstop_words <span class=\"token operator\">=</span> stopwords<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span><span class=\"token string\">'english'</span><span class=\"token punctuation\">)</span>\nnews2 <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> doc <span class=\"token keyword\">in</span> news1<span class=\"token punctuation\">:</span>\n    doc1 <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> doc<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        w <span class=\"token operator\">=</span> w<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>w<span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">3</span> <span class=\"token keyword\">and</span> w <span class=\"token keyword\">not</span> <span class=\"token keyword\">in</span> stop_words<span class=\"token punctuation\">:</span>\n            doc1<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>w<span class=\"token punctuation\">)</span>\n    news2<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>doc1<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    \n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>news2<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>well sure story seem biased disagree statement media ruin israels reputation...</p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># TF-IDF matrix를 생성한다.</span>\ntf_vector <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span>max_features <span class=\"token operator\">=</span> <span class=\"token number\">500</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 변환된 행렬은 희소 행렬</span>\ntfidf <span class=\"token operator\">=</span> tf_vector<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>news2<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tfidf<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span> <span class=\"token comment\"># (11314, 500)</span>\n\nvocab <span class=\"token operator\">=</span> tf_vector<span class=\"token punctuation\">.</span>get_feature_names<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># shape=(500,)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">20</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>['able', 'access', 'actually', 'address', 'advance', 'agree', 'allow', 'almost', 'already', 'also', 'although', 'always', 'american', 'among', 'anonymous', 'another', 'answer', 'anti', 'anybody', 'anyone']</p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Latent Semantic Analysis (LSA)</span></code></pre></div>\n<br>\n```python\nsvd = TruncatedSVD(n_components = len(newsData.target_names), n_iter=1000) # svd.shape = (20,)\nsvd.fit(tfidf) # tfidf.shape = (11314, 500)\n<p>U = svd.fit<em>transform(tfidf) / svd.singular</em>values_ # svd.fit<em>transform(tfidf) = (11314, 20) # svd.singular</em>values_ = (20,) # U.shape = (11314, 20)\nVT = svd.components_ # VT.shape = (20,500) # VT: 직교행렬(내적 0 이라 서로 독립적이다)\nS = np.diag(svd.singular<em>values</em>) # S.shape = (20, 20) # S: 대각 성분을 제외한 원소는 모두 0 </p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;br&gt;\n```python\n# 문서 별 Topic 번호를 확인한다. (문서 10개만 확인)\nfor i in range(10):\n    print(&#39;문서-{:d} : topic = {:d}&#39;.format(i, np.argmax(U[i:(i+1), :][0])))\n    ## 해석: 0문서엔 17번이란 이름으로 부여된 topic(단어..)가 있고, 2문서에도 같은 게 있다. 그럼 0문서랑 2문서랑 clustering 해도 되겠다 &lt; 라고 판단하는 알고리즘임 \n    \n# VT 행렬에서 topic 별로 중요 단어를 표시한다 \n# topic: 주요 성분(내림차순 배열). &quot;tf_vector = TfidfVectorizer(max_features = 500)&quot; 해서 자주 나오는 단어만 500개 남겼잖슴? 거기서 남긴 500개가 주요성분이란 뜻이고 그게 즉 topic임 \nfor i in range(len(VT)):\n    idx = np.flipud(VT[i].argsort())[:10] # np.flipud(): 위아래 반전 # .argsort(): 오름차순 배열 =&gt; 문서순이 아니라 토픽순으로 정렬하겠단 뜻 =&gt; flipud+argsort: 따라서 내림차순 배열이 됨 \n    print(&#39;토픽-{:2d} : &#39;.format(i+1), end=&#39;&#39;)\n    for n in idx:\n        print(&#39;{:s} &#39;.format(vocab[n]), end=&#39;&#39;) # 토픽을 차원축소=묶었으므로 vocab[n] 하면 응축된(topic)이 1개씩 나오게 됨 \n    print()</code></pre></div>\n<br>\n```python\nnewsData.keys()\n```\n<blockquote>\n<p>dict<em>keys(['data', 'filenames', 'target</em>names', 'target', 'DESCR'])</p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 문서별로 분류된 코드를 확인해 본다.</span>\n<span class=\"token comment\"># x, y : 문서 번호</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">checkTopic</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"문서 %d의 topic = %s\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> newsData<span class=\"token punctuation\">.</span>target_names<span class=\"token punctuation\">[</span>newsData<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"문서 %d의 topic = %s\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">,</span> newsData<span class=\"token punctuation\">.</span>target_names<span class=\"token punctuation\">[</span>newsData<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">[</span>y<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\ncheckTopic<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">)</span>\ncheckTopic<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<br>\n<h3 id=\"code-classlanguage-textldacode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textldacode\" aria-label=\"code classlanguage textldacode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">LDA</code></h3>\n<ul>\n<li>잠재 디디클레 할당</li>\n<li>비지도학습</li>\n<li>\n<p>LDA를 사용함으로써 문서 내에서 토픽을 찾을 수 있다.</p>\n<blockquote>\n<p>LDA의 가장 큰 장점은 각각의 문서를 토픽들의 집합으로 본것이다. (즉 Document는 Mixture of Topic이라는 말이다.)</p>\n<p>만약 내가 \"김제동, 외압으로 '김제동 쇼'하차\" 라는 기사를 본다고 생각해보자.</p>\n<p>이 기사에는 김제동이 어떤 정치적 이유 때문에 김제동 쇼에서 하차하게 되었다는 내용이 적혀있을것이다.  일반적으로 '김제동'이라는 단어를 '연예'라는 토픽에 속해 있다고 봤을때 이 기사는 '연예'라는 토픽만을 다룬 것이 아니라 '외압' 즉 어떤 '정치'에 연관된 토픽 또한 다루고 있다.</p>\n<p>이와 같이 모든 문서는 하나의 토픽에만 속하는 것이 아니라 두세가지의 다른 토픽들의 혼합(Mixture)으로 정의 할 수 있는데 이것이 LDA가 가지는 가장 큰 가설중 하나이다.</p>\n<ul>\n<li>출처: arongdari. 2010. 6. 2. \"Latent Dirichlet Allocation\". <a href=\"https://arongdari.tistory.com/entry/Latent-Dirichlet-Allocation\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arongdari.tistory.com/entry/Latent-Dirichlet-Allocation</a>. </li>\n</ul>\n</blockquote>\n<br>\n</li>\n<li>\n<p>현실: 오로지 문서만 주어지고, Topic은 주어지지 않아 추정해서 문서를 해당 Topic으로 할당해야 한다.</p>\n<ul>\n<li>추정 방법: 두 개의 다항분포를 측정하고, 해당 문서를 특정 Topic에 할당</li>\n<li>\n<ol>\n<li>Topic의 문서 분포</li>\n<li>문서의 Topic 분포<br></li>\n</ol>\n</li>\n</ul>\n</li>\n<li>\n<p>문서 집합에서 Topic 분포 등을 추정</p>\n<ul>\n<li>추정 순서:</li>\n<li>\n<ol>\n<li>문서 집합에서</li>\n<li>토픽 추출</li>\n<li>토픽 추정</li>\n<li>토픽의 주요 단어 추정</li>\n<li>(1) 토픽별 문서 분포 추출 </li>\n</ol>\n<p> (2) 문서별 토픽 분포 추출</p>\n<ol start=\"5\">\n<li>시간대 별 토픽의 변화 추정 </li>\n<li>시간에 따라 어떤 토픽이 관심을 받는지, 신규 문서가 어느 토픽에 대한 것인지 등을 추정할 수 있다.</li>\n<li><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/803d285166796481368e37407f2148dc/fba00/image-20200721170724695.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 44.5945945945946%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAAByklEQVQozzVSzc7aMBDk/d+il6qnqrdKVR+gPXyH70Ch8DVAAiRgO3ac2InzNx0HWGnlaDeendnxCox5njEOA8ZxxMCM56seQsD07FuloLWFVCWMNWhsCS0FjDa4mwoD/1m9LtZNA8ULRusF5FVP0xS2rtE7hzY7QQqHrLhDOIXgCtj8ijIvcKgCQtcRkJdieALmpxN0niN4j+nJto5gfc/pI1zdwFoPYyxq36D1FkZIuKrGzWp0ocUqyrtfLigIdj6fkRwOEARtyajjRCklJFlPZYm5vCGO186gn9qFSPD/llOVf+C8wipwenY8IksSyOsVt6JYvhtjHoB3yrMW3XqH9ddvEOkOP77/hh1u6P2MXz8/Iz+kePvyCcFkjx2WnP6x+7swSw4JNus11DVfJjeUHIYecC32b+9w8oLLdseaQ9+NOG22aNIzsvd3jK19AEo6tdlucKH05GOP/X6/7C5GTsaakodpeuzaM+mZZXsIQ3QOY0P50UjnH4DRAEuW9yJHejpy8RavmJ+mTQRczIom0ZSBQxpRodaezydAq5Z4I1aOy6+qChVBIpMoP+ZixjPjcxJCLAoKMi64moLPRcQ+e0pJ3LjrnPX/HuetNQdbV+oAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200721170724695\"\n        title=\"image-20200721170724695\"\n        src=\"/static/803d285166796481368e37407f2148dc/fcda8/image-20200721170724695.png\"\n        srcset=\"/static/803d285166796481368e37407f2148dc/12f09/image-20200721170724695.png 148w,\n/static/803d285166796481368e37407f2148dc/e4a3f/image-20200721170724695.png 295w,\n/static/803d285166796481368e37407f2148dc/fcda8/image-20200721170724695.png 590w,\n/static/803d285166796481368e37407f2148dc/efc66/image-20200721170724695.png 885w,\n/static/803d285166796481368e37407f2148dc/c83ae/image-20200721170724695.png 1180w,\n/static/803d285166796481368e37407f2148dc/fba00/image-20200721170724695.png 1529w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></li>\n<li>\n<p>(시간대 별 토픽의 변화 추정 그림 中) Topic2에 대한 문서들이 점차 증가했다</p>\n<p>→ 관심이 점점 늘어나는 주제인가 보다,를 알 수 있음 </p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>Topic 모델링: 다수의 문서를 유사한 주제별로 분류</p>\n<ul>\n<li>비지도학습</li>\n<li>처음은 유사도에 의해 Clustering</li>\n<li>새로운 문서가 등장하면, K-Means or SOM처럼 중점의 좌표를 찾는 알고리즘을 통해</li>\n<li>거리 측정</li>\n<li>\n<p>가까운 거리에 있는 Cluster로 할당</p>\n<p>ex: 새로운 뉴스가 발생했을 때, 유사도가 높은 중심 토픽을 찾으면 해당 뉴스가 어느 토픽에 속하는지 판별할 수 있다.</p>\n<br>\n</li>\n</ul>\n</li>\n<li>\n<p>LDA 모형으로 Topic 모델링:</p>\n<ul>\n<li><code class=\"language-text\">Generative Model</code></li>\n</ul>\n<blockquote>\n<p>\"데이터가 어떻게 발생되었느냐?\" 의 측면에서 문제를 접근해 나가는 것</p>\n<p>어떤 데이터들이 주어졌을 경우 이 데이터가 생기기 위해서 내부적으로 어떤과정과 어떠한 원인을 통해 이런 데이터가 생성되었느냐의 측면에 초점을 맞추는 것</p>\n</blockquote>\n<br>\n<ul>\n<li><code class=\"language-text\">Discriminative Model</code></li>\n</ul>\n<blockquote>\n<p>발생된 데이터들을 바탕으로 \"이를 나누는 기준이 무엇인가? \"에 초점을 맞춘 접근방식이며 </p>\n<p>데이터가 발생한 원인이나 과정등을 직접적으로 다루지는 않는다.</p>\n<ul>\n<li>출처: arongdari. 2010. 6. 2. \"Latent Dirichlet Allocation\". <a href=\"https://arongdari.tistory.com/entry/Latent-Dirichlet-Allocation\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arongdari.tistory.com/entry/Latent-Dirichlet-Allocation</a>. </li>\n</ul>\n</blockquote>\n<br>\n<ul>\n<li>문서 집합(Corpus)에서 관측된 W(Word)를 이용하여 Hidden 상태의 세타와 베타를 추론한다.</li>\n<li><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/87cf99f86c232f56c8fc361a22086b1c/0c1c2/image-20200721171735402.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 51.35135135135135%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAABzElEQVQoz11Sa4/TMBDM//8vfEHiCzqJDxQkJIRExel66l2vBdJeHm6efsSOM8y69MSx0cTWZj07s05mjcHhcMDxeMTpdErI8/xl/1wUKMsSXT9iGIlhSOj7PkFrnVZDHokMf8N7D+ccQgiIMaZVk0DVNXoSGO1ZM/OghrUWI79N04R/Y1kWZPKSkC5VVaWOAjnUk6xtGnRdj6Z4xvl8RqsUBuakRgjl/BWvFEoYkly7d12HngfFnqjN89+JsGdemowklJr/VWZCIrNxXD2x0O5E+xP3jsVNrWjToSobNOoMzVk5IrBmZiOpk3Fd1WbLHBGZWPjxGkIUrYGfPMrKknDifAO6Ri6gS40d1VkzI1iSOZvmnwgdhzxRgVyEtTp1lo56vBBeRnOZkeSNsZh9SCq1NogUYmnfsTYK4cTN4DsEPkWh4SMQqNqbQmhebk8ihBmj62GXi82ImaAjT0dUudBZVuQDVp/eMatwc7PB02bFfwjY3n5GnX/D6/8C+Lj6AvX1LeqjxXr9HtAtijZg++ENsHtE1qgBu80tgirx89cJ1e4HpsHiYbvHeX+PmQ58uIzBU8HD/Q7t0x1UNeLx7jtiq1AWLfabNeI44A8SeAI/nTDaawAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200721171735402\"\n        title=\"image-20200721171735402\"\n        src=\"/static/87cf99f86c232f56c8fc361a22086b1c/fcda8/image-20200721171735402.png\"\n        srcset=\"/static/87cf99f86c232f56c8fc361a22086b1c/12f09/image-20200721171735402.png 148w,\n/static/87cf99f86c232f56c8fc361a22086b1c/e4a3f/image-20200721171735402.png 295w,\n/static/87cf99f86c232f56c8fc361a22086b1c/fcda8/image-20200721171735402.png 590w,\n/static/87cf99f86c232f56c8fc361a22086b1c/efc66/image-20200721171735402.png 885w,\n/static/87cf99f86c232f56c8fc361a22086b1c/c83ae/image-20200721171735402.png 1180w,\n/static/87cf99f86c232f56c8fc361a22086b1c/0c1c2/image-20200721171735402.png 1542w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></li>\n</ul>\n<blockquote>\n<ul>\n<li>\n<p>Color: </p>\n<ul>\n<li>gray: 관측된 상태(얘만 알고 있는 상태)</li>\n<li>white: 아무것도 모르는 상태 </li>\n</ul>\n</li>\n<li>세타는 LSA Model에서 U*S or U에 해당하며, Topic이 열이다</li>\n<li>베타는 LSA Model에서 Vt에 해당하며, Topic은 각 첫 번째 행이다(1열)</li>\n</ul>\n</blockquote>\n<p><br><br></p>\n<ul>\n<li>\n<h2 id=\"code-4\" style=\"position:relative;\"><a href=\"#code-4\" aria-label=\"code 4 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>code:</h2>\n</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># TF-IDF matrix를 생성한다. &lt; 까지 LSD와 같음</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Latent Dirichlet Allocation (LDA) 여기서부터 다름!!!!</span>\n<span class=\"token comment\"># ---------------------------------</span>\n<span class=\"token comment\"># Return 값이 Document-Topic distribution이다.</span>\n<span class=\"token comment\"># iteration 횟수가 max_iter까지 가면 아직 수렴하지 않은 것이다.</span>\n<span class=\"token comment\"># 아직 수렴하지 않은 경우 mat_iter를 증가시켜야 한다.</span>\n<span class=\"token comment\"># mat_iter를 증가시켜도 수렴하지 못하는 경우는 preprocessing 등을 좀 더 정밀하게 해야 한다.</span>\n<span class=\"token comment\"># evaluate_every=5: 5Step마다 성능 평가 결과 출력. 비지도학습인 만큼, 이때는 perplexity 지표 사용 </span>\n<span class=\"token comment\"># perplexity: LDA에서 깁스 샘플링 관련하여 공식을 형성함. 작거나 작아질수록 Good </span>\n<span class=\"token comment\"># n_compoenets가 적절치 못하면 샘플링이 터진다 </span>\nmodel <span class=\"token operator\">=</span> LDA<span class=\"token punctuation\">(</span>n_components <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>newsData<span class=\"token punctuation\">.</span>target_names<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> \nlearning_method<span class=\"token operator\">=</span><span class=\"token string\">'online'</span><span class=\"token punctuation\">,</span> \nevaluate_every<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> \nmax_iter<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span> \nverbose<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\ndoc_topic <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>tfidf<span class=\"token punctuation\">)</span>    </code></pre></div>\n<p>​    <br></p>\n<ul>\n<li>문서 별 Topic 번호를 확인한다. (문서 10개만 확인)</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'문서-{:d} : topic = {:d}'</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>doc_topic<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">:</span><span class=\"token punctuation\">(</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<ul>\n<li>topic_term 행렬에서 topic 별로 중요 단어를 표시한다</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">    topic_term <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>components_\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>topic_term<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        idx <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>flipud<span class=\"token punctuation\">(</span>topic_term<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>argsort<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'토픽-{:2d} : '</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> end<span class=\"token operator\">=</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> n <span class=\"token keyword\">in</span> idx<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'{:s} '</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">[</span>n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> end<span class=\"token operator\">=</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    \n    newsData<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n* 문서별로 분류된 코드를 확인해 본다.\n  \n```python\n    # x, y : 문서 번호\n    def checkTopic(x, y):\n        print(\"문서 %d의 topic = %s\" % (x, newsData.target_names[newsData.target[x]]))\n        print(\"문서 %d의 topic = %s\" % (y, newsData.target_names[newsData.target[y]]))\n```\n<blockquote>\n<p>토픽- 1 : like article would people second period perhaps nothing called someone\n토픽- 2 : space nasa soon earth cost much idea talking make high <strong>#문서-0은 topic2일 가능성이 높다. 따라서 확인해보니, 이메일 관련 서류인 것 같다.</strong>\n토픽- 3 : hear read news white heard going long ever keep goes\n토픽- 4 : drive disk system scsi apple memory hard computer drives controller\n토픽- 5 : year games last game season team would good years time</p>\n</blockquote>\n<blockquote>\n<p>...(토픽 - 20까지 있음)</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">    checkTopic<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span>\n    checkTopic<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    checkTopic<span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">7</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>문서 1의 topic = alt.atheism\n문서 5의 topic = soc.religion.christian\n문서 0의 topic = talk.politics.mideast\n문서 2의 topic = talk.politics.mideast\n문서 4의 topic = rec.sport.hockey\n문서 7의 topic = talk.politics.mideast</p>\n</blockquote>\n<br>\n<br>\n<hr>\n<p><br><br></p>\n<h2 id=\"code-classlanguage-textpagerank-알고리즘code\" style=\"position:relative;\"><a href=\"#code-classlanguage-textpagerank-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98code\" aria-label=\"code classlanguage textpagerank 알고리즘code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">PageRank 알고리즘</code></h2>\n<ul>\n<li>구글 search 엔진 원리<br></li>\n<li>\n<p>damping-factor (d):</p>\n<ul>\n<li>d: 사람들이 D라는 문서(Hypertext)를 볼 때 거기에 연결된 BAC를 반드시 클릭한다고 가정하는 정도</li>\n<li>Hypertext: 구글에서 searching 하려는 검색어</li>\n<li>문서: 검색어(Hypertext)입력했을 때 나오는 페이지<br></li>\n<li>즉, d는 일종의 가중치 역할(0~1값) = Hyper parameter</li>\n<li>\n<p>d=0 : 해당 페이지에서 클릭하지 않음</p>\n<ul>\n<li>D 문서에 관심 X이면 여기에 연결된 Link인 BAC Link들은 안 보고 나간다 → D 문서는 별로 쓸모 없음 </li>\n</ul>\n</li>\n<li>d=1 : 계속 클릭함</li>\n<li>논문에선 d = 0.85로 설정함<br></li>\n<li>PageRank 업데이트 순서:</li>\n<li>각 페이지의 PageRank  초기화</li>\n<li>\n<p>공식 써서 각 문서의PageRank  계산 ← 반복</p>\n<ul>\n<li>PageRank가 높은 문서가 더 중요한 문서라 판단함 <br></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>damping-factior(d) 적용시 값은 또 달라짐</li>\n</ul>\n<p><br><br></p>\n<h3 id=\"code-classlanguage-texttextrankcode\" style=\"position:relative;\"><a href=\"#code-classlanguage-texttextrankcode\" aria-label=\"code classlanguage texttextrankcode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">TextRank</code></h3>\n<ul>\n<li>PageRank 알고리즘 활용<br></li>\n<li>문서 → 중요 문장 추출(유사도 계산)</li>\n<li>\n<p>문장 → 중요 단어 추출(유사도 계산)</p>\n<ul>\n<li>근데 '단어' 를 추출하는 건 보통 Word Embedding 사용함 <br></li>\n</ul>\n</li>\n<li>\n<p>키워드 추출, 문장 추출 시 활용</p>\n<ul>\n<li>TextRank가 가장 높은 문서의 대표 문장을 뽑아내어 문서 Summary 역할도 수행함 <br></li>\n</ul>\n</li>\n<li>\n<p>TextRank 업데이트 순서:</p>\n<ol>\n<li>각 문장의 TextRank 초기화<br></li>\n<li>문장간 유사도(w) 측정</li>\n<li><strong>한 문서를 문장별로 나누고, 그 문장들에 공통적으로 등장하는 단어(w)의 개수를 세어, 이 수치를 문장 간의 유사도라 측정한다</strong></li>\n<li>w: word. 단어.</li>\n<li>문장 간의 공통된 단어가 없으면 유사도(w)는 0 </li>\n<li>공식: 분모 >> 특정 단어가 i번째 문장과 j번째 문장에도 속하는 개수<br></li>\n<li>공식으로 각 문장의 TextRank 계산 ← 반복</li>\n<li>\n<p>B의 TextRank가 A보다 높으면 B가 A보다 더 중요한 문장이라 판단함.</p>\n<ul>\n<li>따라서 B를 다음 문장으로 선택함</li>\n</ul>\n</li>\n<li>내가(문장이) 중요하면 링크된 문장도 중요해짐 <br></li>\n</ol>\n <br>\n</li>\n<li>\n<h3 id=\"code-5\" style=\"position:relative;\"><a href=\"#code-5\" aria-label=\"code 5 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>code</h3>\n</li>\n<li>\n<p>TextRank 패키지</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> gensim<span class=\"token punctuation\">.</span>summarization<span class=\"token punctuation\">.</span>summarizer <span class=\"token keyword\">import</span> summarize</code></pre></div>\n<blockquote>\n<p>문서 내 중요한 문장만을 뽑아 연결<br></p>\n</blockquote>\n</li>\n<li>\n<p>text 정의</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">text <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token operator\">~</span><span class=\"token punctuation\">]</span></code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>summary</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">s <span class=\"token operator\">=</span> summarize<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> ratio <span class=\"token operator\">=</span> <span class=\"token number\">0.2</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 전체 문장 중에 20% 정도 추출</span></code></pre></div>\n<blockquote>\n<p>And she won't eat her dinner - rice pudding again -\nI've promised her dolls and a daisy-chain,\nI've promised her sweets and a ride in the train,\nAnd it's lovely rice pudding for dinner again!</p>\n</blockquote>\n</li>\n</ul>\n<br>\n<br>\n<hr>\n<p><br><br></p>\n<h2 id=\"anaphora-resolutioncode-classlanguage-text조응어-해석code--대용어-처리\" style=\"position:relative;\"><a href=\"#anaphora-resolutioncode-classlanguage-text%EC%A1%B0%EC%9D%91%EC%96%B4-%ED%95%B4%EC%84%9Dcode--%EB%8C%80%EC%9A%A9%EC%96%B4-%EC%B2%98%EB%A6%AC\" aria-label=\"anaphora resolutioncode classlanguage text조응어 해석code  대용어 처리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Anaphora Resolution(<code class=\"language-text\">조응어 해석</code>) : 대용어 처리</h2>\n<ul>\n<li>이름을 구별하기 위해서는 문장 구조를 분해해야 하고(Chunk)<br></li>\n<li>이름의 성별을 구별하기 위해서는 name corpus를 사용한 학습이 필요하다<br></li>\n<li>\n<blockquote>\n<p>John is a man. He walks. : He는 John을 가리킴 </p>\n<p>John and Mary are married. They have two kids. : They는 John과 Mary를 가리킴 </p>\n</blockquote>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Anaphora resolution 예시</span>\n<span class=\"token keyword\">import</span> nltk\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>chunk <span class=\"token keyword\">import</span> tree2conlltags\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>corpus <span class=\"token keyword\">import</span> names\n<span class=\"token keyword\">import</span> random\n\n<span class=\"token comment\"># name의 마지막 철자를 리턴한다.</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">feature</span><span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">return</span> <span class=\"token punctuation\">{</span><span class=\"token string\">'last(1)'</span> <span class=\"token punctuation\">:</span> word<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span>\n\n<span class=\"token comment\"># name corpus를 읽어온다.</span>\nmales <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">,</span> <span class=\"token string\">'male'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> name <span class=\"token keyword\">in</span> names<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span><span class=\"token string\">'male.txt'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\nfemales <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">,</span> <span class=\"token string\">'female'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> name <span class=\"token keyword\">in</span> names<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span><span class=\"token string\">'female.txt'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>males<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>females<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\ncombined <span class=\"token operator\">=</span> males <span class=\"token operator\">+</span> females\nrandom<span class=\"token punctuation\">.</span>shuffle<span class=\"token punctuation\">(</span>combined<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># supervised learning용 학습  데이터를 생성한다.</span>\n<span class=\"token comment\"># 이름의 마지막 철자로 성별 (male or female)을 학습하기 위한 것이다.</span>\ntraining <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>feature<span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> gender<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">,</span> gender<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> combined<span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>training<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Naive Bayes로 학습한다.</span>\nclassifier <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>NaiveBayesClassifier<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span>training<span class=\"token punctuation\">)</span>\n\nsentences <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n  <span class=\"token string\">\"John is a man. He walks\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token string\">\"John and Mary are married. They have two kids\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token string\">\"In order for Ravi to be successful, he should follow John\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token string\">\"John met Mary in Barista. She asked him to order a Pizza\"</span>\n<span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># name의 마지막 철자로 성별을 예상한다.</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">gender</span><span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">return</span> classifier<span class=\"token punctuation\">.</span>classify<span class=\"token punctuation\">(</span>feature<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 문장을 chunk로 분해해서 사람과 연관된 대명사를 찾는다.</span>\n<span class=\"token keyword\">for</span> sent <span class=\"token keyword\">in</span> sentences<span class=\"token punctuation\">:</span>\n  chunks <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>ne_chunk<span class=\"token punctuation\">(</span>nltk<span class=\"token punctuation\">.</span>pos_tag<span class=\"token punctuation\">(</span>nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> binary<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n  stack <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span>\n  items <span class=\"token operator\">=</span> tree2conlltags<span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">for</span> item <span class=\"token keyword\">in</span> items<span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">if</span> item<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token string\">'NNP'</span> <span class=\"token keyword\">and</span> <span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token string\">'B-PERSON'</span> <span class=\"token keyword\">or</span> item<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token string\">'O'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n          stack<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> gender<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n      <span class=\"token keyword\">elif</span> item<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token string\">'CC'</span><span class=\"token punctuation\">:</span>\n          stack<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n      <span class=\"token keyword\">elif</span> item<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token string\">'PRP'</span><span class=\"token punctuation\">:</span>\n          stack<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\t {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>stack<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>items<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<br>\n<hr>\n<p><br><br></p>\n<h2 id=\"code-classlanguage-textwsdcode-단어의-중의성-분석\" style=\"position:relative;\"><a href=\"#code-classlanguage-textwsdcode-%EB%8B%A8%EC%96%B4%EC%9D%98-%EC%A4%91%EC%9D%98%EC%84%B1-%EB%B6%84%EC%84%9D\" aria-label=\"code classlanguage textwsdcode 단어의 중의성 분석 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">WSD</code> 단어의 중의성 분석</h2>\n<ul>\n<li>Word Sense Disambiguation<br></li>\n<li>\n<p>문장 내에서 단어들의 의미를 파악함</p>\n<ul>\n<li>이를 위해 단어 사전이 필요하고, 주변 단어(context, 문맥)과의 비교가 필요<br></li>\n</ul>\n<blockquote>\n<table>\n<thead>\n<tr>\n<th>문장</th>\n<th>설명</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>She is my <strong>date</strong>.</td>\n<td>date는 달력의 날짜를 의미하는 것이 아니라 인간 관계를 의 미한다.</td>\n</tr>\n<tr>\n<td>You have taken too many <strong>leaves</strong> to skip cleaning <strong>leaves</strong> in the garden.</td>\n<td>첫 번째 'leaves'는 휴식을 의미하고, 두 번째 'leaves'는 나뭇잎 을 의미한다.</td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n</li>\n</ul>\n<p><br><br></p>\n<ul>\n<li>\n<h3 id=\"code-classlanguage-textlesk-알고리즘code\" style=\"position:relative;\"><a href=\"#code-classlanguage-textlesk-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98code\" aria-label=\"code classlanguage textlesk 알고리즘code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Lesk 알고리즘</code></h3>\n<ul>\n<li>문장에 사용된 단어를 사전에서 찾고, 사전의 뜻풀이, 예제 문장들에 등장하는 단어와 분석할 문장의 단어들을 비교해서 겹치는 단어가 많은 뜻풀이를 선택해 단어의 의미를 파악한다.<br></li>\n<li>활용:</li>\n<li>워드넷을 써서 단어가 사용된 문장에,</li>\n<li>중의적 단어가 있을 때, </li>\n<li>단어 주변의 문맥을 살펴,</li>\n<li>\n<p>이 문장에 사용된 단어의 뜻을 찾아냄</p>\n<blockquote>\n<table>\n<thead>\n<tr>\n<th>워드넷을 써서 단어가 사용된 문장에,</th>\n<th>중의적 단어가 있을 때,</th>\n<th>단어 주변의 문맥을 살펴, 이 문장에 사용된 단어의 뜻을 찾아냄</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>wind →</td>\n<td>wind.n.01 →</td>\n<td>trees bent under the fierce winds  → 분석할 문장에 tree, bent 같은 단어가 들어 있다면 wind는 이 의미로 쓰였을 것이다.</td>\n</tr>\n<tr>\n<td>wind →</td>\n<td>wind.n.01 →</td>\n<td>when there is no wind, row</td>\n</tr>\n<tr>\n<td>wind →</td>\n<td>wind.n.02 →</td>\n<td>the winds of change → 분석할 문장에 change (trend) 같은 단어가 들어 있다면 wind는 이 의미 (분위기?)로 쓰였을 가능성이 있다.</td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n</li>\n</ul>\n<br>\n</li>\n<li>\n<p>시소러스 사전. 언어들의 관계까지 정의해놓은 사전. WordNet과 비슷 </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Word Sense Disambiguation (WSD)</span>\n<span class=\"token keyword\">import</span> nltk\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">understandWordSenseExamples</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    words <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'wind'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'date'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'left'</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"-- examples --\"</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> words<span class=\"token punctuation\">:</span>\n        syns <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>corpus<span class=\"token punctuation\">.</span>wordnet<span class=\"token punctuation\">.</span>synsets<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> syn <span class=\"token keyword\">in</span> syns<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">for</span> example <span class=\"token keyword\">in</span> syn<span class=\"token punctuation\">.</span>examples<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"{} -> {} -> {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">,</span> syn<span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> example<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nunderstandWordSenseExamples<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">understandBuiltinWSD</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"-- built-in wsd --\"</span><span class=\"token punctuation\">)</span>\n    maps <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">(</span><span class=\"token string\">'Is it the fish net that you are using to catch fish ?'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'fish'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'n'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">(</span><span class=\"token string\">'Please dont point your finger at others.'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'point'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'n'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">(</span><span class=\"token string\">'I went to the river bank to see the sun rise'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'bank'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'n'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> m <span class=\"token keyword\">in</span> maps<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Sense '{}' for '{}' -> '{}'\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> m<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> \n              nltk<span class=\"token punctuation\">.</span>wsd<span class=\"token punctuation\">.</span>lesk<span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> m<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> m<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nunderstandBuiltinWSD<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nnltk<span class=\"token punctuation\">.</span>corpus<span class=\"token punctuation\">.</span>wordnet<span class=\"token punctuation\">.</span>synsets<span class=\"token punctuation\">(</span><span class=\"token string\">'fish'</span><span class=\"token punctuation\">)</span>\nnltk<span class=\"token punctuation\">.</span>corpus<span class=\"token punctuation\">.</span>wordnet<span class=\"token punctuation\">.</span>synset<span class=\"token punctuation\">(</span><span class=\"token string\">'pisces.n.02'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>lemma_names<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nnltk<span class=\"token punctuation\">.</span>corpus<span class=\"token punctuation\">.</span>wordnet<span class=\"token punctuation\">.</span>synset<span class=\"token punctuation\">(</span><span class=\"token string\">'pisces.n.02'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>definition<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<br>\n<hr>\n<p><br><br></p>\n<h2 id=\"감성분석\" style=\"position:relative;\"><a href=\"#%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D\" aria-label=\"감성분석 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>감성분석</h2>\n<ul>\n<li>\n<p>딥러닝 써서 분석한 건 추후에.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 감정 분석</span>\n<span class=\"token keyword\">import</span> nltk\n<span class=\"token keyword\">import</span> nltk<span class=\"token punctuation\">.</span>sentiment<span class=\"token punctuation\">.</span>sentiment_analyzer\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">wordBasedSentiment</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  positive_words <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'love'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'hope'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'joy'</span><span class=\"token punctuation\">]</span>\n  text <span class=\"token operator\">=</span> <span class=\"token string\">'Rainfall this year brings lot of hope and joy to Farmers.'</span><span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  analysis <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>sentiment<span class=\"token punctuation\">.</span>util<span class=\"token punctuation\">.</span>extract_unigram_feats<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> positive_words<span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">' -- single word sentiment --'</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>analysis<span class=\"token punctuation\">)</span>\n  \n<span class=\"token keyword\">def</span> <span class=\"token function\">multiWordBasedSentiment</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  word_sets <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token string\">'heavy'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'rains'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'flood'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'bengaluru'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n  text <span class=\"token operator\">=</span> <span class=\"token string\">'heavy rains cause flash flooding in bengaluru'</span><span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  analysis <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>sentiment<span class=\"token punctuation\">.</span>util<span class=\"token punctuation\">.</span>extract_bigram_feats<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> word_sets<span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">' -- multi word sentiment --'</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>analysis<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">markNegativity</span><span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  negation <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>sentiment<span class=\"token punctuation\">.</span>util<span class=\"token punctuation\">.</span>mark_negation<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">' -- negativity --'</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>negation<span class=\"token punctuation\">)</span>\n\nwordBasedSentiment<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmultiWordBasedSentiment<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 주어진 문장에서 부정적 의미를 가진 모든 단어에 대해 접미사 _NEG를 표시한다.</span>\nmarkNegativity<span class=\"token punctuation\">(</span><span class=\"token string\">'Rainfall last year did not bring joy to Farmers'</span><span class=\"token punctuation\">)</span>\nmarkNegativity<span class=\"token punctuation\">(</span><span class=\"token string\">\"I didn't like this movie . It was bad.\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><br><br></p>\n</li>\n</ul>\n<h3 id=\"code-classlanguage-textvadercode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textvadercode\" aria-label=\"code classlanguage textvadercode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">VADER</code></h3>\n<ul>\n<li>규칙 기반 알고리즘<br></li>\n<li>\n<p>10명이 느끼는 감정 상태를 survey해서 얻은 score(-4 ~ +4)<br></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># VADER-Sentiment-Analysis</span>\n<span class=\"token keyword\">import</span> nltk\n<span class=\"token keyword\">import</span> nltk<span class=\"token punctuation\">.</span>sentiment<span class=\"token punctuation\">.</span>util\n<span class=\"token keyword\">import</span> nltk<span class=\"token punctuation\">.</span>sentiment<span class=\"token punctuation\">.</span>sentiment_analyzer\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>sentiment<span class=\"token punctuation\">.</span>vader <span class=\"token keyword\">import</span> SentimentIntensityAnalyzer\n\nnltk<span class=\"token punctuation\">.</span>downloader<span class=\"token punctuation\">.</span>download<span class=\"token punctuation\">(</span><span class=\"token string\">'vader_lexicon'</span><span class=\"token punctuation\">,</span> download_dir<span class=\"token operator\">=</span><span class=\"token string\">'./dataset/'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">mySentimentAnalyzer</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">def</span> <span class=\"token function\">score_feedback</span><span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n      positive_words <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'love'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'genuine'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'liked'</span><span class=\"token punctuation\">]</span>\n      <span class=\"token keyword\">if</span> <span class=\"token string\">'_NEG'</span> <span class=\"token keyword\">in</span> <span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>nltk<span class=\"token punctuation\">.</span>sentiment<span class=\"token punctuation\">.</span>util<span class=\"token punctuation\">.</span>mark_negation<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n          score <span class=\"token operator\">=</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span>\n      <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n          analysis <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>sentiment<span class=\"token punctuation\">.</span>util<span class=\"token punctuation\">.</span>extract_unigram_feats<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> positive_words<span class=\"token punctuation\">)</span>\n          <span class=\"token keyword\">if</span> <span class=\"token boolean\">True</span> <span class=\"token keyword\">in</span> analysis<span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n              score <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n          <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n              score <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n      <span class=\"token keyword\">return</span> score\n\n  feedback <span class=\"token operator\">=</span> <span class=\"token triple-quoted-string string\">\"\"\"I love the items in this shop, very genuine and quality is well maintained.\n  I have visited this shop and had samosa, my friends liked it very much.\n  ok average food in this shop.\n  Fridays are very busy in this shop, do not place orders during this day.\"\"\"</span>\n  \n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">' -- custom scorer --'</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">for</span> text <span class=\"token keyword\">in</span> feedback<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"score = {} for >> {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>score_feedback<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">advancedSentimentAnalyzer</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  sentences <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n      <span class=\"token string\">':)'</span><span class=\"token punctuation\">,</span>\n      <span class=\"token string\">':('</span><span class=\"token punctuation\">,</span>\n      <span class=\"token string\">'She is so :('</span><span class=\"token punctuation\">,</span>\n      <span class=\"token string\">'I love the way cricket is played by the champions'</span><span class=\"token punctuation\">,</span>\n      <span class=\"token string\">'She neither likes coffee nor tea'</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">]</span>\n  \n  senti <span class=\"token operator\">=</span> SentimentIntensityAnalyzer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">' -- built-in intensity analyser --'</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">for</span> sentence <span class=\"token keyword\">in</span> sentences<span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'[{}]'</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> end<span class=\"token operator\">=</span><span class=\"token string\">' --> '</span><span class=\"token punctuation\">)</span>\n      kvp <span class=\"token operator\">=</span> senti<span class=\"token punctuation\">.</span>polarity_scores<span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">)</span>\n      <span class=\"token keyword\">for</span> k <span class=\"token keyword\">in</span> kvp<span class=\"token punctuation\">:</span>\n          <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'{} = {}, '</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>k<span class=\"token punctuation\">,</span> kvp<span class=\"token punctuation\">[</span>k<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> end<span class=\"token operator\">=</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\n      <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nmySentimentAnalyzer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nadvancedSentimentAnalyzer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<ul>\n<li>\n<p>참고: </p>\n<blockquote>\n<p>아마추어 퀀트, blog.naver.com/chunjein\n코드 출처: 크리슈나 바브사 외. 2019.01.31. 자연어 처리 쿡북 with 파이썬 [파이썬으로 NLP를 구현하는 60여 가지 레시피]. 에이콘</p>\n<ul>\n<li>참고 및 유용한 블로그:</li>\n<li>\n<p>NLP 기초 용어 및 TF-IDF, BOW의 차이 등 아주 쉽게 정리해주심</p>\n<ul>\n<li>hero4earth. 2018.01.17. \"자연어(NLP) 처리 기초 정리\". <a href=\"http://hero4earth.com/blog/learning/2018/01/17/NLP_Basics_01/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">http://hero4earth.com/blog/learning/2018/01/17/NLP_Basics_01/</a></li>\n</ul>\n</li>\n<li>NLP 패키지 메소드 활용시 참고</li>\n<li>데이터사이언스스쿨. 2016.06.14. \"Scikit-Learn의 문서 전처리 기능\". <a href=\"https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/</a></li>\n<li>\n<p>코사인 유사도:</p>\n<ul>\n<li>데이터 파수꾼 Baek Kyun Shin. 2020. 2. 17. \"NLP - 8. 코사인 유사도(Cosine Similarity)\". <a href=\"https://bkshin.tistory.com/entry/NLP-8-%EB%AC%B8%EC%84%9C-%EC%9C%A0%EC%82%AC%EB%8F%84-%EC%B8%A1%EC%A0%95-%EC%BD%94%EC%82%AC%EC%9D%B8-%EC%9C%A0%EC%82%AC%EB%8F%84\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://bkshin.tistory.com/entry/NLP-8-문서-유사도-측정-코사인-유사도</a></li>\n<li>데이타광 DNA구너. 2020. 5. 20. \"[스팀 2부 - 이론] 협업 필터링 - 스팀, 넷플릭스, 아마존이 당신을 사로잡기 위해 부리는 마법 이해하기\". <a href=\"https://dnagooner.tistory.com/51\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://dnagooner.tistory.com/51</a></li>\n</ul>\n</li>\n<li>\n<p>토픽 모델:</p>\n<ul>\n<li>원준. \"잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)\". <a href=\"https://wikidocs.net/30708\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://wikidocs.net/30708</a></li>\n</ul>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>","excerpt":"고급 NLP 레시피 자연어 기초 용어 편집거리 주제식별 감성분석  자연어 관련 용어 (문서) (말뭉치): 텍스트(문서)의 집합 (토큰): 단어처럼 의미를 가지는 요소 (형태소): 의미를 가지는 언어에서 최소 단위 (품사): ex) Nouns…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#%EA%B3%A0%EA%B8%89-nlp-%EB%A0%88%EC%8B%9C%ED%94%BC\">고급 NLP 레시피</a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#%EC%9E%90%EC%97%B0%EC%96%B4-%EA%B4%80%EB%A0%A8-%EC%9A%A9%EC%96%B4\">자연어 관련 용어</a></li>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#%ED%8E%B8%EC%A7%91%EA%B1%B0%EB%A6%ACedit-distance\">편집거리(Edit distance)</a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#code-classlanguage-texttf-idfcode\"><code class=\"language-text\">TF-IDF</code></a></li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#code-classlanguage-text%EC%BD%94%EC%82%AC%EC%9D%B8-%EC%9C%A0%EC%82%AC%EB%8F%84code%EA%B1%B0%EB%A6%AC\"><code class=\"language-text\">코사인 유사도</code>(거리)</a></li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#code-classlanguage-textbowcode\"><code class=\"language-text\">Bow</code></a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#%EC%A3%BC%EC%A0%9C-%EC%8B%9D%EB%B3%84-topic-model\">주제 식별 (Topic Model)</a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#topic-modellsa-vs-lda\">Topic Model(LSA vs LDA)</a></li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#code-classlanguage-textsvdcode\"><code class=\"language-text\">SVD</code></a></li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#code-classlanguage-textlsacode\"><code class=\"language-text\">LSA</code></a></li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#code-classlanguage-textldacode\"><code class=\"language-text\">LDA</code></a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#code-classlanguage-textpagerank-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98code\"><code class=\"language-text\">PageRank 알고리즘</code></a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#code-classlanguage-texttextrankcode\"><code class=\"language-text\">TextRank</code></a></li>\n</ul>\n</li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#anaphora-resolutioncode-classlanguage-text%EC%A1%B0%EC%9D%91%EC%96%B4-%ED%95%B4%EC%84%9Dcode--%EB%8C%80%EC%9A%A9%EC%96%B4-%EC%B2%98%EB%A6%AC\">Anaphora Resolution(<code class=\"language-text\">조응어 해석</code>) : 대용어 처리</a></li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#code-classlanguage-textwsdcode-%EB%8B%A8%EC%96%B4%EC%9D%98-%EC%A4%91%EC%9D%98%EC%84%B1-%EB%B6%84%EC%84%9D\"><code class=\"language-text\">WSD</code> 단어의 중의성 분석</a></li>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D\">감성분석</a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_1/#code-classlanguage-textvadercode\"><code class=\"language-text\">VADER</code></a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/NLP응용_1/"},"frontmatter":{"title":"NLP 편집거리/주제식별/자연어분석","date":"Jul 20, 2020","tags":["NLP","LDA","PageRank"],"keywords":["JyneeEarth","jynee"],"update":"Aug 16, 2020"}}},"pageContext":{"slug":"/NLP응용_1/","series":[{"slug":"/NLP응용_1/","title":"NLP 편집거리/주제식별/자연어분석","num":1},{"slug":"/NLP응용_2/","title":"NLP Embedding","num":2},{"slug":"/NLP응용_3/","title":"NLP Word2Vec/SGNS","num":3},{"slug":"/NLP응용_4/","title":"NLP Ask Me Anything","num":4}],"lastmod":"2020-08-16"}},"staticQueryHashes":["3649515864","694178885"]}