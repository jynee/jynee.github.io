{"componentChunkName":"component---src-templates-post-tsx","path":"/NLP_BERT_1/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"nlp\" style=\"position:relative;\"><a href=\"#nlp\" aria-label=\"nlp permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NLP</h1>\n<h2 id=\"code-classlanguage-textbertcode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textbertcode\" aria-label=\"code classlanguage textbertcode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">BERT</code></h2>\n<ul>\n<li>Bidirectional Encoder Representations Form Transformer</li>\n<li>transformer의 encoder만 사용한다. </li>\n<li>Pre-traning과 fine-tuning으로 활용한다.</li>\n<li>\n<p>즉, transformer의 encoder을 사전학습(UL) 시킨 언어 모델로, AE처럼 목적에 맞게 활용할 수 있도록 해둔 것이다. <strong>특정 과제의 성능을 더 좋게 할 수 있는 언어 모델이다.</strong></p>\n<ul>\n<li>transformer의 encoder 출력 = BERT의 출력</li>\n<li>transformer의 encoder 부분만 Pre-traning 하여 사용, <strong>실제 사용할 때는 W만 빼내는 fine-tuning 방법으로 응용</strong>한다. </li>\n</ul>\n<blockquote>\n<p>BERT등장 이전에는 데이터의 전처리 임베딩을 Word2Vec, GloVe, Fasttext 방식을 많이 사용했지만, 요즘의 고성능을 내는 대부분의 모델에서 BERT를 많이 사용하고 있다고 합니다.</p>\n</blockquote>\n</li>\n<li>BERT는 이미 총3.3억 단어(BookCorpus + Wikipedia Data)의 거대한 코퍼스를 정제하고, 임베딩하여 학습시킨 모델이 있다. 따라서 새로운 단어를 추가하거나 기타 등등등의 이유로 필요할 때 BERT 기법을 적용하기 위해 작동 원리를 배운다. </li>\n<li>\n<p>AR(Autoregressive) : 과거 데이터로부터 현재 데이터를 추정할 수 있다. </p>\n<blockquote>\n<p>참고: 기존의 ELMO나 GPT는 left to right or right to left Language Model을 사용하여 pre-training을 하지만, BERT는 이와 다르게 2가지의 새로운 unsupervised prediction task로 pre-training을 수행합니다.</p>\n<p>출처: <a href=\"https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">mino-park7. \"BERT 논문정리\"</a></p>\n</blockquote>\n</li>\n</ul>\n<br>\n<br>\n<h3 id=\"알고리즘-원리\" style=\"position:relative;\"><a href=\"#%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9B%90%EB%A6%AC\" aria-label=\"알고리즘 원리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>알고리즘 원리</h3>\n<h4 id=\"input-단계\" style=\"position:relative;\"><a href=\"#input-%EB%8B%A8%EA%B3%84\" aria-label=\"input 단계 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Input 단계</h4>\n<p><img src=\"https://blog.kakaocdn.net/dn/WFCfe/btqBWZ40Gmc/6FkuwsAGN9e7Uudmi03k4k/img.png\" alt=\"img\"></p>\n<br>\n<blockquote>\n<p><em>BERT는 아래 세가지 임베딩을 합치고, Layer에 정규화와 Dropout을 적용하여 입력값으로 사용한다.</em>\n출처: <a href=\"https://ebbnflow.tistory.com/151\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ebb and flow</a></p>\n</blockquote>\n<br>\n<ul>\n<li>\n<p><strong><code class=\"language-text\">Token Embedding</code></strong> ></p>\n<p>\"<em>[<code class=\"language-text\">Word Piece</code>]</em>. 임베딩 방식 사용한다. 즉, 각 Char(문자) 단위로 임베딩한다.\"</p>\n<ul>\n<li>SubWord</li>\n</ul>\n<p>Step 1. 자주 등장하면서 가장 긴 길이로 단어를 분리한다.</p>\n<p>Step 2. 자주 등장하지 않았던 단어도 분리한 후 'OOV'처리하여 모델링의 성능을 저하했던 'OOV'문제도 해결한다.</p>\n<ul>\n<li>두 문장이 들어왔다는 걸 알려준다.</li>\n</ul>\n<p>Step 3. 두 문장을 구분한단 의미에 구분자 [SEP]를 넣어 분리한다. </p>\n<ul>\n<li>pre-trained 일 경우 input을 2개 문장씩 넣어주고, fine-tuning 시 분류할 목적이라면 input에 문장을 하나만 넣어준다.</li>\n</ul>\n<blockquote>\n<p>모든 sentence의 첫번째 token은 언제나 <code class=\"language-text\">[CLS]</code>(special classification token) 입니다. 이 <code class=\"language-text\">[CLS]</code> token은 transformer 전체층을 다 거치고 나면 token sequence의 결합된 의미를 가지게 되는데, 여기에 <strong>간단한 classifier를 붙이면 단일 문장, 또는 연속된 문장의 classification을 쉽게 할 수 있게 됩니다</strong>. 만약 classification task가 아니라면 이 token은 무시하면 됩니다.</p>\n<ul>\n<li>출처: <a href=\"https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">mino-park7. \"BERT 논문정리\"</a></li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p><strong><code class=\"language-text\">Segment Embedding</code></strong> ></p>\n<p>\"<em>[<code class=\"language-text\">Sentence Embedding</code>]</em>. 문장 순서 정보가 있다. 즉, 의미 있는 여러 subword(내부 단어)로 임베딩 한다.\"</p>\n<p>Step 1. 위 Step 3에서 구분한 구분자로 두 문장을 하나의 Segment로 지정하여 입력한다. 토큰 시킨 단어들을 다시 하나의 문장으로 만든다.</p>\n<blockquote>\n<p>첫 번째 문장 속 단어는 전부 1로, 두 번째 문장 속 단어는 전부 2로 임베딩한다.</p>\n</blockquote>\n<blockquote>\n<p>BERT에서는 이 한 세그먼트를 512 sub-word 길이로 제한하는데, 한국어는 보통 20 sub-word가 한 문장을 이룬다고 하며 대부분의 문장은 60 sub-word가 넘지 않는다고 하니 BERT를 사용할 때, 하나의 세그먼트에 128로 제한하여도 충분히 학습이 가능하다고 합니다.</p>\n<p>출처: <a href=\"https://ebbnflow.tistory.com/151\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ebb and flow</a></p>\n</blockquote>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p><strong><code class=\"language-text\">Position Embedding</code></strong> ></p>\n<p>\"<em>[Position Embedding]</em>. 단어 순서 정보\"</p>\n<p>Step 1. 순서정보를 입력하면 저장 + 공식에 맞춰 벡터화(수치화) 시킨다. Token 순서 대로 인코딩 한다.</p>\n</li>\n</ul>\n<br>\n<br>\n<h4 id=\"pre-training-단계\" style=\"position:relative;\"><a href=\"#pre-training-%EB%8B%A8%EA%B3%84\" aria-label=\"pre training 단계 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Pre-Training 단계</h4>\n<blockquote>\n<p>데이터들을 임베딩하여 훈련시킬 데이터를 모두 인코딩 하였으면, 사전훈련을 시킬 단계입니다. 기존의 방법들은 보통 문장을 왼쪽에서 오른쪽으로 학습하여 다음 단어를 예측하는 방식이거나, 예측할 단어의 좌우 문맥을 고려하여 예측하는 방식을 사용합니다.</p>\n<p>하지만 BERT는 언어의 특성을 잘 학습하도록,</p>\n<ul>\n<li><code class=\"language-text\">MLM(Masked Language Model)</code></li>\n<li><code class=\"language-text\">NSP(Next Sentence Prediction)</code></li>\n</ul>\n<p>위 두가지 방식을 사용합니다. </p>\n<p>논문에서는 기존(좌우로 학습하는 모델)방식과 비교하여 MLM방식이 더 좋은 성능을 보여주고 있다고 말합니다!</p>\n<p>출처: <a href=\"https://ebbnflow.tistory.com/151\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ebb and flow</a></p>\n</blockquote>\n<p><br><br></p>\n<h5 id=\"task-1-mlm\" style=\"position:relative;\"><a href=\"#task-1-mlm\" aria-label=\"task 1 mlm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Task 1. MLM</h5>\n<ul>\n<li>MLM: Masked LM</li>\n<li>\n<p>양방향+단방향을 concatnate 한다.</p>\n<ul>\n<li>이를 통하여 LM의 left-to-right을 통하여 문장 전체를 predict하는 방법론과는 달리, <strong>[MASK] token 만을 predict</strong>하는 pre-training task를 수행.</li>\n<li>많은 training step이 필요하지만, 보통 LM보다 훨씬 빠르고 좋은 성능을 낸다. <br></li>\n</ul>\n</li>\n</ul>\n<h6 id=\"denoising--mask\" style=\"position:relative;\"><a href=\"#denoising--mask\" aria-label=\"denoising  mask permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Denoising ~ [mask]</h6>\n<blockquote>\n<ul>\n<li>AE에서는 학습 시, 미리 잡음을 섞어 추후에 입력될지 모르는 잡음을 제거(Denoising)할 수 있도록 모델링을 해둔다. BERT에서도 [mask] 개념을 사용하여 Denoising AE와 같은 역할을 수행하도록 학습한다.</li>\n<li>\n<p>I love you → I [mask] you</p>\n<ul>\n<li>[mask]: noise 처리된 단어를 [mask] 처리함</li>\n<li>이때 주변단어를 이용하여 [mask]된 단어가 나오도록 알아맞추게 학습한다.</li>\n<li>Pre-traning: [mask] 사용</li>\n<li>Fine-tuning: [mask] 안 사용</li>\n<li>해당 token을 맞추어 내는 task를 수행하면서, BERT는 문맥을 파악하는 능력이 생긴다.</li>\n</ul>\n</li>\n<li>\n<ol>\n<li>80% 단어(token)을 [MASK]로 바꾼다. eg., my dog is hairy -> my dog is [MASK]</li>\n<li>10% token을 random word로 바꾼다. eg., my dog is hariy -> my dog is apple</li>\n<li>10%는 원래 token로 그대로 둔다. 이는 실제 관측된 단어를 bias해주기 위해 실시한다.</li>\n</ol>\n</li>\n<li>TransFormer의 Encoder에서도 사용한다.</li>\n</ul>\n</blockquote>\n<br>\n<h5 id=\"task-2-nsp\" style=\"position:relative;\"><a href=\"#task-2-nsp\" aria-label=\"task 2 nsp permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Task 2. NSP</h5>\n<ul>\n<li>\n<p>문장과 문장 간의 관계</p>\n<ul>\n<li>두 문장 학습</li>\n</ul>\n</li>\n<li>NSP: Next Sentence prediction</li>\n<li>\n<p>input 단계에서 붙여진 [CLS], [SEP]를 확인한다. 그리고 두 문장을 이어 붙이곤, 이게 원래의 corpus에서도 바로 이어 붙여져 있던 문장인지를 맞추는 binarized next sentence prediction task를 수행한다.</p>\n<ul>\n<li>연속된 2문장인 A문장과 B문장을 확인하고, A문장 뒤에 B문장이 이어서 나오는 구나,를 파악한다</li>\n<li>Special token: [CLS], [SEP]이 추가되어  있는 점이 Transformer와의 차이점이다.</li>\n<li>[CLS]: 문장 시작</li>\n<li>[SEP]: 단어 구분자</li>\n</ul>\n<br>\n</li>\n<li>\n<p>원리: </p>\n<ul>\n<li>A 다음 B가 나오면 IsNEXT로 분류한다.</li>\n<li>A 다음 엉뚱하게 C가 나오면 NotNext로 분류한다.</li>\n</ul>\n</li>\n</ul>\n<br>\n<h3 id=\"주의\" style=\"position:relative;\"><a href=\"#%EC%A3%BC%EC%9D%98\" aria-label=\"주의 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>주의</h3>\n<ul>\n<li>\n<p>Transformer에서는 만든 SentencePiece를 입력값에 넣을 수 있었지만, BERT는 안된다.</p>\n<ul>\n<li>Input layer에서 자동으로 SentencePiece 처리를 해준다.</li>\n<li>\n<p>code<a href=\"https://github.com/CyberZHG/keras-bert\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Zhao HG keras-bert</a> 中 <strong>.encode()</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">ids<span class=\"token punctuation\">,</span> segments <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> max_len<span class=\"token operator\">=</span>SEQ_LEN<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n<li>Finetuning의 경우, pre-traing 당시 학습 안 된 단어는 OOV로 자동 처리한다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h3 id=\"code\" style=\"position:relative;\"><a href=\"#code\" aria-label=\"code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>CODE</h3>\n<ul>\n<li><a href=\"https://github.com/SKTBrain/KoBERT\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">SKTBrain. \"KoBERT\"</a></li>\n<li><a href=\"https://github.com/CyberZHG/keras-bert\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Zhao HG. \"keras-bert\"</a></li>\n</ul>\n<br>\n<br>\n<h3 id=\"활용\" style=\"position:relative;\"><a href=\"#%ED%99%9C%EC%9A%A9\" aria-label=\"활용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>활용</h3>\n<ul>\n<li>\n<p>BERT 활용 방법: 대량의 코퍼스를 BERT 모델에 넣어 학습하고, BERT 출력 부분에 추가적인 모델(RNN, CNN 등의 ML/DL 모델)을 쌓아 목적을 수행한다. </p>\n<blockquote>\n<ul>\n<li>\n<p>BERT를 사용하지 않은 일반 모델링 > </p>\n<p>분류를 원하는 데이터 → LSTM, CNN 등의 머신러닝 모델 → 분류</p>\n</li>\n<li>\n<p>BERT를 사용한 모델링 > </p>\n<p><strong>관련 대량 코퍼스 → BERT</strong> →  분류를 원하는 데이터 → LSTM, CNN 등의 머신러닝 모델 → 분류</p>\n</li>\n<li>이때 DNN을 이용하였을 때와 CNN등 과 같은 복잡한 모델을 이용하였을 때의 성능 차이가 거의 없다고 알려져 있다.</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c51fdaa01f671e9b5dcbcf982601927f/ace37/image-20200819161110447.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAAD+ElEQVQ4y02UW28aVxSF+dd9qVT1B1RVHlpVUdqnvlWJrcRxEmyDYYY7xsOYAWYAA2NuNtfhOly/7hljJ0c6gnP2Omufs/daEzgAZiZDp91hOJnRH45e52Dk+L/L5Qpv9Not9JjKaCr7g6cjZnrE9XGmUwLb3Z7QdZTPUYWI/shF+o5zVeGzImutzXksR9Xu+oT5UoWz0Dcu0yrB5DVRvUKi7KIaY74mVVK6IYSbDcmyg1raSHBJMN3jc7wt6zVJ0yVqLKl3JkK3x6iPiZZ2xMsbQprDJ6WJWnBIWS4RY4tWGRPYbLek8yYJzSQumS7jBUKxrGSrEC/0iWkNGq2Bf8NitU1Sr5IpVFGzOt/UAkpxSaI0E7yJbj54Tz6Qib8nFf6TVL5KOnlOTn1HJvKWRDZLXPmA/WD7hGUjQeLsJ/Twr+hXP6NlPskr1qSNNreXv1DQVQIesG6U0aNJxoMRlXwBI5bGiEujmg9UtALL2dwn7Lc6JD5KwmCI/GUEu2iykpI544mso348cDgcOGw3LCZjHntd5o7US9bubEpXujqTfTj4hIf9jp27YildHnQ7dFsSn83Yevj5TMq8f77hynUZjkb0ej3Gk4n/fzAcylMfmIoUXsZqtfJjo7Ekf3qiLVLrdrs4ztTH7V8Il8slup5H0zRMs0yz2aBpN2nYNs7sO6EriW3Z8+ZgOGC9XrNcuf75ldz8SPj8nO32wEoCh8PxtDRr8vTITMS7F62+Donv3A3TscPaXcvBtb+5XizxDvs1vDMtYrc33FpdUncWiTsNJRoicnrBxX8n1O6fu1yoVonGr1DDXwgHT8kINh0pkSu1CX95z73dIrCXQif1Gl9TZZSC6DDb5EzRCGYt4tac4O0DhdqzU1KFBhc3FpG7Olc5k7DWIV7xhL7gSmti1HpCuNugVacoJcQxK3HKIydhm5g4wtOYYqyx7KnvlJw1JVpEYge+JEd8CNWIGRNS5koctcVsTgh45cllgyRDb4jemCiJEMno3yQj/6Cm01xHTiiZRf+GuZsw2chvaIm/0NQ3JG9SXOY3KLf35JTfKVul5y5Xizrpy1Pu6y2KeY1c5Cs3Mq2yiZaKiR7bPuG9IZY8/QPj6l9uz99i3WVpDbc0Gi3SH9/J1+jhqMOJQ8+qMhd9DUVbo2aLQb0pXZZPVNNmK3LxZTOdC65B16pjFyycp4FfCncxp12usd9shdBzylERnlhXx8Pbw56uCH3uycEThuD2r5qCibil9/jIxHHY7b/Lyr/hYrGgXC5TFVl0Oh2eBNgTB3jCfSF7MUClUvGxpVKJWq3m4/v9/is28MLskXpz/0O2Vy17r/iBdD6f+x52j6/5Men/3V1r3iL9/hMAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200819161110447\"\n        title=\"image-20200819161110447\"\n        src=\"/static/c51fdaa01f671e9b5dcbcf982601927f/fcda8/image-20200819161110447.png\"\n        srcset=\"/static/c51fdaa01f671e9b5dcbcf982601927f/12f09/image-20200819161110447.png 148w,\n/static/c51fdaa01f671e9b5dcbcf982601927f/e4a3f/image-20200819161110447.png 295w,\n/static/c51fdaa01f671e9b5dcbcf982601927f/fcda8/image-20200819161110447.png 590w,\n/static/c51fdaa01f671e9b5dcbcf982601927f/ace37/image-20200819161110447.png 666w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>QQP</li>\n<li>Q&#x26;A</li>\n<li>Classification</li>\n<li>Tagging</li>\n</ul>\n<br>\n<h4 id=\"qqp-예시\" style=\"position:relative;\"><a href=\"#qqp-%EC%98%88%EC%8B%9C\" aria-label=\"qqp 예시 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>QQP 예시</h4>\n<ul>\n<li>QQP : <em>\"Input된 Q1과 Q2가 얼마나 유사한가? 유사한 문장인가?\"</em> 를 따지는 것.</li>\n</ul>\n<br>\n<h4 id=\"qa-예시\" style=\"position:relative;\"><a href=\"#qa-%EC%98%88%EC%8B%9C\" aria-label=\"qa 예시 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>QA 예시</h4>\n<ul>\n<li>(챗봇) Qustion &#x26; Answer에 활용한다.</li>\n<li>SQuAD 모델: &#x3C;START>와 &#x3C;END>가 나오도록 하는 모델</li>\n<li>\n<table>\n<thead>\n<tr>\n<th>설명</th>\n<th>알고리즘 설명</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>지문이 주어지고, Where ~? 라는 질문을 받으면 Seoul Korea라고 y값을 내놓을 수 있게 지문의 위치를 strat = 13이고 end =14라고 학습한다.</td>\n<td>[CLS] Question [SEP] 지문 [SEP]</td>\n</tr>\n<tr>\n<td>질문에 답을 달아놓을 수 있게, 지문의 몇 번째 있는 단어가 정답인지에 대해 학습을 한다.</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n<br>\n<h4 id=\"stance-classification분류\" style=\"position:relative;\"><a href=\"#stance-classification%EB%B6%84%EB%A5%98\" aria-label=\"stance classification분류 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Stance Classification(분류)</h4>\n<ul>\n<li>일반 LSTM은 문장 내에서 단어들의 흐름을 보지만, Stance classification은 여러 문장들의 서로의 <strong>관련성</strong>을 보고 classification를 한다.</li>\n</ul>\n<br>\n<br>\n<h4 id=\"code--modelsummary\" style=\"position:relative;\"><a href=\"#code--modelsummary\" aria-label=\"code  modelsummary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>code ~ model.summary()</h4>\n<ul>\n<li><a href=\"https://github.com/CyberZHG/keras-bert\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Zhao HG</a> 의 keras_BERT code 를 활용하였다.</li>\n<li>Pre_trained data는 <a href=\"https://github.com/google-research/bert\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Google Research</a>를 활용하였다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a6942057a32f062b1491650a4ac02b48/c1b63/image-20200818172004055.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAABvUlEQVQoz3VS25LSUBDkI9S9IEIIEELI/ULIsiRZXdYqS33y/z+l7Z4Qqyj1oSuZSU5fZs5kmz3BSxsExTP87ASrk6NhGZQGN6yQHF+QNp9Rtm8ozxf4+Ql3ToAHN7zBJKp7hFVrB6JDh7HW+4akPgVW0YGCZ+vH/J4cexO5X+5vyFRPpOQlg0NzyXpLx+o5uwLTVQSHLvfl+UbYCQp8WOzM5b1wJZ+EVWckciAiOdqxVvxVeIC7r8xh9vQF+ekVFSM3L98Qk9gJa+KAOf95pLA51Aepi0QEiql6X7b2LhH1g6tDiZsg603zanApLtKPXorJEKFHytloERtGVSQ5Xe7LK6o/sxtEG6zZjxg1WseYEXJokeVmHPg2HRZgDoh1VNvw13HNzb5ZZM3ZIZm7zZCTIGZMV/MjLLIO66qIQIcXXMRI9omHNMM5n6PbxS5nP8V0k+AdCd6T9E4bHq+NnEl14ed4ZEOEqiUiAW1TRFpKXHeYcU5TRhQe/nVtRKjhz/3MHOmKeOlQy40glyLUhjUSOZx5yV/30BxW7VcUzxfk5qC3u1d339FdfuHY/UDT/ySGZ1C0Jjpnmv8R/gYTS1PKdRbdaQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200818172004055\"\n        title=\"image-20200818172004055\"\n        src=\"/static/a6942057a32f062b1491650a4ac02b48/fcda8/image-20200818172004055.png\"\n        srcset=\"/static/a6942057a32f062b1491650a4ac02b48/12f09/image-20200818172004055.png 148w,\n/static/a6942057a32f062b1491650a4ac02b48/e4a3f/image-20200818172004055.png 295w,\n/static/a6942057a32f062b1491650a4ac02b48/fcda8/image-20200818172004055.png 590w,\n/static/a6942057a32f062b1491650a4ac02b48/efc66/image-20200818172004055.png 885w,\n/static/a6942057a32f062b1491650a4ac02b48/c83ae/image-20200818172004055.png 1180w,\n/static/a6942057a32f062b1491650a4ac02b48/c1b63/image-20200818172004055.png 1200w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<br>\n<br>\n<hr>\n<br>\n<br>\n<br>\n<br>\n<ul>\n<li>\n<p>참고:</p>\n<blockquote>\n<ul>\n<li>아마추어 퀀트, blog.naver.com/chunjein</li>\n<li>mino-park7. 2018.10.12. \"BERT 논문정리\". <a href=\"https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w</a>. Mino-Park7 NLP Blog</li>\n<li>ebb and flow. 2020. 2. 12. \"[BERT] BERT에 대해 쉽게 알아보기1 - BERT는 무엇인가, 동작 구조\". <a href=\"https://ebbnflow.tistory.com/151\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://ebbnflow.tistory.com/151</a></li>\n<li>Zhao HG. \"keras-bert\". <a href=\"https://github.com/CyberZHG/keras-bert\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/CyberZHG/keras-bert</a></li>\n<li>Google Research. \"2/128 (BERT-Tiny)\". <a href=\"https://github.com/google-research/bert\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/google-research/bert</a>. bert</li>\n<li>SKTBrain. \"KoBERT\". <a href=\"https://github.com/SKTBrain/KoBERT\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/SKTBrain/KoBERT</a></li>\n</ul>\n</blockquote>\n</li>\n</ul>","excerpt":"NLP  Bidirectional Encoder Representations Form Transformer transformer의 encoder만 사용한다.  Pre-traning과 fine-tuning으로 활용한다. 즉, transformer…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/NLP_BERT_1/#nlp\">NLP</a></p>\n<ul>\n<li>\n<p><a href=\"/NLP_BERT_1/#code-classlanguage-textbertcode\"><code class=\"language-text\">BERT</code></a></p>\n<ul>\n<li><a href=\"/NLP_BERT_1/#%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9B%90%EB%A6%AC\">알고리즘 원리</a></li>\n<li><a href=\"/NLP_BERT_1/#%EC%A3%BC%EC%9D%98\">주의</a></li>\n<li><a href=\"/NLP_BERT_1/#code\">CODE</a></li>\n<li><a href=\"/NLP_BERT_1/#%ED%99%9C%EC%9A%A9\">활용</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/NLP_BERT_1/"},"frontmatter":{"title":"NLP BERT","date":"Aug 18, 2020","tags":["NLP","BERT","XLNet"],"keywords":["JyneeEarth","jynee"],"update":"Aug 20, 2020"}}},"pageContext":{"slug":"/NLP_BERT_1/","series":[],"lastmod":"2020-08-20"}},"staticQueryHashes":["3649515864","694178885"]}