{"componentChunkName":"component---src-templates-post-tsx","path":"/NLP응용_4/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"nlp-분야에서-딥러닝의-고급-응용\" style=\"position:relative;\"><a href=\"#nlp-%EB%B6%84%EC%95%BC%EC%97%90%EC%84%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98-%EA%B3%A0%EA%B8%89-%EC%9D%91%EC%9A%A9\" aria-label=\"nlp 분야에서 딥러닝의 고급 응용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NLP 분야에서 딥러닝의 고급 응용</h1>\n<ul>\n<li>DMN</li>\n<li>\n<p>Ask Me Anything</p>\n<ul>\n<li>attention score layer</li>\n<li>story layer</li>\n<li>episodic memory layer</li>\n<li>answer layer</li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<h2 id=\"텍스트-자동-생성\" style=\"position:relative;\"><a href=\"#%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%9E%90%EB%8F%99-%EC%83%9D%EC%84%B1\" aria-label=\"텍스트 자동 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>텍스트 자동 생성</h2>\n<p>예제 문장: I love you very much</p>\n<ul>\n<li>\n<p>문자 단위의 시계열 데이터 생성</p>\n<ul>\n<li>\n<p>아래처럼 되도록 생성: 보통의 시계열 Batch data처럼 생성</p>\n<blockquote>\n<p> <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/78dfc4b9095769d7132abeeec32f0606/fe9e8/image-20200822112750536.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 22.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsSAAALEgHS3X78AAAAdElEQVQY032Oyw7DIAwE+f8vRSBRhHkYG2mbuEoTKWkPc9rdsV3rHcyMOeeNEAK890aMESLy2DvYc0dEyDlDVW+klEBUMcYwVNdj72TBlVLw2oQiaheMLfxcFax1Sr75D/aOa62j1moSG13GzPMi0r/fHbs3O1U4qDSKIQAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200822112750536\"\n        title=\"image-20200822112750536\"\n        src=\"/static/78dfc4b9095769d7132abeeec32f0606/fcda8/image-20200822112750536.png\"\n        srcset=\"/static/78dfc4b9095769d7132abeeec32f0606/12f09/image-20200822112750536.png 148w,\n/static/78dfc4b9095769d7132abeeec32f0606/e4a3f/image-20200822112750536.png 295w,\n/static/78dfc4b9095769d7132abeeec32f0606/fcda8/image-20200822112750536.png 590w,\n/static/78dfc4b9095769d7132abeeec32f0606/efc66/image-20200822112750536.png 885w,\n/static/78dfc4b9095769d7132abeeec32f0606/fe9e8/image-20200822112750536.png 1146w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p> y값을 word 단위가 아니고 characters(ex: a, b, c, ... z) 로 설정<br></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li>\n<p>LSTM으로 학습</p>\n<ul>\n<li>I love you를 넣어도 v가 나오게끔 신경망 속 activation은 softmax 함수를 쓴다.</li>\n<li>compile 시, loss 함수는 categorical_crossentropy 사용<br></li>\n</ul>\n</li>\n<li>\n<p>순서:</p>\n<ol>\n<li><strong>문장으로 이루어진 raw data 불러오기</strong></li>\n<li><strong>전처리</strong></li>\n<li>word 아니고 character 단위로 분류</li>\n<li>시계열 x data 생성. (위 예시의 x 처럼)</li>\n<li>\n<p>Converting indices into vectorized format</p>\n<ul>\n<li>X, Y를 np.zeros </li>\n</ul>\n</li>\n<li><strong>Model Building</strong></li>\n<li>softmax: 출력층의 값이 [0.3, 0.4, 0.8] 등으로 나오면 이 총합이 1이 나오게끔 확률분포 다시 계산.\n이때 나온 값들의 차이를 더 크게 조작하고 싶을 때, 베타가 들어간 식을 사용하여 계산.\n이렇게 하면 model.predict(x) 시, 원하는 문자의 수치가 나올 확률이 높아진다\n(역으로 원하지 않는 단어가 나올 확률은 적어진다.)</li>\n<li>softmax 함수를 쓰는 skip-gram은 계산량이 많단 단점이 있는데, 이를 SGNS가 보완한다.</li>\n<li><strong>예측치를 softmax 확률로 뽑아 다시 역연산(exp) 하는 함수 생성</strong></li>\n<li>\n<p>np.random.multinomial로 sampling</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">pred_indices</span><span class=\"token punctuation\">(</span>preds<span class=\"token punctuation\">,</span> metric<span class=\"token operator\">=</span><span class=\"token number\">1.0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n   preds <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>asarray<span class=\"token punctuation\">(</span>preds<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span><span class=\"token string\">'float64'</span><span class=\"token punctuation\">)</span>\n   preds <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span>preds<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> metric\n   exp_preds <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>preds<span class=\"token punctuation\">)</span>\n   preds <span class=\"token operator\">=</span> exp_preds<span class=\"token operator\">/</span>np<span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span>exp_preds<span class=\"token punctuation\">)</span>\n   probs <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>multinomial<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> preds<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n   <span class=\"token keyword\">return</span> np<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>probs<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&gt; * 다항 분포 (Multinomial distribution):\n&gt;\n&gt;   다항 분포는 여러 개의 값을 가질 수 있는 독립 확률변수들에 대한 확률분포로, 여러 번의 독립적 시행에서 각각의 값이 특정 횟수가 나타날 확률을 정의한다. 다항 분포에서 차원이 2인 경우 이항 분포가 된다.\n&gt;\n&gt;   &gt; 출처: [위키백과. 다항분포]([https://ko.wikipedia.org/wiki/%EB%8B%A4%ED%95%AD_%EB%B6%84%ED%8F%AC](https://ko.wikipedia.org/wiki/다항_분포))</code></pre></div>\n</li>\n<li><strong>Train &#x26; Evaluate the Model</strong></li>\n<li>\n<p>batch</p>\n<ol>\n<li>randint로 random하게 시작하도록 설정</li>\n</ol>\n</li>\n<li>\n<p>확률 임의 설정 후 단어 생성</p>\n<ul>\n<li>\n<p>[0.2, 0.7,1.2] 처럼 </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> diversity <span class=\"token keyword\">in</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.7</span><span class=\"token punctuation\">,</span><span class=\"token number\">1.2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span></code></pre></div>\n<blockquote>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">a <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.9</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nb <span class=\"token operator\">=</span> <span class=\"token number\">1.0</span>\ne <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>a<span class=\"token operator\">/</span>b<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>e<span class=\"token operator\">/</span>np<span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span>e<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>'0.9'처럼 유독 하나의 값이 클 경우 다른 값들과의 차이가 더 커짐 </p>\n<table>\n<thead>\n<tr>\n<th>print(e/np.sum(e))<br/>[0.40175958 0.2693075  0.32893292]</th>\n<th>print(e/np.sum(e))<br/>[0.47548496 0.23611884 0.2883962 ]</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a = np.array([0.6, 0.2, 0.4])<br/>b = 1.0<br/>e = np.exp(a/b)</td>\n<td>a = np.array([0.9, 0.2, 0.4])<br/>b = 1.0<br/>e = np.exp(a/b)</td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n</li>\n</ul>\n</li>\n<li>\n<p>model.predict(x):</p>\n<ul>\n<li>예측(predict): model.predict(x) = > [0.01, 0.005, 0.3, 0.8 ...]</li>\n</ul>\n</li>\n<li>\n<p>문자 추출:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">sys<span class=\"token punctuation\">.</span>stdout<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span>pred_char<span class=\"token punctuation\">)</span>\nsys<span class=\"token punctuation\">.</span>stdout<span class=\"token punctuation\">.</span>flush<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ol>\n</li>\n</ul>\n<br>\n<br>\n<h2 id=\"code-classlanguage-textdmncode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textdmncode\" aria-label=\"code classlanguage textdmncode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">DMN</code></h2>\n<ul>\n<li>\n<p>Dynamic Memory Networks</p>\n<ul>\n<li>아래 5가지의 N/W가 결합되어 있는 모습</li>\n<li><code class=\"language-text\">Input Module</code></li>\n<li><code class=\"language-text\">Question Module</code></li>\n<li><code class=\"language-text\">Episodic Memory Module</code></li>\n<li><code class=\"language-text\">Answer Module</code></li>\n<li><strong><code class=\"language-text\">attention score N/W</code></strong> (FNN)<br></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"code-classlanguage-textask-me-anythingcode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textask-me-anythingcode\" aria-label=\"code classlanguage textask me anythingcode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Ask Me Anything</code></h3>\n<ul>\n<li>Q → A: Question &#x26; Answering</li>\n<li>DL 사용</li>\n<li>논문 저자는 GRU 사용</li>\n<li>특징: Q&#x26;A를 기억하는 하나의 경험 단위인 Episode를 기억하는 장치가 있다.</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" checked disabled> 순서:</p>\n<ol>\n<li>Input 문장(text sequence)과 </li>\n<li>attention 연산이 들어간 question 받아 </li>\n<li>attention 연산: attention score</li>\n<li>episodic memory 구성한 후, </li>\n<li>일반적인 답변을 줄 수 있게끔 구성한 네트워크</li>\n<li><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a3b05891cf69b95d604a666ba46c6279/74d4e/image-20200731122306179.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 60.810810810810814%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsSAAALEgHS3X78AAAB5klEQVQoz21Ti27bMAz0/3/bhg1Fk3RZ0qax3Tp+yA9ZsR62byRtZx0wAowU6UgeqXOkh4C6d+juXrw1bvEv++2uWfds/SPuL4bvIw70YcI8z5jI/2d8zPdhnCSIrTUeLoz/4Pgs0oMXIJu1Fvv9HrvdDr9PJxxeXmR1bknCBTPVo2h6xIUmIiOOxyPFHGCHAcaOiJjqOE0PKs5ZSTyGAGeX/bwy54Tt2jIzZSLBO8FRMHh8wpCpplUvIOMm9FTpUxlZ+T8XTUots3rLGmIWMLhRZqjIP+s7YcMyw616pjQ6Q4w8gz3eKfBuPe2DrCkl3KzWVtpjY2zZmkcH0QYqGiMJPSV05NdbA0tgdp7V/pzi9HpBkqZQSuH8dkWSFURglLjNIj64E4ucEpqVEXte98QsiNsw49drgm8/nvD95zOOpzOeng+4JJmM4KPsJLEhbMRzYE05OiD1gJ+H4mXd3NMP33GTZnkTaDtRoWXPGBZKRaOI+GXGVTZKVUiSGB/UVhzHuF6prSRBU9dyPz50OMuDtZ3G++Ui3veaHpBko7/IRusOZVmgbRqoqkKe5yiKHL3Wq6pmUYRgichgHUq6z2830SEXkS+FnfsfqFcqIjPjlRQjPvhZZLF9XmzL5xhkHAtmEhn9AbSIoN9wa3B/AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200731122306179\"\n        title=\"image-20200731122306179\"\n        src=\"/static/a3b05891cf69b95d604a666ba46c6279/fcda8/image-20200731122306179.png\"\n        srcset=\"/static/a3b05891cf69b95d604a666ba46c6279/12f09/image-20200731122306179.png 148w,\n/static/a3b05891cf69b95d604a666ba46c6279/e4a3f/image-20200731122306179.png 295w,\n/static/a3b05891cf69b95d604a666ba46c6279/fcda8/image-20200731122306179.png 590w,\n/static/a3b05891cf69b95d604a666ba46c6279/efc66/image-20200731122306179.png 885w,\n/static/a3b05891cf69b95d604a666ba46c6279/74d4e/image-20200731122306179.png 1157w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></li>\n</ol>\n<blockquote>\n<p>출처: Ankit Kumar외, 2016.05, Ask Me Anything: Dynamic Memory Networks for Natural Language Processing. 에서 'attention score' 추가<br></p>\n</blockquote>\n</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <strong>attention process</strong></li>\n<li>\n<p>attention score 계산</p>\n<ul>\n<li>attention score</li>\n<li>여러 문장에 Epsodic stroy가 있고 이것에 대한 답을 찾을 때 질문과 가장 관련이 높은 (저장된) 문장에 점수를 매기기 위한 계산을 수행함 </li>\n<li>즉, 답을 내기 위해 어떤 문장에 attention을 해야 하는지 attention score로 계산하는 알고리즘</li>\n</ul>\n</li>\n<li>기계번역, test 분류, part-of-speech tagging, image captioning, Dialog system(chatbot) 가능</li>\n<li>\n<p>mission: 주어진 Question의 의미를 파악할 수 있도록 네트워크를 구성해야 한다.</p>\n<ul>\n<li>'의미를 파악할 수 있도록' : 조응어(Anaphora resolution) 해석.</li>\n</ul>\n<br>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" checked disabled> <code class=\"language-text\">input module</code> = story module</p>\n<ol>\n<li>Input에 넣을 문장들을 1행으로 붙이고, [EOS] 로 구분</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>문장 1</th>\n<th></th>\n<th>문장 2</th>\n<th></th>\n<th>문장3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>When I was young, I passed test</td>\n<td>&#x3C;EOS></td>\n<td>But, Now Test is so crazy</td>\n<td>&#x3C;EOS></td>\n<td>Because The test level pretty hard more and more.</td>\n</tr>\n</tbody>\n</table>\n<ol start=\"2\">\n<li>Embedding layer 투입</li>\n<li>RNN 거쳐서</li>\n<li>Hidden layer 출력은 다시 n개의 문장(c1, c2, c3 등)으로 출력</li>\n<li>episodic memory module 투입</li>\n</ol>\n</li>\n</ul>\n<br>\n<ul>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" checked disabled> <code class=\"language-text\">Question module</code></p>\n<ol>\n<li>Question 문장 투입</li>\n<li>Embedding layer 투입</li>\n<li>RNN 거쳐서</li>\n<li>episodic memory module 투입</li>\n</ol>\n</li>\n</ul>\n<br>\n<ul>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" checked disabled> <code class=\"language-text\">episodic memory module</code></p>\n<ul>\n<li>input module(문장마다)+Question module+ atttention mechanism출력된 걸 반복해서 내부의 episodic memory를 반복 update</li>\n<li>\"어떻게 update?\"</li>\n<li>\n<p>input module의  embedding value과 atttention score 계산하여 RNN layer에 통과 시키기</p>\n<ul>\n<li>이때, <code class=\"language-text\">atttention score</code>:\natttention score layer의 출력층에서 나온 w인<code class=\"language-text\">g</code>를 input module의 출력값인 c1,c2,c3 등과 계산한 값</li>\n</ul>\n</li>\n<li>\n<p>Question module의 embedding value 값을 RNN layer에 통과 시키기</p>\n<ul>\n<li>이때, w = <code class=\"language-text\">Q</code></li>\n</ul>\n</li>\n<li>\n<p>2를 answer을 output할 Answer Module의 RNN layer에 통과 시킴</p>\n<ul>\n<li>이때, w = <code class=\"language-text\">m</code></li>\n</ul>\n</li>\n<li>episodic memory module의 RNN layer를 반복할 때마다 <code class=\"language-text\">attetion score</code>계산</li>\n<li>\n<p>그렇게 해서 나온 attetion score 값 중 가장 높은 것(g) 찾음</p>\n<blockquote>\n<ul>\n<li><code class=\"language-text\">atttention mechanism</code></li>\n<li>그렇게 해서 나온 attetion score 값 중 가장 높은 것(g) 찾고</li>\n<li>\n<p>g로 다시 네트워크 형성</p>\n<ul>\n<li>2층 구조가 됨 </li>\n</ul>\n</li>\n<li>\n<p>memory update mechanism</p>\n<ul>\n<li>attention score로 가중 평균</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>용어: </p>\n<ul>\n<li><code class=\"language-text\">c</code>: Input의 출력</li>\n<li><code class=\"language-text\">m</code>: episodic memory module 출력값이자 attention score의 입력값</li>\n<li><code class=\"language-text\">q</code>: question layer의 출력값</li>\n<li><code class=\"language-text\">g</code>: attention score의 출력값</li>\n</ul>\n<br>\n</li>\n</ul>\n<br>\n<h3 id=\"code\" style=\"position:relative;\"><a href=\"#code\" aria-label=\"code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>code:</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/9d93b7c1a2b416814ac294610a2f0c57/c5bb3/image-20200812161218399.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 141.2162162162162%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAYAAABh2p9gAAAACXBIWXMAAAsSAAALEgHS3X78AAAFR0lEQVRIx3VWaVvaaBTl/3+azjNLq+1oizp1aREEK1oBQRZlDzthSSAJewhLEmhtz9w3QDva9sN9IiSc3HvOuefVZsw7eFyKdZ3PJCijDOaGCHMpYaLxmI+aGEsVlJMhFAv34MtJ5HMx1Pg0FkYP+kyBbQXS/Q6od60yjS5qgwB8iQNcRexweJ+hXIjCHAgo3vnh93vgOTvCnn0bfp8bGxzbo+5mHeiaBINqMhQgCRXEwj4c7P2Dvb2X4JJBzNJRfCok0CgmcPHhPexvthCnF+jT1WSPAFnLRq8OrVVCjYvROFGrAtRBNHwFoZnDpMNjQWPP6Llep45yI4y84kGp44VKtNg2nFmAVJ8WA+g0VkepAg9jCI08enIdjRqHHBfFkt0nShZmH5raQvzeR0A1TKbNNYesq9lqXAaqUHeJ6DXcvh0keCfqnTCkMQkgxsClIjAJbEkCtJp5eNzHNPI2gv4LEqVvYaxGpgdYzXoNJImPw8PXcJySEE47vFcnaClpDNUq5HbZUpNVu1VEMHAB++tthOiqrxuyGdUsjEYOJnU2UXhUKinrzaGAF+eu9ySKH1NVxkxTLA6XNCr7oaaKSGcDCEc+IJWmZ6ZkL0tlpQZDKEInQL1LRBPZTeLLdW5HJO1AuX4LdcpjMKyiJRQsQNaNTtznZA+ykgvl7hVGWpUAewQ4aMKo52A0CzCHIgb9BvwfPThybsHp24InsItaN4TuoEiAxTWgYhXjk5VuCbvyso0pxvibk/fEYhwB/zlS5LeTQzuyyQgSsSDdUzAd030a+dMacOUK5ZuYm7KxL5b6ANqojXY1gxGNXaGVuidxqpUk8cZhNpEw1doEmF91xEaebbz7fx+zDucypnoTg1EFgpCllSPCx23LBmwDRCFveZN9ZoAT615vPeL3Yi+yVk/p5xArkpoFB+45D9rDBMYj0QJhIzI1Le+ZK8DN5x9DZT3yldeJa68b8VgAHy9dcLzbx5nrkATIQ5ZKUIdN60HW1Qqw9W3snwJm0rcIBS9wsP8KL54/w+vd55QqSfQ7DcitMnErWRwvzSeAv+qwRLmWuA+gVIgjmwmjkLuD0s1jZjbQEOPoUCYONPIf45AWQBuJ6w6VnwO2xIL1x9cHFQ+fR8CXCWQtgVj5GOc3u3AHt8FLt/hK30vVNCa0fibbll8BMuKZCJbslGkDShm1LyKbiuHkeB+X5y4ELpzoyhW0qEOVJydQVho/CLM2ttDgoFK0P1CH004NQjYMlcTgKdovKZEziRsM6xyGtI6jbg1j2qgvxtrcT4SxfCgKORTES4Ryh7iJ7yFeO0WFQDp3N1gSSJu7QyV8De72CnUKjjTxLZADFmsen3Jpk9sl5PMRnLkPcO134vR0H3Y6J1KpkLXXtyEvXNTpPrnATdejt2/gcZ2QC+q0hkMs5v3HgBLlGl9Ko0p15jzCzqu/sPXidxxTJvYo7nPZKDhSf59e8vzv3/Bm70+U+DDaShbqhEdX5b7v9mZkFj1yJ4d0moJgKoO9pE984YuGGimbvAvScTAlPw4h9u5Rln1oj+4hDKJoDsJg62tuROH5BD7GaYzIFjL8B2vFmOdYIAxpZEZJouSGOIySJzniro/PxogA+pbhF1SPRmbJcuE+ReD6HPVaZnVQLYdghn/77w5evfwD75w76E4LUDQOc52EoK1h0fVUFGtkZht2TDocB8ilbmGy1aKxWWcBnwdO51tUMjHozTKWLR4GxRsLY2Mi/9zY/S5vdZMiMIEOb5OOTTOfwIjOF/ZvRijohUQvNYlTnUT6lvCUj4be+cGL/wHNlt10zii32AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200812161218399\"\n        title=\"image-20200812161218399\"\n        src=\"/static/9d93b7c1a2b416814ac294610a2f0c57/fcda8/image-20200812161218399.png\"\n        srcset=\"/static/9d93b7c1a2b416814ac294610a2f0c57/12f09/image-20200812161218399.png 148w,\n/static/9d93b7c1a2b416814ac294610a2f0c57/e4a3f/image-20200812161218399.png 295w,\n/static/9d93b7c1a2b416814ac294610a2f0c57/fcda8/image-20200812161218399.png 590w,\n/static/9d93b7c1a2b416814ac294610a2f0c57/c5bb3/image-20200812161218399.png 680w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>\n<p>순서</p>\n<ol>\n<li><strong>패키지 불러오기 <br></strong></li>\n<li><strong>전처리</strong></li>\n<li>2-1. Document Data processing(raw data)<br></li>\n<li><strong>데이터 불러오기</strong></li>\n<li>3-1. Raw Document Data 불러오기 </li>\n<li>3-2. train/test data split 해서 가져오기<br></li>\n<li><strong>vocab 만들기</strong></li>\n<li>\n<p>4-1. Train &#x26; Test data를 한꺼번에 묶어서 vocab을 만듦</p>\n<ul>\n<li><code class=\"language-text\">collections.Counter()</code></li>\n</ul>\n</li>\n<li>\n<p>4-2. word2indx / indx2word 만듦</p>\n<ul>\n<li><code class=\"language-text\">padding</code><br></li>\n</ul>\n</li>\n<li><strong>벡터화</strong></li>\n<li>\n<p>5-1. vocab_size 변수 설정</p>\n<ul>\n<li><code class=\"language-text\">len(word2indx)</code></li>\n</ul>\n</li>\n<li>\n<p>5-2. story와 question 각각의 max len 변수 설정</p>\n<ul>\n<li>뒤에서 padding 맞춰 주려고 max len 설정해줌</li>\n</ul>\n</li>\n<li>\n<p>5-3. 벡터화 시킴</p>\n<ul>\n<li><code class=\"language-text\">raw data</code>와 <code class=\"language-text\">word2indx</code>, 각 모듈(story, question)의 <code class=\"language-text\">maxlen</code>을 함수에 넣어 <code class=\"language-text\">padding</code>, <code class=\"language-text\">categorical</code> 등을 진행함<br></li>\n</ul>\n</li>\n<li><strong>모델 빌드</strong></li>\n<li>6-1. train/test data split</li>\n<li>\n<p>이때, Xstrain, Xqtrain, Ytrain = <code class=\"language-text\">data_vectorization</code>(data<em>train, word2indx, story</em>maxlen, question<em>maxlen) 이고,\ndata</em>vectorization의 return 값은 </p>\n<ul>\n<li><code class=\"language-text\">pad_sequences(Xs, maxlen=story_maxlen)</code></li>\n<li><code class=\"language-text\">pad_sequences(Xq, maxlen=question_maxlen)</code></li>\n<li><code class=\"language-text\">to_categorical(Y, num_classes=len(word2indx))</code></li>\n</ul>\n</li>\n<li>6-2. Model Parameters 설정</li>\n<li>6-3. Inputs</li>\n<li>6-4. Story encoder embedding</li>\n<li>6-5. Question encoder embedding</li>\n<li>\n<p>6-6. 모듈 만들어줌</p>\n<ul>\n<li>Question module는 위에서 만들어준 걸로 사용함</li>\n<li>\n<p>attention score layer</p>\n<ul>\n<li><strong>dot</strong>으로 만듦</li>\n</ul>\n</li>\n<li>\n<p>story module</p>\n<ul>\n<li>이 layer는 story layer의 input에서 시작하여 question layer는 건너뛰고 또 다른 embedding layer를 거쳐, 추후에 dot layer와 add를 <strong>해주려고 만듦</strong></li>\n</ul>\n</li>\n<li>\n<p>episodic memory module</p>\n<ul>\n<li>dot한 layer와 바로 위의 story<em>encoder</em>c를 <strong>add</strong>해서 만들어지게 됨</li>\n</ul>\n</li>\n<li>\n<p>answer module</p>\n<ul>\n<li>episodic memory layer(response) + quetion layer<br></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>compile</strong></li>\n</ol>\n<blockquote>\n<p>model = Model(inputs=<strong>[story<em>input, question</em>input]</strong>, outputs=output)</p>\n<ul>\n<li>input에 story와 question 두 개 써줬단 것!<br></li>\n</ul>\n</blockquote>\n<ol start=\"8\">\n<li>**fit **</li>\n<li><strong>loss plot</strong></li>\n<li><strong>정확도 측정(predict)</strong></li>\n<li><strong>적용</strong></li>\n</ol>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/91702bc3f10f90de85396cbf5fb4ba3e/df56e/image-20200803190130197.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 99.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAAClklEQVQ4y51Uia6bMBDk/7+vUl+TkAQIl8Hc9zXdcUKVRnmRWksWxqxnZ2fHWNu2IcsmxGpCrgokKobWGkqep9MRtn3C5eJgmjaYsW4Yux5xGCJNUkRRhOPhgMPxiCJVsPphkeAV0yyBfY++qZDqSkArRLGWAxp53mCeFyzrgq2fMNcdWl2gLSU2TVHXNZqmlUQNrHHP/BhL30r2xDBVwiAMYziOg9vtJkwvaOVwlmWSlIlyw/B6vcIPAnjXM6y8aNCTmcy8KCRYI2Mpno9CDpRlicAPDBMClRJTF6XZ5wyldO5rnaHQCSzPi4xmu262bSOQoJvrQQURMgHlt8v5DN/34QibNFbCUBLk2R/mjusi9FxYw7ii61q0bWsOsgxqUtfCINWo6gZN2yGMYhSimRKmStaDNIYNHYcBmzRqW1Zs1LDvF0M3zzMD6LqO0YwZA8dDJp1z7V8iwRXxTbS8nJBKJfPQQzqFsakh6DJbacAEa1k2DMOKcdz4XZ6ree+6GVVZQ8c+sutPKPsHtPOFwv1CqRXWSZjNgzhDgIQp2bK9Fju7PTYIvqz3Ncc4jkbDIIxMyYlIEKsEZVXdHSFlzmTxwCCwtR9+HXsSDjqgFhAt+oViD6UUVBwb3dd1/SvW2l9e5/MgoLGM2CQWIFqFoAR8JWG9Y2ZumGSuhNXuN67vN6IxgK40jabmmnaapunO8BMggQoxMp+0093A2oBwcj9JEhOzLMt7wGfQ19F1nWFKZjQ0mRHsWfNvGb7TlIAsm9eQTMmQe8/n3jL8TgKCsERO/hB4AciS375tyifG9OUgV42NITjL5fpjyZ9AWR7LDcSLZ/lZUEdK8F8l7ww5CUI/UkMyfI75Zw1paGpIH+4/3t0yjP0NP0kJW9zPtykAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200803190130197\"\n        title=\"image-20200803190130197\"\n        src=\"/static/91702bc3f10f90de85396cbf5fb4ba3e/fcda8/image-20200803190130197.png\"\n        srcset=\"/static/91702bc3f10f90de85396cbf5fb4ba3e/12f09/image-20200803190130197.png 148w,\n/static/91702bc3f10f90de85396cbf5fb4ba3e/e4a3f/image-20200803190130197.png 295w,\n/static/91702bc3f10f90de85396cbf5fb4ba3e/fcda8/image-20200803190130197.png 590w,\n/static/91702bc3f10f90de85396cbf5fb4ba3e/efc66/image-20200803190130197.png 885w,\n/static/91702bc3f10f90de85396cbf5fb4ba3e/c83ae/image-20200803190130197.png 1180w,\n/static/91702bc3f10f90de85396cbf5fb4ba3e/df56e/image-20200803190130197.png 1188w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<br>\n<ul>\n<li>\n<p>패키지 불러오기</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> collections\n<span class=\"token keyword\">import</span> itertools\n<span class=\"token keyword\">import</span> nltk\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n<span class=\"token keyword\">import</span> random\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Input<span class=\"token punctuation\">,</span> Dense<span class=\"token punctuation\">,</span> Activation<span class=\"token punctuation\">,</span> Dropout\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> LSTM<span class=\"token punctuation\">,</span> Permute\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Embedding\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Add<span class=\"token punctuation\">,</span> Concatenate<span class=\"token punctuation\">,</span> Dot\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Model\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>sequence <span class=\"token keyword\">import</span> pad_sequences\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>utils <span class=\"token keyword\">import</span> to_categorical</code></pre></div>\n</li>\n</ul>\n<br>\n<h4 id=\"전처리\" style=\"position:relative;\"><a href=\"#%EC%A0%84%EC%B2%98%EB%A6%AC\" aria-label=\"전처리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>전처리</h4>\n<ul>\n<li>\n<p>Raw Document Data processing </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 문서 내용 예시 : 3문장의 story(episodic story)</span>\n<span class=\"token comment\"># 현재까지 NLP는 한 문장 안에서 단어들의 의미를 파악하고 이를 통해 한 개의 문장을 분석하는 수준에 그쳐있다(step1 수준).</span>\n<span class=\"token comment\"># 이때, episodic story는 '한 문장 안'이 아니라 문장 '간'의 단어들의 관계(=문장 간의 관계)를 파악하는 데에 의의가 있으며 따라서 매우 분석이 어렵다(이는 시의 영역이다. step2). 나아가 문단(Paragrape) 간의 관계까지 파악할 필요가 있다(이는 소설의 영역이다. step3).  </span>\n<span class=\"token triple-quoted-string string\">\"\"\"\ndata 생김새\n# 1 Mary moved to the bathroom.\\n\n# 2 Daniel went to the garden.\\n\n# 3 Where is Mary?\\tbathroom\\t1 \n\"\"\"</span>\n<span class=\"token comment\">## Question과 answer은 #\\t : tab 으로 구분되어 있다.</span>\n<span class=\"token comment\"># Return: # 3개(Stories, question, answer)를 return 해줌 </span>\n<span class=\"token comment\"># Stories = ['Mary moved to the bathroom.\\n', 'John went to the hallway.\\n']</span>\n<span class=\"token comment\"># questions = 'Where is Mary? '</span>\n<span class=\"token comment\"># answers = 'bathroom'</span>\n<span class=\"token comment\">#----------------------------------------------------------------------------</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">get_data</span><span class=\"token punctuation\">(</span>infile<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  stories<span class=\"token punctuation\">,</span> questions<span class=\"token punctuation\">,</span> answers <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n  story_text <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n  fin <span class=\"token operator\">=</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>infile<span class=\"token punctuation\">,</span> <span class=\"token string\">\"r\"</span><span class=\"token punctuation\">)</span> \n  <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> fin<span class=\"token punctuation\">:</span>\n      lno<span class=\"token punctuation\">,</span> text <span class=\"token operator\">=</span> line<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">\" \"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n      <span class=\"token keyword\">if</span> <span class=\"token string\">\"\\t\"</span> <span class=\"token keyword\">in</span> text<span class=\"token punctuation\">:</span> <span class=\"token comment\"># >data 생김새&lt;에서 \\t가 있는 3번을 말하는 것임 </span>\n          question<span class=\"token punctuation\">,</span> answer<span class=\"token punctuation\">,</span> _ <span class=\"token operator\">=</span> text<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">\"\\t\"</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#\\t으로 구분해서 quetion과 answer 구분  # 숫자(ex:1)</span>\n          stories<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>story_text<span class=\"token punctuation\">)</span> \n          questions<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>question<span class=\"token punctuation\">)</span>\n          answers<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>answer<span class=\"token punctuation\">)</span> <span class=\"token comment\"># >data 생김새&lt; 에서 3번의 \\t 앞의 answer문 </span>\n          story_text <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n      <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n          story_text<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 사실상 해당 함수는 else 부터 시작하는 것. </span>\n  fin<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">return</span> stories<span class=\"token punctuation\">,</span> questions<span class=\"token punctuation\">,</span> answers</code></pre></div>\n</li>\n</ul>\n<br>\n<h4 id=\"데이터-불러오기\" style=\"position:relative;\"><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0\" aria-label=\"데이터 불러오기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>데이터 불러오기</h4>\n<ul>\n<li>\n<p>Raw Document Data 불러오기 </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Train_File <span class=\"token operator\">=</span> <span class=\"token string\">\"./dataset/qa1_single-supporting-fact_train.txt\"</span>\nTest_File <span class=\"token operator\">=</span> <span class=\"token string\">\"./dataset/qa1_single-supporting-fact_test.txt\"</span></code></pre></div>\n<br>\n</li>\n<li>\n<p>get the data</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">data_train <span class=\"token operator\">=</span> get_data<span class=\"token punctuation\">(</span>Train_File<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 출력: stories, questions, answers</span>\ndata_test <span class=\"token operator\">=</span> get_data<span class=\"token punctuation\">(</span>Test_File<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n\\nTrain observations:\"</span><span class=\"token punctuation\">,</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>data_train<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"Test observations:\"</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>data_test<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"\\n\\n\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>Train observations: 10000 Test observations: 1000 </p>\n</blockquote>\n</li>\n</ul>\n<br>\n<h4 id=\"vocab-만들기\" style=\"position:relative;\"><a href=\"#vocab-%EB%A7%8C%EB%93%A4%EA%B8%B0\" aria-label=\"vocab 만들기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>vocab 만들기</h4>\n<ul>\n<li>\n<p>Building Vocab dictionary from Train &#x26; Test data </p>\n<ul>\n<li>Train &#x26; Test data를 한꺼번에 묶어서 vocab을 만듦 </li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">dictnry <span class=\"token operator\">=</span> collections<span class=\"token punctuation\">.</span>Counter<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># collections.Counter() 이용하여 단어들이 사용된 count 조회할 예정 </span>\n<span class=\"token keyword\">for</span> stories<span class=\"token punctuation\">,</span> questions<span class=\"token punctuation\">,</span> answers <span class=\"token keyword\">in</span> <span class=\"token punctuation\">[</span>data_train<span class=\"token punctuation\">,</span> data_test<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">for</span> story <span class=\"token keyword\">in</span> stories<span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">for</span> sent <span class=\"token keyword\">in</span> story<span class=\"token punctuation\">:</span>\n          <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n              dictnry<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+=</span><span class=\"token number\">1</span>\n  <span class=\"token keyword\">for</span> question <span class=\"token keyword\">in</span> questions<span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>question<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n          dictnry<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token operator\">+=</span><span class=\"token number\">1</span>\n  <span class=\"token keyword\">for</span> answer <span class=\"token keyword\">in</span> answers<span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>answer<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n          dictnry<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token operator\">+=</span><span class=\"token number\">1</span></code></pre></div>\n</li>\n<li>\n<p>word2indx / indx2word 만듦</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># collections.Counter()과 구조는 같은데, 단어 index는 1부터 시작하게 바꿔줌.  </span>\nword2indx <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>w<span class=\"token punctuation\">:</span><span class=\"token punctuation\">(</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span>w<span class=\"token punctuation\">,</span>_<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>dictnry<span class=\"token punctuation\">.</span>most_common<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span> \nword2indx<span class=\"token punctuation\">[</span><span class=\"token string\">\"PAD\"</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token number\">0</span> <span class=\"token comment\"># padding</span>\nindx2word <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>v<span class=\"token punctuation\">:</span>k <span class=\"token keyword\">for</span> k<span class=\"token punctuation\">,</span>v <span class=\"token keyword\">in</span> word2indx<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span> \n<span class=\"token comment\"># 위에서 word2indx[\"PAD\"] 해줘서 print(indx2word) 하면, 맨 마지막에 ',0: 'PAD'' 가 들어가 있다. </span></code></pre></div>\n</li>\n</ul>\n<br>\n<h4 id=\"벡터화\" style=\"position:relative;\"><a href=\"#%EB%B2%A1%ED%84%B0%ED%99%94\" aria-label=\"벡터화 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>벡터화</h4>\n<ul>\n<li>\n<p><code class=\"language-text\">vocab_size</code> 변수 설정</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">vocab_size <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>word2indx<span class=\"token punctuation\">)</span> <span class=\"token comment\"># vocab_size = 22 -> 즉 21개 단어만이 쓰인 것(하나는 패딩)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"vocabulary size:\"</span><span class=\"token punctuation\">,</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>word2indx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>word2indx<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<ul>\n<li>vocabulary size: 22</li>\n<li>{'to': 1, 'the': 2, '.': 3, 'where': 4, 'is': 5, '?': 6, 'went': 7, 'john': 8, 'sandra': 9, 'mary': 10, 'daniel': 11, 'bathroom': 12, 'office': 13, 'garden': 14, 'hallway': 15, 'kitchen': 16, 'bedroom': 17, 'journeyed': 18, 'travelled': 19, 'back': 20, 'moved': 21, 'PAD': 0}<br></li>\n</ul>\n</blockquote>\n</li>\n<li>\n<p>story와 question 각각의 <code class=\"language-text\">max len</code> 변수 설정</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">story_maxlen <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nquestion_maxlen <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n<span class=\"token keyword\">for</span> stories<span class=\"token punctuation\">,</span> questions<span class=\"token punctuation\">,</span> answers <span class=\"token keyword\">in</span> <span class=\"token punctuation\">[</span>data_train<span class=\"token punctuation\">,</span> data_test<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">for</span> story <span class=\"token keyword\">in</span> stories<span class=\"token punctuation\">:</span>\n      story_len <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n      <span class=\"token keyword\">for</span> sent <span class=\"token keyword\">in</span> story<span class=\"token punctuation\">:</span>\n          swords <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span>\n          story_len <span class=\"token operator\">+=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>swords<span class=\"token punctuation\">)</span>\n      <span class=\"token keyword\">if</span> story_len <span class=\"token operator\">></span> story_maxlen<span class=\"token punctuation\">:</span>\n          story_maxlen <span class=\"token operator\">=</span> story_len <span class=\"token comment\"># story 중 가장 긴 문장 찾기(=단어가 가장 많은 거)</span>\n          \n  <span class=\"token keyword\">for</span> question <span class=\"token keyword\">in</span> questions<span class=\"token punctuation\">:</span>\n      question_len <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>question<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n      <span class=\"token keyword\">if</span> question_len <span class=\"token operator\">></span> question_maxlen<span class=\"token punctuation\">:</span> \n          question_maxlen <span class=\"token operator\">=</span> question_len <span class=\"token comment\"># question 중 가장 긴 문장 찾기 </span>\n          \n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"Story maximum length:\"</span><span class=\"token punctuation\">,</span> story_maxlen<span class=\"token punctuation\">,</span> <span class=\"token string\">\"Question maximum length:\"</span><span class=\"token punctuation\">,</span> question_maxlen<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>Story maximum length: 14 Question maximum length: 4<br></p>\n</blockquote>\n</li>\n<li>\n<p>Converting data into <code class=\"language-text\">Vectorized</code> form </p>\n<ul>\n<li>위의 문장을 수치화함</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">data_vectorization</span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">,</span> word2indx<span class=\"token punctuation\">,</span> story_maxlen<span class=\"token punctuation\">,</span> question_maxlen<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>  \n  Xs<span class=\"token punctuation\">,</span> Xq<span class=\"token punctuation\">,</span> Y <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n  stories<span class=\"token punctuation\">,</span> questions<span class=\"token punctuation\">,</span> answers <span class=\"token operator\">=</span> data\n  <span class=\"token keyword\">for</span> story<span class=\"token punctuation\">,</span> question<span class=\"token punctuation\">,</span> answer <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>stories<span class=\"token punctuation\">,</span> questions<span class=\"token punctuation\">,</span> answers<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n      xs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span>word2indx<span class=\"token punctuation\">[</span>w<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> s <span class=\"token keyword\">in</span> story<span class=\"token punctuation\">]</span> <span class=\"token comment\"># vocab의 index로 단어를 표시한다(수치화한다)</span>\n      xs <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>itertools<span class=\"token punctuation\">.</span>chain<span class=\"token punctuation\">.</span>from_iterable<span class=\"token punctuation\">(</span>xs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># chain.from_iterable(['ABC', 'DEF']) --> ['A', 'B', 'C', 'D', 'E', 'F']</span>\n      xq <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>word2indx<span class=\"token punctuation\">[</span>w<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>question<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n      Xs<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>xs<span class=\"token punctuation\">)</span>\n      Xq<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>xq<span class=\"token punctuation\">)</span>\n      Y<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>word2indx<span class=\"token punctuation\">[</span>answer<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># Y = answer</span>\n  <span class=\"token keyword\">return</span> pad_sequences<span class=\"token punctuation\">(</span>Xs<span class=\"token punctuation\">,</span> maxlen<span class=\"token operator\">=</span>story_maxlen<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> pad_sequences<span class=\"token punctuation\">(</span>Xq<span class=\"token punctuation\">,</span> maxlen<span class=\"token operator\">=</span>question_maxlen<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\\\n         to_categorical<span class=\"token punctuation\">(</span>Y<span class=\"token punctuation\">,</span> num_classes<span class=\"token operator\">=</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>word2indx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n         <span class=\"token comment\"># 가장 긴 문장(maxlen=story_maxlen))을 기준으로 문장의 길이를 통일시킨다. 이것보다 짧은 부분은 padding(0)으로 채움</span>\n         <span class=\"token comment\"># y: anwser이고, 여기선 한 단어로 나온다. 즉, 한 단어 = 숫자 1개 </span>\n         <span class=\"token comment\"># to_categorical: 안 쓰고 sparse categorical을 써도 ok </span></code></pre></div>\n<blockquote>\n<ul>\n<li>함수 data_vectorization() 中</li>\n<li>\n<p>xs = [[word2indx[w.lower()] for w in nltk.word_tokenize(s)] for s in story]</p>\n<p><em>xs</em> >\nOut[19]: [[8, 7, 20, 1, 2, 13, 3], [10, 19, 1, 2, 17, 3]]\n<em>story</em> >\nOut[20]: ['John went back to the office.\\n', 'Mary travelled to the bedroom.\\n']</p>\n</li>\n<li>xs = list(itertools.chain.from_iterable(xs))\n<em>xs</em> >\nOut[22]: [8, 7, 20, 1, 2, 13, 3, 10, 19, 1, 2, 17, 3]</li>\n<li>Xs.append(xs) 해줌으로써\nfor문 통해서 output 된 것들을 list 형태로 축적해줌 </li>\n<li>padding 해줌\npad<em>sequences(Xs, maxlen=story</em>maxlen) # story_maxlen = 14\nOut[31]: array([[ 0,  8,  7, 20,  1,  2, 13,  3, 10, 19,  1,  2, 17,  3]])</li>\n<li>pad<em>sequences(Xs, maxlen=story</em>maxlen)\nOut[31]: array([[ 0,  8,  7, 20,  1,  2, 13,  3, 10, 19,  1,  2, 17,  3]])</li>\n<li>pad<em>sequences(Xq, maxlen=question</em>maxlen)\nOut[32]: array([], shape=(0, 4), dtype=int32)</li>\n<li>to<em>categorical(Y, num</em>classes=len(word2indx))\nOut[33]: array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<br>\n<h4 id=\"모델-빌드\" style=\"position:relative;\"><a href=\"#%EB%AA%A8%EB%8D%B8-%EB%B9%8C%EB%93%9C\" aria-label=\"모델 빌드 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>모델 빌드</h4>\n<ul>\n<li>\n<p>train/test data split </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Xstrain<span class=\"token punctuation\">,</span> Xqtrain<span class=\"token punctuation\">,</span> Ytrain <span class=\"token operator\">=</span> data_vectorization<span class=\"token punctuation\">(</span>data_train<span class=\"token punctuation\">,</span> word2indx<span class=\"token punctuation\">,</span> story_maxlen<span class=\"token punctuation\">,</span> question_maxlen<span class=\"token punctuation\">)</span>\nXstest<span class=\"token punctuation\">,</span> Xqtest<span class=\"token punctuation\">,</span> Ytest <span class=\"token operator\">=</span> data_vectorization<span class=\"token punctuation\">(</span>data_test<span class=\"token punctuation\">,</span> word2indx<span class=\"token punctuation\">,</span> story_maxlen<span class=\"token punctuation\">,</span> question_maxlen<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Train story\"</span><span class=\"token punctuation\">,</span>Xstrain<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span><span class=\"token string\">\"Train question\"</span><span class=\"token punctuation\">,</span> Xqtrain<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span><span class=\"token string\">\"Train answer\"</span><span class=\"token punctuation\">,</span> Ytrain<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span> <span class=\"token string\">\"Test story\"</span><span class=\"token punctuation\">,</span>Xstest<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span> <span class=\"token string\">\"Test question\"</span><span class=\"token punctuation\">,</span>Xqtest<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span> <span class=\"token string\">\"Test answer\"</span><span class=\"token punctuation\">,</span>Ytest<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p><em>print</em> >\nTrain story (10000, 14) Train question (10000, 4) Train answer (10000, 22)\nTest story (1000, 14) Test question (1000, 4) Test answer (1000, 22)</p>\n</blockquote>\n<br>\n</li>\n<li>\n<p>Model Parameters 설정</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">EMBEDDING_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">128</span>\nLATENT_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">64</span>\nBATCH_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">64</span>\nNUM_EPOCHS <span class=\"token operator\">=</span> <span class=\"token number\">40</span></code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>Inputs</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">story_input <span class=\"token operator\">=</span> Input<span class=\"token punctuation\">(</span>shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>story_maxlen<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># story_maxlen = 14</span>\nquestion_input <span class=\"token operator\">=</span> Input<span class=\"token punctuation\">(</span>shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>question_maxlen<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n</li>\n<li>\n<p><code class=\"language-text\">Story encoder embedding</code></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">story_encoder <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>vocab_size<span class=\"token punctuation\">,</span> <span class=\"token comment\"># vocab_size: 22</span>\n                        output_dim<span class=\"token operator\">=</span>EMBEDDING_SIZE<span class=\"token punctuation\">,</span> <span class=\"token comment\"># EMBEDDING_SIZE* = 128(한 단어를 128개의 vector로 표시, embedding layer의 colum 담당)</span>\n                        input_length<span class=\"token operator\">=</span>story_maxlen<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>story_input<span class=\"token punctuation\">)</span> <span class=\"token comment\"># story_maxlen = 14</span>\nstory_encoder <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>story_encoder<span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n</li>\n<li>\n<p><code class=\"language-text\">Question encoder embedding</code></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">question_encoder <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>vocab_size<span class=\"token punctuation\">,</span>\n                           output_dim<span class=\"token operator\">=</span>EMBEDDING_SIZE<span class=\"token punctuation\">,</span>\n                           input_length<span class=\"token operator\">=</span>question_maxlen<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>question_input<span class=\"token punctuation\">)</span>\nquestion_encoder <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>question_encoder<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<p><br><br></p>\n<h5 id=\"code-classlanguage-textattention-score-layercode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textattention-score-layercode\" aria-label=\"code classlanguage textattention score layercode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">attention score layer</code></h5>\n<ul>\n<li>\n<p>attention score layer</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">match <span class=\"token operator\">=</span> Dot<span class=\"token punctuation\">(</span>axes<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>story_encoder<span class=\"token punctuation\">,</span> question_encoder<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> </code></pre></div>\n<blockquote>\n<ul>\n<li>\n<p>Match between story and question: story and question를 dot 연산 수행.</p>\n<ul>\n<li>여기서 dot 연산은 attention score로 사용함 </li>\n</ul>\n</li>\n<li>\n<p>story<em>encoder = [None, 14, 128], question</em>encoder = [None, 4, 128]</p>\n<ul>\n<li>match = [None, 14, 4]</li>\n</ul>\n</li>\n<li>\n<p>axes=[2, 2]?  story D2(=128 = embedding vector)와 question D2(=128 = embedding vector)를 dot 해라</p>\n<ul>\n<li>즉, (x, 128)과 (128,y)로 한쪽을 transpose 시켜서 연산 수행</li>\n</ul>\n</li>\n<li>우선 story input의 embedding layer의 출력은 story_encoder = (None, 한 story에 사용된 최대 단어 개수(=14), embedding vector(128)) 이다. </li>\n<li>question input의 embedding layer의 출력은 question_encoder = (None, 한 question에 사용된 최대 단어 개수(=14), embedding vecotr(128)) 이다. </li>\n<li>dot -> (None)을 빼고 (row, colum)끼리(=14, 128)과 (128, 14)가 연산 수행 </li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<p><br><br></p>\n<h5 id=\"code-classlanguage-textstory-layercode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textstory-layercode\" aria-label=\"code classlanguage textstory layercode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">story layer</code></h5>\n<ul>\n<li>\n<p>story layer</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">story_encoder_c <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>vocab_size<span class=\"token punctuation\">,</span> <span class=\"token comment\"># vocab_size = 22</span>\n                          output_dim<span class=\"token operator\">=</span>question_maxlen<span class=\"token punctuation\">,</span> <span class=\"token comment\"># question_maxlen = 4 </span>\n                          input_length<span class=\"token operator\">=</span>story_maxlen<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>story_input<span class=\"token punctuation\">)</span> <span class=\"token comment\"># story_maxlen = 14 </span>\n\nstory_encoder_c <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>story_encoder_c<span class=\"token punctuation\">)</span> <span class=\"token comment\"># story_encoder_c.shap=(14, 4)</span></code></pre></div>\n<blockquote>\n<ul>\n<li>이 layer는 story layer의 input에서 시작하여 question layer는 건너뛰고 또 다른 embedding layer를 거쳐, 추후에 dot layer와 add를 하게 됨  </li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<br>\n<br>\n<h5 id=\"code-classlanguage-textepisodic-memory-layercode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textepisodic-memory-layercode\" aria-label=\"code classlanguage textepisodic memory layercode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">episodic memory layer</code></h5>\n<ul>\n<li>\n<p>episodic memory layer</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">response <span class=\"token operator\">=</span> Add<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>match<span class=\"token punctuation\">,</span> story_encoder_c<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># dot한 layer와 바로 위의 story_encoder_c를 add함 => (14, 4)</span>\nresponse <span class=\"token operator\">=</span> Permute<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>response<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 결론 shape = (4, 14) # Permute((2, 1)): (D2, D1)으로 transpose. permute가 transpose보다 더 축이동이 자유로움 </span></code></pre></div>\n</li>\n</ul>\n<br>\n<br>\n<h5 id=\"code-classlanguage-textanswer-layercode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textanswer-layercode\" aria-label=\"code classlanguage textanswer layercode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">answer layer</code></h5>\n<ul>\n<li>\n<p>episodic memory layer(response) + quetion layer</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">answer <span class=\"token operator\">=</span> Concatenate<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>response<span class=\"token punctuation\">,</span> question_encoder<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nanswer <span class=\"token operator\">=</span> LSTM<span class=\"token punctuation\">(</span>LATENT_SIZE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>answer<span class=\"token punctuation\">)</span> <span class=\"token comment\"># LATENT_SIZE = 64</span>\nanswer <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>answer<span class=\"token punctuation\">)</span>\nanswer <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span>vocab_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>answer<span class=\"token punctuation\">)</span> <span class=\"token comment\"># shape=(None, 22) # 마지막 dense는 vocab_size=22(단어들의 총 개수)로!</span>\noutput <span class=\"token operator\">=</span> Activation<span class=\"token punctuation\">(</span><span class=\"token string\">\"softmax\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>answer<span class=\"token punctuation\">)</span> <span class=\"token comment\"># shape=(None, 22)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<h5 id=\"compile\" style=\"position:relative;\"><a href=\"#compile\" aria-label=\"compile permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>compile</h5>\n<ul>\n<li>\n<p>모델 빌드 마지막</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span>inputs<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>story_input<span class=\"token punctuation\">,</span> question_input<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> outputs<span class=\"token operator\">=</span>output<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 합쳤으니 input을 []로 써주는 것 </span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span><span class=\"token string\">\"adam\"</span><span class=\"token punctuation\">,</span> loss<span class=\"token operator\">=</span><span class=\"token string\">\"categorical_crossentropy\"</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 처음에 to_categorial 안 해줬으면 loss=\"sparse_categorical_crossentropy\" 해야함 </span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<h5 id=\"fit\" style=\"position:relative;\"><a href=\"#fit\" aria-label=\"fit permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>fit</h5>\n<ul>\n<li>\n<p>모델 학습</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Model Training</span>\nhistory <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>Xstrain<span class=\"token punctuation\">,</span> Xqtrain<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>Ytrain<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># Ytrain: answer</span>\n                  batch_size <span class=\"token operator\">=</span> BATCH_SIZE<span class=\"token punctuation\">,</span> \n                  epochs <span class=\"token operator\">=</span> NUM_EPOCHS<span class=\"token punctuation\">,</span>\n                  validation_data<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>Xstest<span class=\"token punctuation\">,</span> Xqtest<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>Ytest<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># ytest??? fit 하면 0,0,0 이던 게 13,14 등지로 바뀜</span></code></pre></div>\n<blockquote>\n<ul>\n<li>Ytest.shape\nOut[78]: (1000, 22)</li>\n<li>Ytest\nOut[79]:\narray([[0., 0., 0., ..., 0., 0., 0.],\n[0., 0., 0., ..., 0., 0., 0.],\n[0., 0., 0., ..., 0., 0., 0.],\n...,\n[0., 0., 0., ..., 0., 0., 0.],\n[0., 0., 0., ..., 0., 0., 0.],\n[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)</li>\n<li>ytest.shape\nOut[87]: (1000,)</li>\n<li>ytest\nOut[92]:\narray([15, 12, 16, 15, 16, 15, 14 ... ])</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<br>\n<h4 id=\"loss-plot\" style=\"position:relative;\"><a href=\"#loss-plot\" aria-label=\"loss plot permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>loss plot</h4>\n<ul>\n<li>\n<p>loss plot</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">plt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">\"Episodic Memory Q &amp; A Loss\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">\"loss\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> color<span class=\"token operator\">=</span><span class=\"token string\">\"g\"</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"train\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">\"val_loss\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> color<span class=\"token operator\">=</span><span class=\"token string\">\"r\"</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"validation\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token string\">\"best\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<h4 id=\"정확도-측정\" style=\"position:relative;\"><a href=\"#%EC%A0%95%ED%99%95%EB%8F%84-%EC%B8%A1%EC%A0%95\" aria-label=\"정확도 측정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정확도 측정</h4>\n<ul>\n<li>\n<p>get predictions of labels</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">ytest <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>Ytest<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nYtest_ <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>Xstest<span class=\"token punctuation\">,</span> Xqtest<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nytest_ <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>Ytest_<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<h4 id=\"적용\" style=\"position:relative;\"><a href=\"#%EC%A0%81%EC%9A%A9\" aria-label=\"적용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>적용</h4>\n<ul>\n<li>\n<p>적용</p>\n<ul>\n<li>Select Random questions and predict answers</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">NUM_DISPLAY <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\n \n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> random<span class=\"token punctuation\">.</span>sample<span class=\"token punctuation\">(</span><span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>Xstest<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>NUM_DISPLAY<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  story <span class=\"token operator\">=</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>indx2word<span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> Xstest<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>tolist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">if</span> x <span class=\"token operator\">!=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n  question <span class=\"token operator\">=</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>indx2word<span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> Xqtest<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>tolist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n  label <span class=\"token operator\">=</span> indx2word<span class=\"token punctuation\">[</span>ytest<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\n  prediction <span class=\"token operator\">=</span> indx2word<span class=\"token punctuation\">[</span>ytest_<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>story<span class=\"token punctuation\">,</span> question<span class=\"token punctuation\">,</span> label<span class=\"token punctuation\">,</span> prediction<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<br>\n<h2 id=\"출력층\" style=\"position:relative;\"><a href=\"#%EC%B6%9C%EB%A0%A5%EC%B8%B5\" aria-label=\"출력층 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>출력층</h2>\n<ul>\n<li>\n<p>출력층이 0 or 1 처럼 하나일 때</p>\n<ul>\n<li>Binary classification. 따라서 sigmoid - binary-crossentropy 사용</li>\n<li>\n<table>\n<thead>\n<tr>\n<th>y</th>\n<th>yHat</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n<blockquote>\n<p>정확도: 2/3<br></p>\n</blockquote>\n</li>\n<li>\n<p>출력층이 두 개 이상 나올 때</p>\n<ul>\n<li>multi-classification. 따라서 softmax - categorical-crossentropy 사용</li>\n<li>one-hot 구조</li>\n<li>\n<table>\n<thead>\n<tr>\n<th>y</th>\n<th>yHat</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0 1 0</td>\n<td>0 1 0</td>\n</tr>\n<tr>\n<td>0 0 1</td>\n<td>0 1 0</td>\n</tr>\n<tr>\n<td>1 0 0</td>\n<td>1 0 0</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n<blockquote>\n<p>정확도: 2/3<br></p>\n</blockquote>\n</li>\n<li>\n<p>출력층에 '1'이 여러 개인 구조. one-hot 구조가 아닐 때</p>\n<ul>\n<li>multi-labeled classification. 따라서 sigmoid - binary-crossentropy 사용</li>\n<li>입력 뉴런 각각에 대해 binary-classification 해야 함</li>\n<li>\n<table>\n<thead>\n<tr>\n<th>y</th>\n<th>yHat</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0 1 0</td>\n<td>0 1 0</td>\n</tr>\n<tr>\n<td>0 0 1</td>\n<td>0 1 0</td>\n</tr>\n<tr>\n<td>1 0 0</td>\n<td>1 0 0</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n<blockquote>\n<p>정확도: 9개 중 7개 맞춤.  따라서 7/9</p>\n<ul>\n<li>위에처럼 row 전체가 다 맞았을 때 맞았다고 보는 게 아니고, row+colum으로 각각 개별로 봄 <br></li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<br>\n<ul>\n<li>\n<p>참고: </p>\n<blockquote>\n<ul>\n<li>아마추어 퀀트, blog.naver.com/chunjein</li>\n<li>크리슈나 바브사 외. 2019.01.31. 자연어 처리 쿡북 with 파이썬 [파이썬으로 NLP를 구현하는 60여 가지 레시피]. 에이콘</li>\n<li><a href=\"https://frhyme.github.io/python-libs/ML_multilabel_classfication/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://frhyme.github.io/python-libs/ML_multilabel_classfication/</a></li>\n</ul>\n</blockquote>\n</li>\n</ul>","excerpt":"NLP 분야에서 딥러닝의 고급 응용 DMN Ask Me Anything attention score layer story layer episodic memory layer answer layer 텍스트 자동 생성 예제 문장: I love you…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_4/#nlp-%EB%B6%84%EC%95%BC%EC%97%90%EC%84%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98-%EA%B3%A0%EA%B8%89-%EC%9D%91%EC%9A%A9\">NLP 분야에서 딥러닝의 고급 응용</a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_4/#%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%9E%90%EB%8F%99-%EC%83%9D%EC%84%B1\">텍스트 자동 생성</a></li>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_4/#code-classlanguage-textdmncode\"><code class=\"language-text\">DMN</code></a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_4/#code-classlanguage-textask-me-anythingcode\"><code class=\"language-text\">Ask Me Anything</code></a></li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_4/#code\">code:</a></li>\n</ul>\n</li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_4/#%EC%B6%9C%EB%A0%A5%EC%B8%B5\">출력층</a></li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/NLP응용_4/"},"frontmatter":{"title":"NLP Ask Me Anything","date":"Aug 05, 2020","tags":["NLP","attention","ask_me_anything"],"keywords":["JyneeEarth","jynee"],"update":"Aug 18, 2020"}}},"pageContext":{"slug":"/NLP응용_4/","series":[{"slug":"/NLP응용_1/","title":"NLP 편집거리/주제식별/자연어분석","num":1},{"slug":"/NLP응용_2/","title":"NLP Embedding","num":2},{"slug":"/NLP응용_3/","title":"NLP Word2Vec/SGNS","num":3},{"slug":"/NLP응용_4/","title":"NLP Ask Me Anything","num":4}],"lastmod":"2020-08-18"}},"staticQueryHashes":["3649515864","694178885"]}