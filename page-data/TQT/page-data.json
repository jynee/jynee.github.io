{"componentChunkName":"component---src-templates-post-tsx","path":"/TQT/","result":{"data":{"markdownRemark":{"html":"<p><strong>TQT(The question I asked the teacher today.)</strong></p>\n<h1 id=\"dense\" style=\"position:relative;\"><a href=\"#dense\" aria-label=\"dense permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Dense</h1>\n<ul>\n<li><code class=\"language-text\">Dense</code>: fully connected</li>\n<li>\n<p><code class=\"language-text\">ANN(FNN)</code>에서는 여러 <code class=\"language-text\">Dense</code>를 써도 되지만, <code class=\"language-text\">RNN(LSTM)</code>에선 마지막 층에서만 <code class=\"language-text\">Dense</code>를 써야함.</p>\n<ul>\n<li><code class=\"language-text\">CNN</code>에서는 여러 <code class=\"language-text\">Dense</code> 써도 됨  ##?#?#??#?#???</li>\n<li>=> 일단... <code class=\"language-text\">lstm</code>에서 <code class=\"language-text\">lstm() → Dense → lstm()</code>은 <code class=\"language-text\">lstm 네트워크가 2개</code> 만들어진다고 보면 됨. <code class=\"language-text\">lstm() → Dense</code> 했을 때, <code class=\"language-text\">1개의 네트워크</code>가 형성된 것</li>\n<li>=> 그리고 <code class=\"language-text\">CNN</code>은 일종의 잘 짜여진 레시피라서 <code class=\"language-text\">con1D → pooling → Dense → con1D → pooling</code>은 위 <code class=\"language-text\">lstm</code>처럼 좀 이상한 네트워크 구조가 되는 거라 생각함...</li>\n</ul>\n</li>\n<li>Dense(1, activation='sigmoid')</li>\n<li>==LSTM에서 FNN으로 보내는 마지막 Dense에선 relu 쓰면 안됨==</li>\n</ul>\n<h1 id=\"fnn순방향-신경망\" style=\"position:relative;\"><a href=\"#fnn%EC%88%9C%EB%B0%A9%ED%96%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D\" aria-label=\"fnn순방향 신경망 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>FNN(순방향 신경망)</h1>\n<ul>\n<li>↔ RNN</li>\n<li>\n<p>hidden 층에서</p>\n<ul>\n<li>Dense(4, <code class=\"language-text\">activation</code> = 'sigmoid', <code class=\"language-text\">kernel_regularizer</code>=regularizers.l2(0.0001), activation='relu')</li>\n<li><code class=\"language-text\">Dropout</code>(rate=0.5)</li>\n</ul>\n</li>\n<li><code class=\"language-text\">BatchNormalization</code>(momentum=0.9, epsilon=0.005, center=True, scale=True, moving<em>variance</em>initializer='ones')</li>\n<li>\n<p><code class=\"language-text\">predict</code>까지 끝낸 <strong>연속형</strong> <code class=\"language-text\">yHat</code> 값을, <code class=\"language-text\">np.where</code> 써줘서 <strong>바이너리 형태</strong>로 변환 </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">np<span class=\"token punctuation\">.</span>where<span class=\"token punctuation\">(</span>yHat <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 딥러닝_파일: 4-4.ANN(Credit_Keras)_직접 해보기_커스텀loss.py</span></code></pre></div>\n</li>\n<li>\n<p><code class=\"language-text\">history</code> 활용</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">hist<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span>\nhist<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># 딥러닝_파일: 4-4.ANN(Credit_Keras)_직접 해보기.py</span></code></pre></div>\n</li>\n<li>\n<p>학습/평가/예측용 model로 나누었을 때 <strong>평가 데이터 활용</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>trainX<span class=\"token punctuation\">,</span> trainY<span class=\"token punctuation\">,</span> validation_data<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>evlX<span class=\"token punctuation\">,</span> evlY<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">200</span><span class=\"token punctuation\">,</span> batch_size<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<h1 id=\"lstm\" style=\"position:relative;\"><a href=\"#lstm\" aria-label=\"lstm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LSTM</h1>\n<ul>\n<li>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>설명</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2층</td>\n<td><code class=\"language-text\">lstm()</code>을 2번 써준다</td>\n</tr>\n<tr>\n<td>양방향</td>\n<td><code class=\"language-text\">bidirectional</code> + <code class=\"language-text\">merge_mode = ‘concat’</code> <br />FNN, BFN 값을 merge_mode 형태로 합쳐서 list형으로 되돌려줌</td>\n</tr>\n<tr>\n<td>many-to-many</td>\n<td><code class=\"language-text\">return-sequences = True</code><br />LSTM의 중간 스텝의 출력을 모두 사용</td>\n</tr>\n<tr>\n<td></td>\n<td><code class=\"language-text\">timedistributed</code><br /> FFN으로 가기 전 LSTM 마지막 층에서 각 뉴런의 각 지점에서 계산한 오류를 다음 층으로 전파</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n<h1 id=\"cnn\" style=\"position:relative;\"><a href=\"#cnn\" aria-label=\"cnn permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>CNN</h1>\n<ul>\n<li>이미지를 대표할 수 있는 특성들을 도출해서 FNN에 넣어줌</li>\n<li>\n<table>\n<thead>\n<tr>\n<th>code</th>\n<th>설명</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code class=\"language-text\">Input</code>(batch_shape = (None, nStep, nFeature, nChannel))</td>\n<td></td>\n</tr>\n<tr>\n<td><code class=\"language-text\">Conv2D</code>(filters=30, kernel_size=(8,3), strides=1, padding = 'same', activation='relu')</td>\n<td></td>\n</tr>\n<tr>\n<td><code class=\"language-text\">MaxPooling2D</code>(pool_size=(2,1), strides=1, padding='valid')</td>\n<td>- 경우에 따라 conv2D, pooling 더 써줄 수 있음<br />- <code class=\"language-text\">GlobalMaxPooling1D()</code>도 있음</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">Flatten()</code></td>\n<td>2D는 4차원이라 shape 맞추려고 보통 flatten을 써줌<br />1d는 안 써도 되는 듯(?)</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">Dense</code>(nOutput, activation='linear')</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n<h1 id=\"activation\" style=\"position:relative;\"><a href=\"#activation\" aria-label=\"activation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>activation</h1>\n<ul>\n<li>\n<table>\n<thead>\n<tr>\n<th>activation(비선형 함수)</th>\n<th>loss</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code class=\"language-text\">softmax</code></td>\n<td><code class=\"language-text\">sparse_categorical_crossentropy</code></td>\n</tr>\n<tr>\n<td><code class=\"language-text\">sigmoid</code></td>\n<td><code class=\"language-text\">binary_crossentropy</code></td>\n</tr>\n<tr>\n<td><code class=\"language-text\">linear</code></td>\n<td><code class=\"language-text\">mse</code></td>\n</tr>\n<tr>\n<td><code class=\"language-text\">relu</code></td>\n<td>← Hidden layer에 씀. 기울기가 0이기 때문에 뉴런이 죽을 수 있는 단점 有</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Leakly ReLU</td>\n<td>뉴런이 죽을 수 있는 현상 해결</td>\n</tr>\n<tr>\n<td>PReLU</td>\n<td>x&#x3C;0 에서 학습 가능</td>\n</tr>\n<tr>\n<td>granger causality</td>\n<td>통제된 상황에서 인과관계가 가능하다고 말할 수 있음. 시계열 데이터에서 쓰일 수 있음</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>\n<p>딥러닝 네트워크(DN)의 노드는 입력값을 전부 더한 후, 활성화 함수(Activation function)를 통과시켜 다음 노드에 전달한다.</p>\n<ul>\n<li>이때 사용하는 활성화 함수는 비선형 함수를 쓴다. </li>\n</ul>\n</li>\n</ul>\n<h2 id=\"relu\" style=\"position:relative;\"><a href=\"#relu\" aria-label=\"relu permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ReLu</h2>\n<ul>\n<li>히든층에 자주 쓰임</li>\n<li>\n<p>그냥 CNN이든 LSTM이든 출력층 Dense에 Relu 쓰지 말자</p>\n<ul>\n<li>LSTM에선 Relu 안 쓰는 게 좋음. 특히 출력층엔 쓰면 안 됨.</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"학습compile-예측predict\" style=\"position:relative;\"><a href=\"#%ED%95%99%EC%8A%B5compile-%EC%98%88%EC%B8%A1predict\" aria-label=\"학습compile 예측predict permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>학습(compile), 예측(predict)</h1>\n<h2 id=\"optimizer\" style=\"position:relative;\"><a href=\"#optimizer\" aria-label=\"optimizer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>optimizer</h2>\n<ul>\n<li>\n<table>\n<thead>\n<tr>\n<th>종류(빈도순)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code class=\"language-text\">adam</code></td>\n</tr>\n<tr>\n<td>Adadelta, RMSprop, Adagrad</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">momentum</code></td>\n</tr>\n<tr>\n<td>GD, NAG</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>최적화가 잘 안 되면 글로벌 minmun을 찾지 못하고 로컬 minimum에 빠진다. 이때 로컬 minimum을 <strong>어떻게 빨리</strong> 탈출할 수 있을지 U턴 메소드를 쓸지, 다른 1차 미분방법(GD)를 쓸 지 결정하게 된다. </li>\n</ul>\n<h2 id=\"epoch\" style=\"position:relative;\"><a href=\"#epoch\" aria-label=\"epoch permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>epoch</h2>\n<ul>\n<li><code class=\"language-text\">epoch</code> 수치가 커지면 <code class=\"language-text\">optimizer</code>가 일을 해서 local이 아닌 global을 찾아간다.</li>\n<li>그런데 너무 크면 overfitting</li>\n<li>따라서 적당한 <code class=\"language-text\">epoch</code> 설정이 필요 </li>\n</ul>\n<h2 id=\"batch_size\" style=\"position:relative;\"><a href=\"#batch_size\" aria-label=\"batch_size permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Batch_size</h2>\n<ul>\n<li>\n<p>data가 크면 <code class=\"language-text\">batch_size</code>도 크게</p>\n<ul>\n<li>25,000개의 raw data라면 <code class=\"language-text\">batch_size</code> = 20 보다 300 이 정도로 설정</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"nlp--dl\" style=\"position:relative;\"><a href=\"#nlp--dl\" aria-label=\"nlp  dl permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NLP &#x26; DL</h1>\n<ul>\n<li>SGNS</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>용어</th>\n<th>설명</th>\n<th>CODE</th>\n<th>참고</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>pre-trained</td>\n<td>SKNS에서 학습한 We를 적용</td>\n<td>model.layers[1]<strong>.set_weights</strong>(We)</td>\n<td>해당 code 적용 후 model fit 진행</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>fine-training</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>\n<p>SGNS에 모델 학습(fit) 시, 학습을 따로 시키는 이유?</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 학습</span>\nhist <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>X<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> X<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> X<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> \n               batch_size<span class=\"token operator\">=</span>BATCH_SIZE<span class=\"token punctuation\">,</span>\n               epochs<span class=\"token operator\">=</span>NUM_EPOCHS<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<ul>\n<li>각기 연결된 가중치 선이 구분되어 있기 때문에</li>\n</ul>\n</blockquote>\n</li>\n<li>\n<p>SGNS 모델 만들 때 dot을 한다면, </p>\n<ol>\n<li><strong>axis=2</strong>    <em>@2</em></li>\n</ol>\n<p> → 후에</p>\n<ol start=\"2\">\n<li>reshape<strong>(())</strong>    <em>@괄호 두 개</em></li>\n</ol>\n</li>\n<li>\n<p>SGNS로 만든 Embedding의 w(가중치)를 basic한 word data에 적용할 때, load_weights 사용하는 방법도 있다.</p>\n<ul>\n<li>근데 이땐 shape을 맞춰줘야 한다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">w <span class=\"token operator\">=</span> encoder<span class=\"token punctuation\">.</span>load_weights<span class=\"token punctuation\">(</span><span class=\"token string\">'model_w.h5'</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 가중치(w) 불러온 후,</span>\nemb <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>max_features<span class=\"token punctuation\">,</span> embedding_dims<span class=\"token punctuation\">,</span> load_weights <span class=\"token operator\">=</span> w<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>xInput<span class=\"token punctuation\">)</span> <span class=\"token comment\"># embedding layer에 바로 적용</span></code></pre></div>\n<ul>\n<li>보통 이런 느낌으로 씀</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">weights <span class=\"token operator\">=</span> load_weights<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nembedding_layer <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>V<span class=\"token punctuation\">,</span>\n                            output_dim<span class=\"token operator\">=</span>embedding_dim<span class=\"token punctuation\">,</span>\n                            input_length<span class=\"token operator\">=</span>input_length<span class=\"token punctuation\">,</span>\n                            trainable<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n                            weights<span class=\"token operator\">=</span>weights<span class=\"token punctuation\">,</span>\n                            name<span class=\"token operator\">=</span><span class=\"token string\">'embedding'</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<h1 id=\"기타\" style=\"position:relative;\"><a href=\"#%EA%B8%B0%ED%83%80\" aria-label=\"기타 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>기타</h1>\n<h2 id=\"유클리디안-거리\" style=\"position:relative;\"><a href=\"#%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%94%94%EC%95%88-%EA%B1%B0%EB%A6%AC\" aria-label=\"유클리디안 거리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>유클리디안 거리</h2>\n<ul>\n<li>\n<p>거리 계산할 때, 비교하고 싶은 건 <code class=\"language-text\">[]</code>를 쳐서 넣어주기  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">euclidean_distances<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>father<span class=\"token punctuation\">,</span> mother<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<h2 id=\"가중치-저장save\" style=\"position:relative;\"><a href=\"#%EA%B0%80%EC%A4%91%EC%B9%98-%EC%A0%80%EC%9E%A5save\" aria-label=\"가중치 저장save permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>가중치 저장(Save)</h2>\n<ul>\n<li>\n<p>Embedding (left side) layer의 W를 저장할 때, [2]를 저장한단 사실 알아두기</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'data/embedding_W.pickle'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'wb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n  pickle<span class=\"token punctuation\">.</span>dump<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>get_weights<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> f<span class=\"token punctuation\">,</span> pickle<span class=\"token punctuation\">.</span>HIGHEST_PROTOCOL<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>","excerpt":"TQT(The question I asked the teacher today.) Dense : fully connected…","tableOfContents":"<ul>\n<li><a href=\"/TQT/#dense\">Dense</a></li>\n<li><a href=\"/TQT/#fnn%EC%88%9C%EB%B0%A9%ED%96%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D\">FNN(순방향 신경망)</a></li>\n<li><a href=\"/TQT/#lstm\">LSTM</a></li>\n<li><a href=\"/TQT/#cnn\">CNN</a></li>\n<li>\n<p><a href=\"/TQT/#activation\">activation</a></p>\n<ul>\n<li><a href=\"/TQT/#relu\">ReLu</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/TQT/#%ED%95%99%EC%8A%B5compile-%EC%98%88%EC%B8%A1predict\">학습(compile), 예측(predict)</a></p>\n<ul>\n<li><a href=\"/TQT/#optimizer\">optimizer</a></li>\n<li><a href=\"/TQT/#epoch\">epoch</a></li>\n<li><a href=\"/TQT/#batch_size\">Batch_size</a></li>\n</ul>\n</li>\n<li><a href=\"/TQT/#nlp--dl\">NLP &#x26; DL</a></li>\n<li>\n<p><a href=\"/TQT/#%EA%B8%B0%ED%83%80\">기타</a></p>\n<ul>\n<li><a href=\"/TQT/#%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%94%94%EC%95%88-%EA%B1%B0%EB%A6%AC\">유클리디안 거리</a></li>\n<li><a href=\"/TQT/#%EA%B0%80%EC%A4%91%EC%B9%98-%EC%A0%80%EC%9E%A5save\">가중치 저장(Save)</a></li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/TQT/"},"frontmatter":{"title":"TQT","date":"Aug 01, 2020","tags":["TQT"],"keywords":["jynee","jynee"],"update":"Jan 01, 0001"}}},"pageContext":{"slug":"/TQT/","series":[],"lastmod":"2020-08-01"}},"staticQueryHashes":["3649515864","694178885"]}