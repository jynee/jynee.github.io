{"componentChunkName":"component---src-templates-post-tsx","path":"/NLP한글_3/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"auc\" style=\"position:relative;\"><a href=\"#auc\" aria-label=\"auc permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AUC</h1>\n<ul>\n<li>Area under the roc curve (AUC)</li>\n<li>Confusion matix(Binaray classification)</li>\n<li>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>prediction<br />P</th>\n<th><br />N</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>actual P</td>\n<td>TP</td>\n<td>FP</td>\n</tr>\n<tr>\n<td>N</td>\n<td>FN</td>\n<td>TN</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>TPR </li>\n<li>FPR </li>\n<li>Thes(임계치)가 작을수록 TPR이 높아진다.</li>\n<li>\n<p>ROC 면적이 클수록 좋다.</p>\n<ul>\n<li>Area Under the ROC(AUC) : AUC 값을 0~1 사이의 값으로 표현.</li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<h2 id=\"kaggle-competition\" style=\"position:relative;\"><a href=\"#kaggle-competition\" aria-label=\"kaggle competition permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Kaggle Competition</h2>\n<ul>\n<li>\n<p>논문 분석: </p>\n<ul>\n<li>Alejandro Peláez외. Sring 2015. Sentiment analysis of IMDb movie reviews. Rutgers University </li>\n</ul>\n</li>\n</ul>\n<br>\n<h3 id=\"code-classlanguage-textnegation-handlingcode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textnegation-handlingcode\" aria-label=\"code classlanguage textnegation handlingcode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Negation Handling</code></h3>\n<ul>\n<li>\n<p><em>새로운 접근</em> > ''<strong>Negation Handling'</strong>'</p>\n<ul>\n<li>hardly good → <strong>neg</strong>.good</li>\n</ul>\n</li>\n</ul>\n<br>\n<h3 id=\"code-classlanguage-textmutual-information상호-정보량code\" style=\"position:relative;\"><a href=\"#code-classlanguage-textmutual-information%EC%83%81%ED%98%B8-%EC%A0%95%EB%B3%B4%EB%9F%89code\" aria-label=\"code classlanguage textmutual information상호 정보량code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong><code class=\"language-text\">Mutual information(상호 정보량)</code></strong></h3>\n<ul>\n<li>\n<p>(쿡북에서) 사전 생성할 때, 전체 단어 中 6,000개만 썼다.</p>\n<ul>\n<li>6,000개: most common() 빈도가 가장 높은 6,000개의 단어를 선택하여 vocab을 만들고, 이걸 가지고 문서 전처리를 했었음.</li>\n<li>이때 핵심: <strong>빈도가 가장 높은</strong></li>\n<li>\n<p>단어 빈도수를 수동 카운트</p>\n<ul>\n<li>collection.counter()</li>\n<li>counter.most_common()</li>\n</ul>\n</li>\n<li>하지만 이 분은 Mutual information(상호정보량)이라는 개념을 사용했다.<br></li>\n</ul>\n</li>\n<li>\n<p><em>새로운 접근</em> > <strong>Mutual information</strong></p>\n<ul>\n<li>class인 Y와 일정한 관계가 높은 순서를 vocab 생성</li>\n<li>순서:</li>\n<li>\n<p>review 전처리</p>\n<table>\n<thead>\n<tr>\n<th>review</th>\n<th>y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>I like you</td>\n<td>1</td>\n</tr>\n<tr>\n<td>I dislike you</td>\n<td>0</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n<br>\n</li>\n<li>\n<p>모든 단어의 Mutual information 계산</p>\n<table>\n<thead>\n<tr>\n<th>단어(Unigram)</th>\n<th>y</th>\n<th>MI(공식에 의해 연산)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>I</td>\n<td>1</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>I</td>\n<td>0</td>\n<td></td>\n</tr>\n<tr>\n<td>like</td>\n<td>1</td>\n<td>0.8</td>\n</tr>\n<tr>\n<td>dislike</td>\n<td>0</td>\n<td>0.7</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<ul>\n<li>\n<p>\"I\"와 y는 무관 → x, y는 독립</p>\n<ul>\n<li>P(x, y) = P(x|y) * P(y)</li>\n</ul>\n<p>'like, dislike'와 y는 연관 → x, y는 종속 </p>\n<ul>\n<li>P(x, y) = P(x) * P(y)</li>\n</ul>\n</li>\n<li>\n<p>ML에서 Entropy 공부 할 때, KL값으로 Cross Entropy 값을 구했었다.</p>\n<ul>\n<li>KL : 두 분포의 정보량의 차이(두 분포의 유사성)</li>\n</ul>\n</li>\n<li>\n<p>Mutual Information의 KL</p>\n<ul>\n<li>KL(P(x, y) || P(x) * P(y))</li>\n<li>예: MI('like') = ?</li>\n<li>P(x, y) = P(x|y) * P(y) = > </li>\n<li>P(x|y = 0) * P(y = 0)  + P(x|y = 1) * P(y = 1)</li>\n<li>\n<p>P(like|y = 0) * P(y = 0)</p>\n<p>P(like|y = 0) : label(y)가 0(neg)인 리뷰 中 'like' 단어가 등장한 비율</p>\n<p>P(y = 0) : 리뷰 25,000개 中 y=0(neg)인 비율</p>\n</li>\n<li>따라서 MI('like') = 0.8</li>\n</ul>\n</li>\n<li>'I'의 경우 긍/부정이 섞여 있어 MI 수치가 적다. </li>\n<li>'MI 수치가 적다'는 말은, 긍/부정이 명확하지 않다는 뜻이고, 이는 리뷰의 긍/부정을 분류하기 애매하다는 뜻이므로 실질적으로 목적을 이루는 데엔 도움이 되진 않는단 뜻이다.</li>\n</ul>\n</blockquote>\n<br>\n</li>\n<li>MI 높은 순서로 Sort</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>단어</th>\n<th>MI</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>like</td>\n<td>0.8</td>\n</tr>\n<tr>\n<td>dislike</td>\n<td>0.7</td>\n</tr>\n</tbody>\n</table>\n  <br>\n<ol start=\"3\">\n<li>\n<p>상위 N% 선택하여 vocab 생성</p>\n<ul>\n<li>상위만 선택한 이유: 분석에 별로 도움되지 않는 단어도 vocab에 포함해 생성해봤자 연산량만 늘어나고 분석에 실질적인 도움은 되지 않으니까 후순위로 밀어낸다.<br></li>\n</ul>\n</li>\n<li>\n<p>TF - IDF</p>\n<ul>\n<li>예: movie → 여러 리뷰에 등장 DF 가 높다 IDF가 낮다<br></li>\n</ul>\n</li>\n<li>\n<p>감정분석</p>\n<ul>\n<li>학습 기반(신경망 속 Embedding)이 아닌 사전 기반의 감정분석(VADER 알고리즘) 사용</li>\n<li>VADER 알고리즘</li>\n<li>리뷰 속 단어들을 사전에 등재된 단어에 따라 VADER Score를 매기고, 그 하나의 리뷰의 VADER Score 값을 평균낸 것<br></li>\n<li>각 단어마다 score를 매겨 양수/음수에 따라 positive and negative를 구분한다</li>\n</ul>\n</li>\n<li>\n<p>score 식 > label = 1인 리뷰 中 'like'의 개수 - label = 0인 리뷰 中 'like'의 개수</p>\n<ul>\n<li>평균, top k, bottom 값을 matrix로 해서 구해봄<br></li>\n</ul>\n</li>\n</ol>\n</li>\n<li>\n<p>word2vec</p>\n<ul>\n<li>\n<p>단어 vector들을 평균 내서 이걸 문장 vector로 쓰는 것</p>\n<ul>\n<li>문제: 정보가 손실되어 별로 좋은 방법은 아니다.<br></li>\n</ul>\n</li>\n<li>\n<p>doc2vec</p>\n<ul>\n<li>단어를 50~60% 선택할 때 score가 가장 좋았다.</li>\n</ul>\n</li>\n<li>\n<p>하지만 대체적으로 많은 단어를 선택할 때 score가 좋아지긴 한다.</p>\n<ul>\n<li>doc2vec + TF-IDF : 의미적관계+구조적 관계를 보기 위해 CONCAT or average 등을 사용하여 두 값을 합침<br></li>\n</ul>\n</li>\n<li>\n<p>감정분석</p>\n<ul>\n<li>학습 기반<br></li>\n</ul>\n</li>\n<li>결과 ROC : 0.99259<br></li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<h1 id=\"사전-만들기\" style=\"position:relative;\"><a href=\"#%EC%82%AC%EC%A0%84-%EB%A7%8C%EB%93%A4%EA%B8%B0\" aria-label=\"사전 만들기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>사전 만들기</h1>\n<ul>\n<li>\n<p>패키지 다운 경로: C:\\Users\\콘다경로\\Lib\\site-packages\\konlpy\\java</p>\n<ul>\n<li>그 中 사전 파일: open-korean-text-2.1.0.jar</li>\n<li>open-korean-text-2.1.0.jar ← 여기에 단어 등록하기<br></li>\n</ul>\n</li>\n<li>\n<p>단어 등록 방법</p>\n<ul>\n<li>작업 폴더 생성</li>\n<li>폴더 이름: aaa ←  라고 생성함 <br></li>\n<li>cmd 창에다가</li>\n<li>cd C:\\Users\\콘다경로\\Lib\\site-packages\\konlpy\\java\\aaa 입력</li>\n<li>\n<p>jar xvf ../open-korean-text-2.1.0.jar 입력</p>\n<blockquote>\n<ul>\n<li>jar xvf  :묶음파일 풀기\n../ : 이전 폴더에 있는\nopen-korean-text-2.1.0.jar : 이 파일을 풀어라</li>\n<li>jar cvf : 묶기</li>\n<li>* : 와일드카드</li>\n</ul>\n</blockquote>\n<br>\n</li>\n<li>사전들이 있는 폴더: C:\\Users\\콘다경로\\Lib\\site-packages\\konlpy\\java\\aaa\\org\\openkoreantext\\processor\\util</li>\n</ul>\n<br>\n<ul>\n<li>그 中 명사 사전이 들은 파일: C:\\Users\\콘다경로\\Lib\\site-packages\\konlpy\\java\\aaa\\org\\openkoreantext\\processor\\util\\noun</li>\n<li>이름 정의된 파일: 'names.txt' or 'company_names.txt'</li>\n<li>\n<p>여기에 단어 하나 추가해보기</p>\n<ul>\n<li>'이자용' 이라고 추가하였음<br></li>\n</ul>\n</li>\n<li>cmd 창에 jar cvf open-korean-text-2.1.0.jar * 입력<br></li>\n<li>다시 'aaa' 폴더로 가기: C:\\Users\\콘다경로\\Lib\\site-packages\\konlpy\\java\\aaa<br></li>\n<li>'aaa' 폴더에 생긴 open-korean-text-2.1.0.jar ← ctrl+c해서, 상위 폴더인 'java'에 붙여넣기(ctrl+v)<br></li>\n<li>적용 완료<br></li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<br>\n<br>\n<ul>\n<li>\n<p>참고: </p>\n<blockquote>\n<ul>\n<li>아마추어 퀀트, blog.naver.com/chunjein</li>\n</ul>\n</blockquote>\n</li>\n</ul>","excerpt":"AUC Area under the roc curve (AUC) Confusion matix(Binaray classification)  predictionP N actual P TP FP N FN TN TPR  FPR  Thes…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/NLP%ED%95%9C%EA%B8%80_3/#auc\">AUC</a></p>\n<ul>\n<li>\n<p><a href=\"/NLP%ED%95%9C%EA%B8%80_3/#kaggle-competition\">Kaggle Competition</a></p>\n<ul>\n<li><a href=\"/NLP%ED%95%9C%EA%B8%80_3/#code-classlanguage-textnegation-handlingcode\"><code class=\"language-text\">Negation Handling</code></a></li>\n<li><a href=\"/NLP%ED%95%9C%EA%B8%80_3/#code-classlanguage-textmutual-information%EC%83%81%ED%98%B8-%EC%A0%95%EB%B3%B4%EB%9F%89code\"><strong><code class=\"language-text\">Mutual information(상호 정보량)</code></strong></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/NLP%ED%95%9C%EA%B8%80_3/#%EC%82%AC%EC%A0%84-%EB%A7%8C%EB%93%A4%EA%B8%B0\">사전 만들기</a></li>\n</ul>","fields":{"slug":"/NLP한글_3/"},"frontmatter":{"title":"NLP Kaggle competition 우승자가 제안한 새로운 접근방법을 배워보자","date":"Aug 06, 2020","tags":["NLP","논문 분석"],"keywords":["JyneeEarth","jynee"],"update":"Aug 19, 2020"}}},"pageContext":{"slug":"/NLP한글_3/","series":[{"slug":"/NLP한글_1/","title":"NLP 카운트 기반 방법의 텍스트 유사도 측정","num":1},{"slug":"/NLP한글_2/","title":"NLP Doc2Vec","num":2},{"slug":"/NLP한글_3/","title":"NLP Kaggle competition 우승자가 제안한 새로운 접근방법을 배워보자","num":3},{"slug":"/NLP한글_4/","title":"NLP Doc2Vec","num":4}],"lastmod":"2020-08-19"}},"staticQueryHashes":["3649515864","694178885"]}