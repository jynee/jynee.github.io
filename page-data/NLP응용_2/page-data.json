{"componentChunkName":"component---src-templates-post-tsx","path":"/NLP응용_2/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"nlp--딥러닝\" style=\"position:relative;\"><a href=\"#nlp--%EB%94%A5%EB%9F%AC%EB%8B%9D\" aria-label=\"nlp  딥러닝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NLP &#x26; 딥러닝</h1>\n<p>핵심 문제: \"단어를 어떻게 수치화할 것인가?\"</p>\n<br>\n<br>\n<h2 id=\"email---classification\" style=\"position:relative;\"><a href=\"#email---classification\" aria-label=\"email   classification permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Email - Classification</h2>\n<ul>\n<li>딥러닝을 이용하여 20개의 카테고리로 분류된 이메일 데이터를 학습하고, 시험 이메일을 20개 카테고리 중 하나로 분류한다. </li>\n<li>Email별 카테고리(20개. 총 2,000개) → (1:1 대응: label 매겨 학습 시킴) email 데이터 2,000개 → text 추출 → pre-processing → 수치화 → TF-IDF(10,000개의 Feature) → 신경망에 넣기 (10,000개 뉴런 → 1,000개 히든 뉴런 → 20개 출력 뉴런) → 20개 category (softmax)</li>\n<li>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8acd0cb68e2a8724089d31dfe3bd51b2/c1bea/image-20200722170909820.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAABZklEQVQoz12RTW+DMAyG+f8/pNqOu0/bZZUmVdqktdp22mlQQQsECAnkg3e2ER2tJcuJEr9+bCcgm6bpytlijBcPIUgcxxFlWaKqKrRte/V/seRWcC12+1lrjbquoZSS82LWWnRdJ4WTOek/ke8str6zD8MgZEVRiJ/PZxHiNyY3xsB7PxM6OihVoywyqtxfyJbIlU+nE9I0xfF4RJZlyNJMhJ1z4n3fi3DCB8ZX388w2aewTlMUZ/IYA5qmEbo8zyUupEzJb5zPLTNxEpnCB0ymp0HPc1ibPPt5KYHEffCXJS0LW5zviaEW877Gg/vCvX6DJfzgLezwSG3cYbAvCJOH11T0MCHsSXTvgQMwliMGN9D/sNoyEfyOFTb1Dq/6hypFmokh/CdqZwNrtvDRw7Uk8kEb3VmorUJ8j7CFhTZaZngR5M0EctN0cL0VdOdIwHGrkLgMfnQjWt2irEoRWtqcc5xs+Q9Os7r1xAm/1wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200722170909820\"\n        title=\"image-20200722170909820\"\n        src=\"/static/8acd0cb68e2a8724089d31dfe3bd51b2/fcda8/image-20200722170909820.png\"\n        srcset=\"/static/8acd0cb68e2a8724089d31dfe3bd51b2/12f09/image-20200722170909820.png 148w,\n/static/8acd0cb68e2a8724089d31dfe3bd51b2/e4a3f/image-20200722170909820.png 295w,\n/static/8acd0cb68e2a8724089d31dfe3bd51b2/fcda8/image-20200722170909820.png 590w,\n/static/8acd0cb68e2a8724089d31dfe3bd51b2/efc66/image-20200722170909820.png 885w,\n/static/8acd0cb68e2a8724089d31dfe3bd51b2/c83ae/image-20200722170909820.png 1180w,\n/static/8acd0cb68e2a8724089d31dfe3bd51b2/c1bea/image-20200722170909820.png 1388w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li><code class=\"language-text\">pre-processing</code>: 지도학습. pos_tag, token, stemer(어근) 등의 방법으로 전처리. 이게 오래 걸림</li>\n</ul>\n</li>\n</ul>\n</br>\n</br>\n<hr>\n</br>\n</br>\n<h2 id=\"code-classlanguage-textembedding-layercode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textembedding-layercode\" aria-label=\"code classlanguage textembedding layercode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Embedding layer</code></h2>\n<ul>\n<li>수치화 방법</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>빈도기반</th>\n<th>학습 기반</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ex:</td>\n<td><code class=\"language-text\">TF-IDF</code>, <code class=\"language-text\">BOW</code>, <code class=\"language-text\">Doc2Bow</code>, <code class=\"language-text\">Co-occurtance</code>, <code class=\"language-text\">Matrix</code><br />SVD(특이값 분해), 잠재의미분석(LSA), HAL, PCA</td>\n<td><code class=\"language-text\">word2vec</code> , <code class=\"language-text\">WordEmbedding</code>, <br />NNLM, RNNLM</td>\n</tr>\n<tr>\n<td>특징</td>\n<td>통계 기반</td>\n<td>학습 기반<br />딥러닝을 통해 수치화 시켜서 <strong>단어에 의미를 부여</strong>할 수 있는 개념</td>\n</tr>\n<tr>\n<td></td>\n<td>'의미'란 개념이 없다</td>\n<td>'의미'란 개념이 있다<br />단어에 의미를 부여할 수 있음</td>\n</tr>\n<tr>\n<td></td>\n<td>기본적으로 동시 등장 횟수를 하나의 행렬로 나타낸 뒤 그 행렬을 수치화해서 단어 벡터로 만드는 방법</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>단어의 의미: 맥락을 분석할 수 있음</p>\n</blockquote>\n</br>\n<ul>\n<li>\n<p>'단어' 기반의 NLP</p>\n<ul>\n<li>현재의 NLP의 기본 원리</li>\n<li>EX: <code class=\"language-text\">Token</code>, <code class=\"language-text\">pos-tag</code>, <code class=\"language-text\">stemmer</code></br></li>\n</ul>\n</li>\n<li>\n<p>단어 자체로는 의미를 갖지 못하고, 단어가 조합을 이루어야 의미를 형성할 수 있음 </p>\n<ul>\n<li><code class=\"language-text\">character-based NLP</code>도 연구 중이긴 함 </li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8acd0cb68e2a8724089d31dfe3bd51b2/c1bea/image-20200722170909820.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAABZklEQVQoz12RTW+DMAyG+f8/pNqOu0/bZZUmVdqktdp22mlQQQsECAnkg3e2ER2tJcuJEr9+bCcgm6bpytlijBcPIUgcxxFlWaKqKrRte/V/seRWcC12+1lrjbquoZSS82LWWnRdJ4WTOek/ke8str6zD8MgZEVRiJ/PZxHiNyY3xsB7PxM6OihVoywyqtxfyJbIlU+nE9I0xfF4RJZlyNJMhJ1z4n3fi3DCB8ZX388w2aewTlMUZ/IYA5qmEbo8zyUupEzJb5zPLTNxEpnCB0ymp0HPc1ibPPt5KYHEffCXJS0LW5zviaEW877Gg/vCvX6DJfzgLezwSG3cYbAvCJOH11T0MCHsSXTvgQMwliMGN9D/sNoyEfyOFTb1Dq/6hypFmokh/CdqZwNrtvDRw7Uk8kEb3VmorUJ8j7CFhTZaZngR5M0EctN0cL0VdOdIwHGrkLgMfnQjWt2irEoRWtqcc5xs+Q9Os7r1xAm/1wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200722170909820\"\n        title=\"image-20200722170909820\"\n        src=\"/static/8acd0cb68e2a8724089d31dfe3bd51b2/fcda8/image-20200722170909820.png\"\n        srcset=\"/static/8acd0cb68e2a8724089d31dfe3bd51b2/12f09/image-20200722170909820.png 148w,\n/static/8acd0cb68e2a8724089d31dfe3bd51b2/e4a3f/image-20200722170909820.png 295w,\n/static/8acd0cb68e2a8724089d31dfe3bd51b2/fcda8/image-20200722170909820.png 590w,\n/static/8acd0cb68e2a8724089d31dfe3bd51b2/efc66/image-20200722170909820.png 885w,\n/static/8acd0cb68e2a8724089d31dfe3bd51b2/c83ae/image-20200722170909820.png 1180w,\n/static/8acd0cb68e2a8724089d31dfe3bd51b2/c1bea/image-20200722170909820.png 1388w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>\n<p><code class=\"language-text\">Embedding</code> 방법도 위 그림과 같은데, 다만 단어에 의미를 부여하지 못하는 count based인 <code class=\"language-text\">TF-IDF</code> → <code class=\"language-text\">Embedding</code>으로 바꿈 </p>\n<ul>\n<li><code class=\"language-text\">Embedding</code> 위치: 입력층과 은닉층 사이에 끼어든다</li>\n</ul>\n</li>\n</ul>\n</br>\n<h3 id=\"code-classlanguage-textembeddingcode-방법\" style=\"position:relative;\"><a href=\"#code-classlanguage-textembeddingcode-%EB%B0%A9%EB%B2%95\" aria-label=\"code classlanguage textembeddingcode 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Embedding</code> 방법</h3>\n<ol>\n<li>\n<p>문서의 단어 <code class=\"language-text\">Vocabulary</code> 딕셔너리 형성</p>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 514px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d9e794920263d3d04e59cc695a32d2a2/dea13/image-20200723102118222.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 43.24324324324324%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAABD0lEQVQoz41SWaqEQAzs+x9OP0RFcVfEBRfc9zwq0OIDZ5iC2DEdkqp0xHVd9MmApy8xDAOFYUhJklBRFBQEAcVxzL6gL3gWqqqKoihiv+97MgyDHMehtm1JURQyTZMsyyJR1zV33PedE2HjOPI/cBwHFwYDXddp2zbOBxucsgHiYC3wQfd5ninPc6aP2DRNN8tlWbhJ13V3nqZpXAgoy5LvoUCgQJZl1DQNz8R1XfJ9n32w8jyPWT4BmWma/mOIpsxwXVeCIQADMySiI9jAlwXP8+QTOZihbdt3A8S4IP2I52ujMSTjBKAOBVnyt7V5WyEpUVVVVgHgYaEG4xNva/K2e897+QAohG2AZMQw1z+5ObZa3t0yQQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200723102118222\"\n        title=\"image-20200723102118222\"\n        src=\"/static/d9e794920263d3d04e59cc695a32d2a2/dea13/image-20200723102118222.png\"\n        srcset=\"/static/d9e794920263d3d04e59cc695a32d2a2/12f09/image-20200723102118222.png 148w,\n/static/d9e794920263d3d04e59cc695a32d2a2/e4a3f/image-20200723102118222.png 295w,\n/static/d9e794920263d3d04e59cc695a32d2a2/dea13/image-20200723102118222.png 514w\"\n        sizes=\"(max-width: 514px) 100vw, 514px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span> </p>\n<p>이 과정을 거치면</p>\n<p>문서-1 : [1,2,3,0]</p>\n<p>문서-2 : [4,2,3,5]로 변환됨</p>\n<ul>\n<li>문서-1의 [1,2,3,0] '0'은 문서 길이를 맞추기 위한 <code class=\"language-text\">padding</code></li>\n<li><code class=\"language-text\">padding</code>: 비교하려는 문서마다 문장의 길이가 다르므로 <code class=\"language-text\">.pad_sequences</code> 사용하면,<code class=\"language-text\">vocabulary</code> 자체에 <code class=\"language-text\">padding</code>이 삽입됨</li>\n</ul>\n</blockquote>\n<br>\n</li>\n<li>\n<p>문서별로 <code class=\"language-text\">one-hot encoding</code></p>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 538px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6839bef66a3f95ef60a3c60796a28c98/9516f/image-20200723102329504.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 51.35135135135135%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAABIUlEQVQoz42S54qFQAyFff9nE0V/KPaOBVGx9ywnoLhyubsDIZmSb04yI9AfY55n6rqO7TiOe/08T57DX7bvOwnjOFKWZZyAGL5pGqqqirZt4z3btsl1XRqGgdZ1pTzPSdM09kEQsEVRxGcEHIrjmAFt21JZllTXNRVFwQCogOH2ZVn4nOd5JEkSOY7DubgQhvVbIQAXCEnwfd+zyqtEDFwKRYqiMBDKkiRhdQz81j9ApmniVkAhBi4Nw5Asy2IY4L7v3154NvVt1wAMUDwQFEO5ruusEKUCBEP8FfgGQy3aATWyLHOJUIs5PD/Ku8RnfM2fMXqIZFVVGZim6f3Cv3r4KfmTUvQQpYqiSIZh3F/q38D3Pv4hVJqmefcOMKiE/QDsDPza//MeFgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200723102329504\"\n        title=\"image-20200723102329504\"\n        src=\"/static/6839bef66a3f95ef60a3c60796a28c98/9516f/image-20200723102329504.png\"\n        srcset=\"/static/6839bef66a3f95ef60a3c60796a28c98/12f09/image-20200723102329504.png 148w,\n/static/6839bef66a3f95ef60a3c60796a28c98/e4a3f/image-20200723102329504.png 295w,\n/static/6839bef66a3f95ef60a3c60796a28c98/9516f/image-20200723102329504.png 538w\"\n        sizes=\"(max-width: 538px) 100vw, 538px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span> </p>\n<ul>\n<li>\n<p>one-hot encoding한 이유:</p>\n<p>vector 내적의 합이 0인 직교행렬을 만들기 위함. </p>\n<p>내적 0 = 유사도 0</p>\n<p>따라서 먼저 모든 단어의 의미를 초기화한다.</p>\n<p>이후 Embedding layer를 거치면 각 word vector들이 <strong>어떤 의미를 갖는 수치 vector로 환원된다.</strong></p>\n</li>\n<li>이때 생각해야 하는 부분은 <strong>'단어를 몇 byte의 수치로 표현할 것인가?'</strong> 이며, 이건 사람이 결정해야 함 </li>\n</ul>\n</blockquote>\n<br>\n</li>\n<li>\n<p><code class=\"language-text\">Embedding layer</code>에 넣어 학습하기</p>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 579px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a566a901df5a309e04b0abec5756fdb5/c08bc/image-20200723102500632.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.10810810810811%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsSAAALEgHS3X78AAACIklEQVQoz02SbY+aQBCA+f9/oJ/64ZJe00Z7NvUtvXrXSxp7OWP1RM6eqICy4AKioLC7sgvbEdK082Gy2Zln3hV+PkcI7Q0j9v3Fcjmfz03TdBxHSlkUoGSW8dFI/TVW+4+DyXSmafPV0iqthSIrrzwXQjDGOOdZllFK4RcsudgTqiPrEc0f1r/vTa3noqfd7lkIdIGTJPE875Smvu8fDocwDOEHQoCN0sx1hi5uYuOTN76O1y1+6BXnn1IOcrGCLIphGKqqYowty4K3rutBEOR5XhQ5Y9k+nDluFxmNrVEjyY/U6wnal9IuCgaFXuDpdLouxbbtxWIBySs4zyXGE8ftbK2G+XKlD9/qd28csx4dR5zPGIuUZTkkIFer1Wazgcy73a6EizRlePuM3A6a15H+3sY107oyzQ+efyv4E2NH5XQ6QXKAQUPlEKsqO0lOx2Pi+6qNWtv15yP9FoiudbhG7k0Y3gvxyvlZgWyapoEGGDYEewIY5iwETxLq4YmNmh5qHumtR9rbqGZtPnp+L8seCYkU8AbYKaXqGcqW5QIpPUPPtt3C9pcD6cb8DqFG4LUJ6XP+RGmkQKswsGrUAIP+D858f4Kctru5lC3kcLvpxEEv3H/PzgNKYwVcYWAAQxSoAh5/YUkIw1jD3tcdborkjp8eDPWdbdzoizrGXcZipTo0OK+8lOo85D8pCDEJeWHsNU1fwv04jtUomgTBiLHkD2+iiCtNsLu/AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200723102500632\"\n        title=\"image-20200723102500632\"\n        src=\"/static/a566a901df5a309e04b0abec5756fdb5/c08bc/image-20200723102500632.png\"\n        srcset=\"/static/a566a901df5a309e04b0abec5756fdb5/12f09/image-20200723102500632.png 148w,\n/static/a566a901df5a309e04b0abec5756fdb5/e4a3f/image-20200723102500632.png 295w,\n/static/a566a901df5a309e04b0abec5756fdb5/c08bc/image-20200723102500632.png 579w\"\n        sizes=\"(max-width: 579px) 100vw, 579px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span> </p>\n<ul>\n<li><code class=\"language-text\">Embedding layer</code>: 일반 layer에 <code class=\"language-text\">Embedding layer</code>를 넣은 것 → 학습할 수 있게 되었음 </li>\n<li><code class=\"language-text\">Embedding layer</code>의 입력층의 뉴런 개수: 사전에 등록된 단어 개수(뉴런 개수는 임의적으로 설정 가능)</li>\n<li>\n<p>단어 1개의 <code class=\"language-text\">latent feature</code> 형성. </p>\n<ul>\n<li><code class=\"language-text\">latent feature</code>: 차원이 늘어나거나 줄어든 채로 데이터를 잘 설명할 수 있는 잠재 공간에서의 벡터</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<br>\n<ul>\n<li><code class=\"language-text\">Embedding layer</code> 자세히 보기:</li>\n</ul>\n<blockquote>\n<p>학습을 통한 문서-1 의 word embedding 표현</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 498px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b58ee8be59728012f8672dba10fb9d4e/79e1b/image-20200723103020209.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 106.75675675675676%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAIAAADJt1n/AAAACXBIWXMAAAsSAAALEgHS3X78AAADEElEQVQ4y32Ta3PiNhSG8/9/Qmf6qfnQTrfbCZfdQAhkl1kIgSyhZIFgwDds2ZZ8kSxbvsoVbNp0J6Fnzmhk+zzzvudIPtM0DUIIANhsNqZp6rquKoquaeunp+VisdtuLcuS1msEYUSpaRguQhYA4qWgzuI4TtM0iiKMMWOM0BBHIWWxBR2Rru+RiLqBHzOWc06P1VSUhaGoP6uOURQFL7nYpA5OVkYugfCvXfSoFBJIlvtCNXNNy1St0PRc0wtF5VEkig+wUD6Yd6BLAvXX681PQ/dC3v7yFb6XwO8r9XzuDtf42zyczfFojMcT/Llf2s4zPJ1OZ7NZ4AeOB9VP0urnIW4ou/N7/2IL3i3k8ykeb+h6RbY7outkvxeJJCn2/QPM+cEwgshG9n5k7GoaHTK5qTtd1+4itWkGMg7APjBMKsbmeQkhEUJifYEd24FCeag+/bGEPUdpyqgHrQ6Qa1uy9TjPeVFWP8bZj4+8KqpDlsc142JThmmxlf/5zl/yX/i7OIvZ4+NyuZKmD/PF8knTjbW0xWFUJQknpHoVL7A4RUqN5XK0Wt0tvg1ns760njzOv/i+mjFYIvSs/BrO8wLjie+3WTKoqtkxH6rqa8WnVTWJjVYW4vzNnsuyhBAp6icAPhpOM0x7ZXlX8QnnY17dV3GfGF1Cyfdb9N82n5XjmGl6X9caFrz00zbNbrJiIMgsvaV2x/UGRZ6e7JmxxPfvDaNpw1aQXaH4A8muAanpRgN53SSZFMUJWNh2XU9VP0PYht4VTjswboDoYh++V7UGDm4YGzMWlyV/Az4OOrasoa7XbdQK8y5iH2X8mxq8M61LjG8oHRESCHdvwqXn+aY50LQacC5JJpSbdlwz6J+yUguCXppO8lM9C2WMiW3fCtuO13aZmFkL4kvTrUO3Q8O+sP1/cBhGtj0C5gfoXgVJO+NfXO/asTtpeptlwzgeF0V2cmAIubLc07W6OOesFPfkAaFrWa4rSl2WL8JwWJbZyaPK85wxgPGA0n6e3eXFnWG0bLsDQFvwqtoWf9Vr+G+K/Y4tL24O8gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200723103020209\"\n        title=\"image-20200723103020209\"\n        src=\"/static/b58ee8be59728012f8672dba10fb9d4e/79e1b/image-20200723103020209.png\"\n        srcset=\"/static/b58ee8be59728012f8672dba10fb9d4e/12f09/image-20200723103020209.png 148w,\n/static/b58ee8be59728012f8672dba10fb9d4e/e4a3f/image-20200723103020209.png 295w,\n/static/b58ee8be59728012f8672dba10fb9d4e/79e1b/image-20200723103020209.png 498w\"\n        sizes=\"(max-width: 498px) 100vw, 498px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span> </p>\n<ul>\n<li>\n<p>입력된 단어는 one-hot으로 단어간 유사도 = 0(직교행렬)이었으나, Embedding vector로 표현된 단어들에는 유사도가 존재한다. 의미적으로 가까운 단어들은 유사도가 높다</p>\n<ul>\n<li>ex: love you / love hate 中 love you의 유사도가 더 높다. </li>\n<li>ex: wine __  __  __  __ objok</li>\n</ul>\n<p>​      __ __ __ __ __ objok</p>\n<p>​\t 모델을 돌리면 여기서 objok는 wine의 종류라고 알 수 있다</p>\n</li>\n<li><code class=\"language-text\">Embedding layer</code>가 하는 일:</li>\n<li>(수치화된 word data로 <code class=\"language-text\">one-hot vector</code>를 생성한다)</li>\n<li>\n<p>위의 1의 one-hot vector와 w행렬을 곱한다 </p>\n<ol>\n<li>one hot * W</li>\n<li>ex) (5,500) * (500,64) = (5,64)</li>\n<li>의문: 1번대로라면 보통 \"vocabulary는 3만개가 되어 곱행렬의 수가 늘어나기 때문에 줄어줄 필요가 있다 <em>?</em> \"</li>\n<li>아니다. 직교 행렬(one-hot vector)일 경우 곱행렬을 할 필요가 없다.</li>\n<li>따라서 행렬이 아무리 커도 곱행렬(곱셈)이 필요 없다 </li>\n<li>look-up 사용</li>\n</ol>\n</li>\n</ul>\n</blockquote>\n <br>\n</li>\n<li>\n<p><code class=\"language-text\">Embedding layer</code>를 <code class=\"language-text\">DL layer</code>에 넣고 학습 + 역전파</p>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1cefea1133a90b7454344e3e6fc320a1/76cea/image-20200723105340770.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 44.5945945945946%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAABrklEQVQoz3VSa0/bQBDM//8r/dRWAYJAaikoCPIhBAU1OCDixMZv+3y+8yOOp7urFiQqVhqtvHc3np3dESgOhw7O42/c399huVzAcVawVQlYi7br0DSNoK5rKlnJHdU5t20rZ5w5Rl1XI0kukeU/6OAGtb1G096gVVdoihBJmiHPc7qTwPM8QRAEKMsSURShKAqkaSp3hLCqFF5ejhFmR2iGCyrdAsMtTHwOYzNCjd1uJwQcrPCzGIYBI2M1XHeCJD9F2pwIvGBMf75A32tUlaXzDeI4lkdMzO3u93sYYwTcOpOJwiyLsd2eCmHWTOBWX+C+foVWv4gshtYGq9UKs9lM1GmtJXO70+kU8/mcPHfeFZa6+KtwQoQneK2+wY/GqPQlKVGi8Pn5SR6GYSiErI4Hwd/s4T8yIdRaYUcKg/QY9vATcXJGAyD/zBU91KLQ930xnpVxy33f/+fdG2FZFthsxlD6TAYSRROy4DuRHImHjvOEh4clFosFrdQS6/Wa1uzwRvQxRtYaaisk0Coon1rwoJSHLNuS2Uba4yEweNcYXFNKyaDYS86u68pq/QHBxaev2DSeswAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200723105340770\"\n        title=\"image-20200723105340770\"\n        src=\"/static/1cefea1133a90b7454344e3e6fc320a1/fcda8/image-20200723105340770.png\"\n        srcset=\"/static/1cefea1133a90b7454344e3e6fc320a1/12f09/image-20200723105340770.png 148w,\n/static/1cefea1133a90b7454344e3e6fc320a1/e4a3f/image-20200723105340770.png 295w,\n/static/1cefea1133a90b7454344e3e6fc320a1/fcda8/image-20200723105340770.png 590w,\n/static/1cefea1133a90b7454344e3e6fc320a1/76cea/image-20200723105340770.png 799w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span> </p>\n</blockquote>\n</li>\n</ol>\n</br>\n<ul>\n<li>\n<p><code class=\"language-text\">embedding</code> → 일반적인 <code class=\"language-text\">DL</code> N/W(<code class=\"language-text\">LSTM</code>, <code class=\"language-text\">CNN</code>등) → 출력층  → W 역전파  →  <code class=\"language-text\">embedding</code> 단어들이 의미를 갖게됨</p>\n<ul>\n<li>역전파되어 의미를 갖게된 수치벡터들은 우리가 공유하는 일반적인 단어의 의미가 아니라, 어떤 특정 목적을 위한 의미가 된다.</li>\n<li>ex) '어떤 특정 목적' : Email classification 등</li>\n</ul>\n</li>\n</ul>\n</br>\n<h4 id=\"code-classlanguage-textembeddingcode-그림-총-정리\" style=\"position:relative;\"><a href=\"#code-classlanguage-textembeddingcode-%EA%B7%B8%EB%A6%BC-%EC%B4%9D-%EC%A0%95%EB%A6%AC\" aria-label=\"code classlanguage textembeddingcode 그림 총 정리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Embedding</code> 그림 총 정리</h4>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6b8ae9a6bd68007d9da160abf753e570/1bba3/image-20200723105226275.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAABlklEQVQozzVSXY+bMBDk//+dVncPV/XlKp1U9fmiKiiUnCDYYAzGBAiQ6c5GWWmFP2ZnZ1gn+76Deb/fNa/XK7qugzEG4zjCNQ51U8N1DqYxMNag73ts2wbGLthFsGtdY59nJDFGOOfQNI2SKYnsvfeYpkmbBR/gjkJ8sIg+og+9YkjshaguS4S2xV2wCQtmYV6WBeu6qkprrYLX26oqzNnC/qhhXoWwGGCdRf4vR1EUOH99wQipkRoKSFhAZSRkkPB2u2EYBrXOGOOA/G+GKiuwDVHxbBhCUBLimRSX8JJJ6yShZRLV0tX7TlU37jfi/CHrP5hCKpbFdt8p7umOxBST8IBkzGenNE1xPB7V+rLMKMqfcNMLmv4FcUilWat3HAyJHriHQx0KB8IhMKjodDpp8nyTvbXvqON3VP4bruNJ9g6Hw6eSsf5yuWidEtIinwgvechOeZ4jyzIhdKqwqn4hxHeY6k3+W4qyNDifc23IoFI641cJmU+lzyfEJrTyUG/RtpWsS/nWOhD+Pw6Fzoijylaezn/Nlq3Ew+BovAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200723105226275\"\n        title=\"image-20200723105226275\"\n        src=\"/static/6b8ae9a6bd68007d9da160abf753e570/fcda8/image-20200723105226275.png\"\n        srcset=\"/static/6b8ae9a6bd68007d9da160abf753e570/12f09/image-20200723105226275.png 148w,\n/static/6b8ae9a6bd68007d9da160abf753e570/e4a3f/image-20200723105226275.png 295w,\n/static/6b8ae9a6bd68007d9da160abf753e570/fcda8/image-20200723105226275.png 590w,\n/static/6b8ae9a6bd68007d9da160abf753e570/efc66/image-20200723105226275.png 885w,\n/static/6b8ae9a6bd68007d9da160abf753e570/c83ae/image-20200723105226275.png 1180w,\n/static/6b8ae9a6bd68007d9da160abf753e570/1bba3/image-20200723105226275.png 1433w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<br>\n</br>\n<hr>\n</br>\n<br>\n<h3 id=\"실습\" style=\"position:relative;\"><a href=\"#%EC%8B%A4%EC%8A%B5\" aria-label=\"실습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>실습</h3>\n<h4 id=\"word-embedding--cnn-개념\" style=\"position:relative;\"><a href=\"#word-embedding--cnn-%EA%B0%9C%EB%85%90\" aria-label=\"word embedding  cnn 개념 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Word Embedding + CNN 개념</h4>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d0e42fb436e362ba9281b26b4e833c75/36eca/image-20200723114609507.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40.54054054054054%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsSAAALEgHS3X78AAABV0lEQVQoz21SSU7DQBD0/yXewAs45MQVznACcQgyKAlZHMvLLJ7FnqXomcQhimhpNO3uVvVUlYsYI9Lx3iEED+ccpsnlWoo4n/P3bVzXU16kJFBtv68wTh5CSOx2WwIPuREsgaecQtsRx26FaVRU+wOLae48U/hRw5kOku8h+29EL+BoIDqPyvZ4ZG9oRIvJTthsXvBePoC1NdSWg3NGzDyMMRiG4QToJMfADtCqR3X4oVtCSImh41jaCnf2CUfbIhiL5dcCH5tF7qmtAhccWussk1Iqgxe2aQlQgHEBKQcaMKfn+4j11OFeveJIS4ML+Kye0ckVyRBhuYWx5kJ1jiLpY6nBWE+HEQ2OcRxzM22uDw1EJWCUQdvVWTtNFJVWmeY8O5tb3DrWNA3KssR6vc7gyflsHC1ODBKQJEmCD/86XczIczG9sq5rcltkfa57//0yF6fP9y8nMWzWKRF0fQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200723114609507\"\n        title=\"image-20200723114609507\"\n        src=\"/static/d0e42fb436e362ba9281b26b4e833c75/fcda8/image-20200723114609507.png\"\n        srcset=\"/static/d0e42fb436e362ba9281b26b4e833c75/12f09/image-20200723114609507.png 148w,\n/static/d0e42fb436e362ba9281b26b4e833c75/e4a3f/image-20200723114609507.png 295w,\n/static/d0e42fb436e362ba9281b26b4e833c75/fcda8/image-20200723114609507.png 590w,\n/static/d0e42fb436e362ba9281b26b4e833c75/efc66/image-20200723114609507.png 885w,\n/static/d0e42fb436e362ba9281b26b4e833c75/c83ae/image-20200723114609507.png 1180w,\n/static/d0e42fb436e362ba9281b26b4e833c75/36eca/image-20200723114609507.png 1526w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>\n<p>Word Embedding: </p>\n<ul>\n<li>특정한 목적을 위한 Word Embedding 방법</li>\n<li>예를 들어 IMDB 파일처럼 영화 리뷰를 긍정적/부정적인 것으로 분류(classification)할 때 쓸 word를 embedding 한다.</li>\n<li>범용적인 embedding을 위한 Word Embedding 방법</li>\n<li>다음 시간에 배움</li>\n</ul>\n</br>\n</li>\n<li>Word Embedding&#x26;CNN, LSTM의 <code class=\"language-text\">단점</code>: </li>\n<li>\n<p>단어에 의미를 부여하지만, 맥락상의 의미가 다른 단어를 구분하지는 못함</p>\n<ul>\n<li>동의어 구분 못함</li>\n<li>bank: 은행과 bank:둑을 구분하지 못한다</li>\n<li>형태적으로 같은 단어는 모두 동일한 <code class=\"language-text\">Embedding vector</code>를 갖는다</li>\n<li>이유: 같은 w를 공유하기 때문 ex: ~의 love</li>\n</ul>\n</li>\n</ul>\n</br>\n<ol start=\"2\">\n<li>\n<p>계속 <code class=\"language-text\">vocabulary</code>를 업데이트 할 수 없는데, <code class=\"language-text\">vocabulary</code>의 업데이트 주기와 새로운 <code class=\"language-text\">말뭉치(corpus)</code> 투입 사이의 간극으로, 새로운 말뭉치가 과거의 <code class=\"language-text\">vocabulary</code>을 고려하기 때문에 문제가 발생한다\t\t\t\t</p>\n<ul>\n<li>없는 단어가 발생한다</li>\n<li>\n<p>out of vocabulary(<code class=\"language-text\">OOV</code>)</p>\n<ul>\n<li>따라서, 모르는 단어는 1로 coding한다</li>\n<li>\n<table>\n<thead>\n<tr>\n<th>idx</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>← padding 용</td>\n</tr>\n<tr>\n<td>1</td>\n<td>← OOV용</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>사용자가 임의 설정해야 함 </p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n  </br>\n</li>\n<li>\n<p><code class=\"language-text\">padding</code>: 문장마다 길이가 다르므로 맞추기 위해 계속 <code class=\"language-text\">padding</code>을 해줘야 한다</p>\n<ul>\n<li>LSTM에서</li>\n<li>\n<p><code class=\"language-text\">time step</code>을 고정시키면 반드시 n개가 필요하여 부족하면 <code class=\"language-text\">padding</code>으로 채워줘야 함</p>\n<ul>\n<li>\n<p>이때 <code class=\"language-text\">padding</code> 귀찮으면, </p>\n<blockquote>\n<p>x = Input(batch_size = (None, None, f=8))로 None을 2번 써도 됨</p>\n<p>근데 이렇게 하면, 학습 시킬 때가 문제다. <code class=\"language-text\">.fit</code> 할 때, <code class=\"language-text\">batch_size</code>를 따로 또 적어줘야 한다.</p>\n<p>model.fit(A, batch_size = 64) </p>\n<p>model.fit(B, batch_size = 1)</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n <br>\n</li>\n</ol>\n</br>\n<h4 id=\"embeddin--cnn-lstm-적용\" style=\"position:relative;\"><a href=\"#embeddin--cnn-lstm-%EC%A0%81%EC%9A%A9\" aria-label=\"embeddin  cnn lstm 적용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Embeddin + cnn-lstm 적용</h4>\n<ol>\n<li>\n<p><code class=\"language-text\">단어 간 유사도 파악</code></p>\n<ol>\n<li>CNN or LSTM 모델 학습(fit) 후</li>\n<li>embedding layer만 따로 빼서</li>\n<li>유클리디안 거리(유사도) 분석 진행</li>\n<li>유클리디안 거리의 숫자가 작을수록 두 단어 사이의 거리가 가깝다.</li>\n</ol>\n</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">### model의 embedding layer = model.layers[0]</span>\n<span class=\"token comment\">### embedding layer의 선이자 가중치 확인법: .get_weights()</span>\n\na <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>get_weights<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># a.shape = (1, 6000, 60)</span>\nfather <span class=\"token operator\">=</span> a<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span>wind<span class=\"token punctuation\">[</span><span class=\"token string\">'father'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span> <span class=\"token comment\"># a[0].shape = (6000, 60)</span>\nmother <span class=\"token operator\">=</span> a<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span>wind<span class=\"token punctuation\">[</span><span class=\"token string\">'mother'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\ndaughter <span class=\"token operator\">=</span> a<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span>wind<span class=\"token punctuation\">[</span><span class=\"token string\">'daughter'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics<span class=\"token punctuation\">.</span>pairwise <span class=\"token keyword\">import</span> euclidean_distances <span class=\"token comment\"># 숫자가 작을수록 유사한 것 </span>\n\nf_m_e_e <span class=\"token operator\">=</span> euclidean_distances<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>father<span class=\"token punctuation\">,</span> mother<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 거리 계산할 때, 비교하고 싶은 건 []를 쳐서 넣어주기  </span>\nf_d_e_e <span class=\"token operator\">=</span> euclidean_distances<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>father<span class=\"token punctuation\">,</span> daughter<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nm_d_e_e <span class=\"token operator\">=</span> euclidean_distances<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>mother<span class=\"token punctuation\">,</span> daughter<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\neuclidean_distances<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>father<span class=\"token punctuation\">,</span> mother<span class=\"token punctuation\">,</span> daughter<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token punctuation\">[</span>f_m_e_e<span class=\"token punctuation\">,</span> f_d_e_e<span class=\"token punctuation\">,</span> m_d_e_e<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n</br>\n<ol start=\"2\">\n<li>\n<p><code class=\"language-text\">CNN Embedding과 LSTM Embedding</code> 따로 구해서 성과 분석 = <code class=\"language-text\">병렬 처리</code></p>\n<ul>\n<li><code class=\"language-text\">CNN</code>이 느끼는 단어의 의미와 <code class=\"language-text\">LSTM</code>이 느끼는 단어의 의미가 다르다, 라는 관점에\n<code class=\"language-text\">Embedding layer</code>를 같이 쓸까(직렬), 따로 쓸까(병렬) 구분 <br></li>\n</ul>\n</li>\n<li>학습 <strong>데이터 구성</strong><br></li>\n<li>문장 <strong>padding</strong><br></li>\n<li>\n<p>(편의를 위해) CNN, LSTM <strong>변수 정의</strong></p>\n<ul>\n<li>각각 <strong>모델 빌드</strong></li>\n<li>input</li>\n<li>embedding</li>\n<li>lstm / cnn(→ pooling → flatten → Dense)<br></li>\n</ul>\n</li>\n<li><strong>Concat</strong><br></li>\n<li><strong>성능 확인</strong><br></li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>x_test<span class=\"token punctuation\">)</span>\ny_pred <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>where<span class=\"token punctuation\">(</span>y_pred <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"Test accuracy:\"</span><span class=\"token punctuation\">,</span> accuracy_score<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> y_pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">y_train_predclass <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict_classes<span class=\"token punctuation\">(</span>x_train<span class=\"token punctuation\">,</span> batch_size<span class=\"token operator\">=</span>batch_size<span class=\"token punctuation\">)</span>\ny_test_predclass <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict_classes<span class=\"token punctuation\">(</span>x_test<span class=\"token punctuation\">,</span> batch_size<span class=\"token operator\">=</span>batch_size<span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>\n<p>predict는 probability를 predict_class는 label을 제공</p>\n<blockquote>\n<table>\n<thead>\n<tr>\n<th>predict의 경우</th>\n<th>predict_classes의 경우</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>[[0.22520512]</td>\n<td>[[0]</td>\n</tr>\n<tr>\n<td>[0.9520419 ]</td>\n<td>[1]</td>\n</tr>\n<tr>\n<td>[0.9672848 ]</td>\n<td>[1]</td>\n</tr>\n<tr>\n<td>[0.02690617]]</td>\n<td>[0]]</td>\n</tr>\n<tr>\n<td>functional keras()</td>\n<td>sequential keras()</td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n</li>\n</ul>\n<p></br><br></p>\n<h5 id=\"학습-데이터-구성\" style=\"position:relative;\"><a href=\"#%ED%95%99%EC%8A%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B5%AC%EC%84%B1\" aria-label=\"학습 데이터 구성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>학습 데이터 구성</h5>\n<h3 id=\"code\" style=\"position:relative;\"><a href=\"#code\" aria-label=\"code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>code</h3>\n<ul>\n<li>imdb data 기준 code<br></li>\n<li>\n<p>imdb.load_data: 빈도별 내림차순으로 정렬된 총 88,584 단어(vocabulary) 中 6,000개의 단어에 index가 표시되어 있다.(제작자가 만들어 둔 것임)</p>\n<ul>\n<li>6,000번째 이후 데이터는 out-of-vocabulary 표시인 '2'로 표시되어 있음</li>\n<li>y_train/test: (binary data) 긍정적 리뷰: 1, 부정적 리뷰: 0</li>\n<li>num<em>words=max</em>features 설정했기 때문에 vocabulary의 6,000번째 이후 데이터는 out-of-vocabulary 표시인 '2'가 표시돼 있다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">max_features <span class=\"token operator\">=</span> <span class=\"token number\">6000</span>    <span class=\"token comment\"># max_features : 최대 단어수</span>\n<span class=\"token punctuation\">(</span>x_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>x_test<span class=\"token punctuation\">,</span> y_test<span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> imdb<span class=\"token punctuation\">.</span>load_data<span class=\"token punctuation\">(</span>num_words<span class=\"token operator\">=</span>max_features<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>0 : padding, 1 : start, 2 : OOV, 3 : Invalid를 의미</p>\n</blockquote>\n<br>\n</li>\n<li>\n<p>vocabulary를 생성</p>\n<ul>\n<li>word2idx : {'단어' : idx} 구조</li>\n<li>idx2word : {idx : '단어'} 구조</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">word2idx <span class=\"token operator\">=</span> imdb<span class=\"token punctuation\">.</span>get_word_index<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nidx2word <span class=\"token operator\">=</span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>v<span class=\"token punctuation\">,</span>k<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> k<span class=\"token punctuation\">,</span>v <span class=\"token keyword\">in</span> word2idx<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>volcaburary idx는 1부터 시작한다. idx2word[1] = 'the'</br>\nx_train에는 단어들이 vocabulary의 index로 표시돼 있다.</br>\n그러나 idx2word에는 padding=0, start=1, OOV=2, Invalid=3은 <strong>포함돼 있지 않다.</strong></br>\n따라서 idx2word의 idx를 3증가 시키고, 아래와 같이 0, 1, 2, 3을 <strong>추가한다.</br></strong>\n즉, **실제 사용된 단어는 idx 4번부터다. **</br></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">idx2word <span class=\"token operator\">=</span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>v<span class=\"token operator\">+</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> k<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> k<span class=\"token punctuation\">,</span> v <span class=\"token keyword\">in</span> word2idx<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># word2idx.items() = (idx, 단어)순으로 나옮</span>\nidx2word<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'&lt;PAD>'</span>  <span class=\"token comment\"># padding 문자 표시</span>\nidx2word<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'&lt;START>'</span>  <span class=\"token comment\"># start 문자 표시</span>\nidx2word<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'&lt;OOV>'</span>  <span class=\"token comment\"># OOV 문자 표시</span>\nidx2word<span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'&lt;INV>'</span>  <span class=\"token comment\"># Invalid 문자 표시</span>\nword2idx <span class=\"token operator\">=</span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>k<span class=\"token punctuation\">,</span> v<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> v<span class=\"token punctuation\">,</span> k <span class=\"token keyword\">in</span> idx2word<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 요걸 실제 사용</span></code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>숫자로 표시된 x_train을 실제 단어로 변환해서 육안으로 확인해 본다.\n(학습과는 무관하다.)</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token triple-quoted-string string\">\"\"\"\nx_train은 idx 값만으로 구성된 list이므로, \n이를 (idx, word)로 되어 있는 word2idx에 넣는다면 딕셔너리의 key값이 일치하는 x_train을 찾고, \n그것의 value값을 뽑으면 x_train의 idx가 무슨 단어를 뜻하는지 알 수 있다.\n(x_train은 0, 1, 2, 3 전처리를 거친 파일이므로 위의 idx2word, word2idx 작업을 통해 두 파일의 길이를 맞춰주었다.\n이제 각 idx의 word만 확인하면 된다.)\n\"\"\"</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">decode</span><span class=\"token punctuation\">(</span>review<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  x <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>idx2word<span class=\"token punctuation\">[</span>s<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> s <span class=\"token keyword\">in</span> review<span class=\"token punctuation\">]</span> \n  <span class=\"token keyword\">return</span> <span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\ndecode<span class=\"token punctuation\">(</span>x_train<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">####### 여기까지가 주어진 데이터에 관한 부분이다.</span></code></pre></div>\n</li>\n</ul>\n<br>\n</br>\n<h5 id=\"embedding--cnn\" style=\"position:relative;\"><a href=\"#embedding--cnn\" aria-label=\"embedding  cnn permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Embedding + CNN</h5>\n<ul>\n<li>1개 리뷰 문서의 단어 개수를 max_length = 400으로 맞춘다.\n400개 보다 작으면 padding = 0을 추가하고, <strong>400개 보다 크면 뒷 부분을 자른다(← index 번호 400 이후는 안 본단 뜻).</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">max_length <span class=\"token operator\">=</span> <span class=\"token number\">400</span>       <span class=\"token comment\"># 한 개 리뷰 문서의 최대 단어 길이</span>\nx_train <span class=\"token operator\">=</span> sequence<span class=\"token punctuation\">.</span>pad_sequences<span class=\"token punctuation\">(</span>x_train<span class=\"token punctuation\">,</span> maxlen<span class=\"token operator\">=</span>max_length<span class=\"token punctuation\">)</span>\nx_test <span class=\"token operator\">=</span> sequence<span class=\"token punctuation\">.</span>pad_sequences<span class=\"token punctuation\">(</span>x_test<span class=\"token punctuation\">,</span> maxlen<span class=\"token operator\">=</span>max_length<span class=\"token punctuation\">)</span></code></pre></div>\n</br>\n<ul>\n<li>Deep Learning architecture parameters</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">batch_size <span class=\"token operator\">=</span> <span class=\"token number\">32</span>\nembedding_dims <span class=\"token operator\">=</span> <span class=\"token number\">60</span> <span class=\"token comment\">#단어 1개를 60개의 수치(feature)로 표현 </span>\nnum_kernels <span class=\"token operator\">=</span> <span class=\"token number\">260</span>        <span class=\"token comment\"># convolution filter 개수</span>\nkernel_size <span class=\"token operator\">=</span> <span class=\"token number\">3</span>          <span class=\"token comment\"># convolution filter size</span>\nhidden_dims <span class=\"token operator\">=</span> <span class=\"token number\">300</span>\nepochs <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n\nxInput <span class=\"token operator\">=</span> Input<span class=\"token punctuation\">(</span>batch_shape <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> max_length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nemb <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>max_features<span class=\"token punctuation\">,</span> embedding_dims<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>xInput<span class=\"token punctuation\">)</span> <span class=\"token comment\">#(400,6) 한 단어(row)당 60개의 wordvector로 수치화한 것 </span>\nemb <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>emb<span class=\"token punctuation\">)</span>\nconv <span class=\"token operator\">=</span> Conv1D<span class=\"token punctuation\">(</span>num_kernels<span class=\"token punctuation\">,</span> kernel_size<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'valid'</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>emb<span class=\"token punctuation\">)</span>\nconv <span class=\"token operator\">=</span> GlobalMaxPooling1D<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>conv<span class=\"token punctuation\">)</span>\nffn <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span>hidden_dims<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>conv<span class=\"token punctuation\">)</span>\nffn <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>ffn<span class=\"token punctuation\">)</span>\nffn <span class=\"token operator\">=</span> Activation<span class=\"token punctuation\">(</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>ffn<span class=\"token punctuation\">)</span>\nffn <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>ffn<span class=\"token punctuation\">)</span>\nyOutput <span class=\"token operator\">=</span> Activation<span class=\"token punctuation\">(</span><span class=\"token string\">'sigmoid'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>ffn<span class=\"token punctuation\">)</span>\n\nmodel <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span>xInput<span class=\"token punctuation\">,</span> yOutput<span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>loss<span class=\"token operator\">=</span><span class=\"token string\">'binary_crossentropy'</span><span class=\"token punctuation\">,</span> optimizer<span class=\"token operator\">=</span><span class=\"token string\">'adam'</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n</br>\n<ul>\n<li>학습</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">hist <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>x_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> \n                 batch_size<span class=\"token operator\">=</span>batch_size<span class=\"token punctuation\">,</span> \n                 epochs<span class=\"token operator\">=</span>epochs<span class=\"token punctuation\">,</span>\n                 validation_data <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>x_test<span class=\"token punctuation\">,</span> y_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n</br>\n<ul>\n<li>성능 확인</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>x_test<span class=\"token punctuation\">)</span>\ny_pred <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>where<span class=\"token punctuation\">(</span>y_pred <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"Test accuracy:\"</span><span class=\"token punctuation\">,</span> accuracy_score<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> y_pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n</br>\n</br>\n</br>\n</br>\n<ul>\n<li>\n<p>참고: </p>\n<blockquote>\n<p> 아마추어 퀀트, blog.naver.com/chunjein</p>\n</blockquote>\n<blockquote>\n<p> 코드 출처: 크리슈나 바브사 외. 2019.01.31. 자연어 처리 쿡북 with 파이썬 [파이썬으로 NLP를 구현하는 60여 가지 레시피]. 에이콘</p>\n</blockquote>\n</li>\n</ul>","excerpt":"NLP & 딥러닝 핵심 문제: \"단어를 어떻게 수치화할 것인가?\" Email - Classification 딥러닝을 이용하여 20개의 카테고리로 분류된 이메일 데이터를 학습하고, 시험 이메일을 20개 카테고리 중 하나로 분류한다.  Email…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_2/#nlp--%EB%94%A5%EB%9F%AC%EB%8B%9D\">NLP &#x26; 딥러닝</a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_2/#email---classification\">Email - Classification</a></li>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_2/#code-classlanguage-textembedding-layercode\"><code class=\"language-text\">Embedding layer</code></a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_2/#code-classlanguage-textembeddingcode-%EB%B0%A9%EB%B2%95\"><code class=\"language-text\">Embedding</code> 방법</a></li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_2/#%EC%8B%A4%EC%8A%B5\">실습</a></li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_2/#code\">code</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/NLP응용_2/"},"frontmatter":{"title":"NLP Embedding","date":"Jul 22, 2020","tags":["NLP","Embedding"],"keywords":["JyneeEarth","jynee"],"update":"Aug 16, 2020"}}},"pageContext":{"slug":"/NLP응용_2/","series":[{"slug":"/NLP응용_1/","title":"NLP 편집거리/주제식별/자연어분석","num":1},{"slug":"/NLP응용_2/","title":"NLP Embedding","num":2},{"slug":"/NLP응용_3/","title":"NLP Word2Vec/SGNS","num":3},{"slug":"/NLP응용_4/","title":"NLP Ask Me Anything","num":4}],"lastmod":"2020-08-16"}},"staticQueryHashes":["3649515864","694178885"]}