{"componentChunkName":"component---src-pages-search-tsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"rawMarkdownBody":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n# Hyper parameter\r\n\r\n* `weight decay`(annealing): epoch(alpha)를 처음에는 적당히 높게 했다가, 점차 줄여 나가는 방법\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# Dense\r\n\r\n* `Dense`: fully connected\r\n\r\n* `ANN(FNN)`에서는 여러 `Dense`를 써도 되지만, `RNN(LSTM)`에선 마지막 층에서만 `Dense`를 써야함.\r\n  * `CNN`에서는 여러 `Dense` 써도 될까?\r\n  * => 일단... `lstm`에서 `lstm() → Dense → lstm()`은 `lstm 네트워크가 2개` 만들어진다고 보면 된다. `lstm() → Dense` 했을 때, `1개의 네트워크`가 형성된 것\r\n  * => 그리고 `CNN`은 일종의 잘 짜여진 레시피라서 `con1D → pooling → Dense → con1D → pooling`은 위 `lstm`처럼 좀 이상한 네트워크 구조가 되는 거라 생각함...\r\n\r\n* Dense(1, activation='sigmoid')\r\n\r\n* **LSTM에서 FNN으로 보내는 마지막 Dense에선 relu 쓰면 안됨**\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# FNN(순방향 신경망)\r\n\r\n* ↔ RNN\r\n\r\n* hidden 층에서\r\n\r\n  * Dense(4, `activation` = 'sigmoid', `kernel_regularizer`=regularizers.l2(0.0001), activation='relu')\r\n  * `Dropout`(rate=0.5)\r\n\r\n* `BatchNormalization`(momentum=0.9, epsilon=0.005, center=True, scale=True, moving_variance_initializer='ones')\r\n\r\n* `predict`까지 끝낸 **연속형** `yHat` 값을, `np.where` 써줘서 **바이너리 형태**로 변환 \r\n\r\n  ``` python\r\n  np.where(yHat > 0.5, 1, 0)\r\n  # 딥러닝_파일: 4-4.ANN(Credit_Keras)_직접 해보기_커스텀loss.py\r\n  ```\r\n\r\n* `history` 활용\r\n\r\n  ```python\r\n  hist.history['loss']\r\n  hist.history['val_loss']\r\n  # 딥러닝_파일: 4-4.ANN(Credit_Keras)_직접 해보기.py\r\n  ```\r\n\r\n* 학습/평가/예측용 model로 나누었을 때 **평가 데이터 활용**\r\n\r\n  ```python\r\n  model.fit(trainX, trainY, validation_data=(evlX, evlY), epochs=200, batch_size=50)\r\n  ```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# LSTM\r\n\r\n* |              | 설명                                                         |\r\n  | ------------ | ------------------------------------------------------------ |\r\n  | 2층          | `lstm()`을 2번 써준다                                        |\r\n  | 양방향       | `bidirectional` + `merge_mode = ‘concat’` <br />FNN, BFN 값을 merge_mode 형태로 합쳐서 list형으로 되돌려줌 |\r\n  | many-to-many | `return-sequences = True`<br />LSTM의 중간 스텝의 출력을 모두 사용 |\r\n  |              | `timedistributed`<br /> FFN으로 가기 전 LSTM 마지막 층에서 각 뉴런의 각 지점에서 계산한 오류를 다음 층으로 전파 |\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# CNN\r\n\r\n* 이미지를 대표할 수 있는 특성들을 도출해서 FNN에 넣어줌\r\n\r\n* | code                                                         | 설명                                                         |\r\n  | ------------------------------------------------------------ | ------------------------------------------------------------ |\r\n  | `Input`(batch_shape = (None, nStep, nFeature, nChannel))     |                                                              |\r\n  | `Conv2D`(filters=30, kernel_size=(8,3), strides=1, padding = 'same', activation='relu') |                                                              |\r\n  | `MaxPooling2D`(pool_size=(2,1), strides=1, padding='valid')  | - 경우에 따라 conv2D, pooling 더 써줄 수 있음<br />- `GlobalMaxPooling1D()`도 있음 |\r\n  | `Flatten()`                                                  | 2D는 4차원이라 shape 맞추려고 보통 flatten을 써줌<br />1d는 안 써도 되는 듯(?) |\r\n  | `Dense`(nOutput, activation='linear')                        |                                                              |\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n## LSTM과 CNN의 차이\r\n\r\n* 둘다 (흐름을 보는)시계열 데이터에 사용할 수 있다.\r\n* LSTM과 CNN1D는 기능은 비슷하지만 CNN1D는 table 中 **(colum 전체+n row)아래 방향으로의 흐름**을 보는 거고, \r\n* LSTM은 bidirectional 을 사용해서 table 中 **위/아래로 흐름**을 이동시켜서 볼 수 있다.\r\n* CNN2D는 kernel_size, pooling 등을 통해 tabel 中 **(n colum(일부분) + n row(일부분) = 내가 focus를 맞춰 보고 싶은 부분)에 따라 그 흐름을 볼 수 있다는 데**서 차이가 있다.\r\n\r\n![image-20200805122723206](../../../../Dropbox/%25EB%25A9%2580%25ED%258B%25B0%25EC%25BA%25A0%25ED%258D%25BC%25EC%258A%25A4%2520nlp%2520%25EA%25B3%25BC%25EC%25A0%2595/md/markdown-images/image-20200805122723206.png)\r\n\r\n\r\n\r\n\r\n\r\n# activation\r\n\r\n* | activation(비선형 함수) | loss                                                         |\r\n  | ----------------------- | ------------------------------------------------------------ |\r\n  | `softmax`               | `sparse_categorical_crossentropy`                            |\r\n  | `sigmoid`               | `binary_crossentropy`                                        |\r\n  | `linear`                | `mse`                                                        |\r\n  | `relu`                  | ← Hidden layer에 씀. 기울기가 0이기 때문에 뉴런이 죽을 수 있는 단점 有 |\r\n  |                         |                                                              |\r\n  | Leakly ReLU             | 뉴런이 죽을 수 있는 현상 해결                                |\r\n  | PReLU                   | x<0 에서 학습 가능                                           |\r\n  | granger causality       | 통제된 상황에서 인과관계가 가능하다고 말할 수 있음. 시계열 데이터에서 쓰일 수 있음 |\r\n\r\n* 딥러닝 네트워크(DN)의 노드는 입력값을 전부 더한 후, 활성화 함수(Activation function)를 통과시켜 다음 노드에 전달한다.\r\n\r\n  * 이때 사용하는 활성화 함수는 비선형 함수를 쓴다. \r\n\r\n\r\n\r\n\r\n\r\n## softmax - sigmoid\r\n\r\n| 구분           | 함수                           | code                                                         |\r\n| -------------- | ------------------------------ | ------------------------------------------------------------ |\r\n| 회귀           | 항등함수(출력값을 그대로 반환) |                                                              |\r\n| 분류(0/1)      | sigmoid                        | # 시험 데이터로 학습 성능을 평가한다<br/>predicted = model.predict(test_input)<br/>test_pred = np.where(predicted > 0.5, 1, 0)<br/>accuracy = (test_label == test_pred).mean() |\r\n| 분류(multiple) | softmax                        |                                                              |\r\n\r\n\r\n\r\n>  Cross-Entropy : 예측한 값과 실제값의 차를 계산. entropy 값이 감소하는 방향으로 진행하다 보면 최저 값을 찾을 수 있다. \r\n>\r\n>  * sshkim Sh.TK. 2017. 8. 23. \"[모두의딥러닝] Softmax Regression (Multinomial Logistic Regression)\". \"https://sshkim.tistory.com/146\"\r\n\r\n>  argmax 을 사용하면 2라는 값이 나온다. 가장 큰 값의 위치가 2번째에 있는 1이기 때문\r\n>\r\n>  * JINSOL KIM. 2017. 12. 24. \"Softmax vs Sigmoid\". https://blog.naver.com/infoefficien/221170205067\r\n\r\n\r\n\r\n\r\n\r\n## ReLu\r\n\r\n* 히든층에 자주 쓰임\r\n\r\n* 그냥 CNN이든 LSTM이든 출력층 Dense에 Relu 쓰지 말자\r\n\r\n  * LSTM에선 Relu 안 쓰는 게 좋음. 특히 출력층엔 쓰면 안 됨.\r\n\r\n  \r\n\r\n\r\n\r\n\r\n\r\n------------------\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# 학습(compile), 예측(predict)\r\n\r\n\r\n\r\n## optimizer\r\n\r\n* | 종류(빈도순)               |\r\n  | -------------------------- |\r\n  | `adam`                     |\r\n  | Adadelta, RMSprop, Adagrad |\r\n  | `momentum`                 |\r\n  | GD, NAG                    |\r\n\r\n* 최적화가 잘 안 되면 글로벌 minmun을 찾지 못하고 로컬 minimum에 빠진다. 이때 로컬 minimum을 **어떻게 빨리** 탈출할 수 있을지 U턴 메소드를 쓸지, 다른 1차 미분방법(GD)를 쓸 지 결정하게 된다. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n## epoch\r\n\r\n* `epoch` 수치가 커지면 `optimizer`가 일을 해서 local이 아닌 global을 찾아간다.\r\n* 그런데 너무 크면 overfitting\r\n* 따라서 적당한 `epoch` 설정이 필요 \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n## Batch_size\r\n\r\n* data가 크면 `batch_size`도 크게\r\n  * 25,000개의 raw data라면 `batch_size` = 20 보다 300 이 정도로 설정\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n-----------------------\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# NLP & DL\r\n\r\n\r\n\r\n## SGNS\r\n\r\n| 용어          | 설명                      | CODE                                | 참고                             |\r\n| ------------- | ------------------------- | ----------------------------------- | -------------------------------- |\r\n| pre-trained   | SGNS에서 학습한 We를 적용 | model.layers[1]**.set_weights**(We) | 해당 code 적용 후 model fit 진행 |\r\n|               |                           |                                     |                                  |\r\n| fine-training |                           |                                     |                                  |\r\n\r\n\r\n\r\n* SGNS에 모델 학습(fit) 시, 학습을 따로 시키는 이유?\r\n\r\n  ```python\r\n  # 학습\r\n  hist = model.fit([X[:, 0], X[:, 1]], X[:, 2], \r\n                   batch_size=BATCH_SIZE,\r\n                   epochs=NUM_EPOCHS)\r\n  ```\r\n\r\n  > *  각기 연결된 가중치 선이 구분되어 있기 때문에\r\n\r\n\r\n\r\n* SGNS 모델 만들 때 dot을 한다면, \r\n\r\n  1. **axis=2**    *@2*\r\n\r\n     → 후에\r\n\r\n  2. reshape**(())**    *@괄호 두 개*\r\n\r\n\r\n\r\n* SGNS로 만든 Embedding의 w(가중치)를 basic한 word data에 적용할 때, load_weights 사용하는 방법도 있다.\r\n\r\n  * 근데 이땐 shape을 맞춰줘야 한다.\r\n\r\n  ```python\r\n  w = encoder.load_weights('model_w.h5') # 가중치(w) 불러온 후,\r\n  emb = Embedding(max_features, embedding_dims, load_weights = w)(xInput) # embedding layer에 바로 적용\r\n  ```\r\n\r\n  * 보통 이런 느낌으로 씀\r\n\r\n    ```python\r\n    weights = load_weights()\r\n    embedding_layer = Embedding(input_dim=V,\r\n                                output_dim=embedding_dim,\r\n                                input_length=input_length,\r\n                                trainable=False,\r\n                                weights=weights,\r\n                                name='embedding')\r\n    ```\r\n\r\n    \r\n\r\n\r\n\r\n## Embedding & pad_sequences\r\n\r\n\r\n\r\n### word2vec 기준\r\n\r\n| word2vec      | code                                                         | input                                                  | output                                                 |\r\n| ------------- | ------------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------ |\r\n| tokenizer     | tokenizer = Tokenizer()<br />tokenizer.fit_on_texts(clean_train_review)<br />train_sequences = tokenizer.texts_to_sequences(clean_train_review) | [안녕, 만나서, 반가워]                                 | [13, 4, 3]                                             |\r\n| pad_sequences | train_inputs = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') | [13, 4, 3]                                             | ([0,0...1,..],<br />[0,0,0,1,0,...],<br />[0,0,1,...]) |\r\n| Embedding     | embedding_layer = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE) | ([0,0...1,..],<br />[0,0,0,1,0,...],<br />[0,0,1,...]) | [0,0...1,..] -> ANN layer                              |\r\n\r\n> embedding_layer 는 결국 pad_sequence된 단어들끼리 모임. 즉, 1개 문장에 대한 임베딩 행렬이 됨 \r\n>\r\n> 1개 단어 = 1개 임베딩 레이어=벡터값\r\n\r\n\r\n\r\n\r\n\r\n### doc2vec 기준\r\n\r\n| doc2vec        | code                                                         | input                                                        | output                                                       |\r\n| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\r\n| TaggedDocument | documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)] | [...,'laughabl',<br/>  'horror'],<br/> ...]                  | TaggedDocument(words=['move', 'last', ... 'horror'], tags=[999]) |\r\n| Embedding      | model = Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.00025, min_count=10, workers=4, dm =1) | TaggedDocument(words=['move', 'last', ... 'horror'], tags=[999]) | [벡터값]                                                     |\r\n\r\n> tags=[999] : 999번 째 문장\r\n>\r\n> Embedding은 model.build_vocab, model.train 거치면 한 문장에 대한 하나의 벡터가 나온다.\r\n>\r\n> (word2vec의 경우 한 문장에 있는 각각의 단어 수만큼 벡터가 나온다.)\r\n>\r\n> 1개 문장 = 1개 임베딩 레이어 = 1개 벡터값\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n----------------------\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# 기타\r\n\r\n\r\n\r\n## 유클리디안 거리\r\n\r\n* 거리 계산할 때, 비교하고 싶은 건 `[]`를 쳐서 넣어주기  \r\n\r\n  ```python\r\n  euclidean_distances([father, mother])\r\n  ```\r\n\r\n\r\n\r\n## 가중치 저장(Save)\r\n\r\n* Embedding (left side) layer의 W를 저장할 때, [2]를 저장한단 사실 알아두기\r\n\r\n  ```python\r\n  with open('data/embedding_W.pickle', 'wb') as f:\r\n      pickle.dump(model.layers[2].get_weights(), f, pickle.HIGHEST_PROTOCOL)\r\n  ```\r\n\r\n\r\n\r\n","excerpt":"Hyper parameter (annealing): epoch(alpha)를 처음에는 적당히 높게 했다가, 점차 줄여 나가는 방법 Dense : fully connected…","fields":{"slug":"/TQT(The question I asked the teacher today.)/"},"frontmatter":{"date":"Aug 01, 2020","title":"TQT(The question I asked the teacher today)","tags":["TQT"],"update":"Aug 06, 2020"}}}]}},"pageContext":{}},"staticQueryHashes":["3649515864","694178885"]}