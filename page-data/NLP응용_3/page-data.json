{"componentChunkName":"component---src-templates-post-tsx","path":"/NLP응용_3/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"nlp--dl\" style=\"position:relative;\"><a href=\"#nlp--dl\" aria-label=\"nlp  dl permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NLP &#x26; DL</h1>\n<ul>\n<li>특수 목적이 아닌, <strong>범용적(일반적)으로 쓰일 Word Embedding</strong>을 만든다.</li>\n<li>\n<p>embedding의 방법</p>\n<ul>\n<li>따라서 문장 속 단어의 맥락(의미)를 파악할 줄 안다.</li>\n<li>즉, semantic 방법을 사용한다.</li>\n</ul>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>참고: </p>\n<ul>\n<li>분포 가설:</li>\n<li>같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다. 따라서 어떤 글에서 비슷한 위치에 존재하는 단어는 단어 간의 유사도가 높다고 판단.</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>빈도 기반의 문서 수치화</th>\n<th>학습 기반의 문서 수치화</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>카운트 기반 방법</td>\n<td>예측 방법</td>\n</tr>\n<tr>\n<td>빠르다<br /></td>\n<td>단어들의 복잡한 특징까지 잘 파악할 수 있다.</td>\n</tr>\n<tr>\n<td>Bag of word(BOW), TF-IDF 활용한 SVD 등</td>\n<td>Word Embedding / Word2Vec</td>\n</tr>\n</tbody>\n</table>\n<br>\n</li>\n</ul>\n</br>\n<h2 id=\"code-classlanguage-textword2veccode\" style=\"position:relative;\"><a href=\"#code-classlanguage-textword2veccode\" aria-label=\"code classlanguage textword2veccode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Word2Vec</code></h2>\n<table>\n<thead>\n<tr>\n<th>Word2Vec</th>\n<th>Word Embedding</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>특정 목적이 아닌 범용적인 목적으로 사용된다.<br />- 방대한 양의 아무문서나 코퍼스를 학습하여 <strong>단어들이 어떤 관계를 갖도록 벡터화(수치화)하는 기술</strong>이다.<br />- 따라서 단어들의 의미가 범용적이다.<br /></td>\n<td>classification 등의 특정 목적을 달성하기 위해 그때마다 학습하는 방식. <br /> - 단어들이 특정 목적에 맞게 벡터화된다.</td>\n</tr>\n<tr>\n<td>사후적으로 결정되는 <strong>Word Embedding 과 달리 사전에 학습</strong>하여 단어의 <strong>맥락을 참조</strong>한 <strong>벡터화</strong>를 진행한다.<br />- 분포가설 이론 사용: ''주변 단어들을 참조하는 등 단어들의 분포를 통해 해당 단어의 의미를 파악한다'', 란 뜻단어의 - 주변 단어(맥락:context)를 참조하여 해당 단어를 수치화한다. 그러면 해당 단어는 인접 단어들과 관계가 맺어지고 인접 단어들 간에는 단어 벡터의 유사도가 높다.</td>\n<td>Word Embedding 벡터는 사후적으로 결정되고, <strong>특정 목적의 용도에 한정</strong>된다.</td>\n</tr>\n<tr>\n<td>방법: continuous back of word (<code class=\"language-text\">CBOW</code>), <code class=\"language-text\">Skip-gram</code></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<br>\n<ul>\n<li>\n<p><em>대표적인 Word2Vec</em> ></p>\n<table>\n<thead>\n<tr>\n<th>CBOW</th>\n<th>Skip-Gram</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>문장을 단어로 전처리한다.</td>\n<td>문장을 단어로 전처리한다.</td>\n</tr>\n<tr>\n<td>수치화하고 싶은 단어가 output 되도록 <br />네트워크 구성주변 단어를 <br />* 순서:<br />input<br />ex: (input) alic, bit 등 여러 개 ... →  <br />hidden layer → <br />(output) hurt<br />따라서:  hidden layer =  중간출력. <br /></td>\n<td>CBOW 를 거꾸로 한 것. <br />- input 1개 , <br />- output 여러 개  <br />* 순서: <br />(input) hurt → <br />hidden layer → <br />(output) alic, bit 등 여러 개 ... <br />- AE 배울 때, 모델 전체 학습 시킨 후, <br />autoencoding 부문만 따로 빼서 <br />목적에 맞게 돌린 것처럼 <br />Skip-Gram도 그렇게 진행함.</td>\n</tr>\n<tr>\n<td>즉, 여러 개로 한 개 예측</td>\n<td>즉, 한 개로 여러 개 예측</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p><em>원리</em> > </p>\n<p>예를 들어 </p>\n<ul>\n<li>(input) hurt를 one-hot encoding해서 hurt의 위치(index) 를 파악한 후, </li>\n<li>(output) alic를 넣었을 때 hurt의 위치(index)를 찾도록 함</li>\n</ul>\n</li>\n<li>예측 시, 더 편리한 건 Skip-Gram.</li>\n<li>단어 하나만 넣으면 output이 나오므로</li>\n</ul>\n<br>\n<ul>\n<li><em>단점</em> ></li>\n<li>\n<p><strong>동음이의어를 제대로 파악하지 못한다.</strong></p>\n<ul>\n<li>실제 word2vec의 위치를 계산할 때 가까운 거리에 있는 값들의 평균으로 계산하기 때문에</li>\n<li>해결방법: <code class=\"language-text\">ELMo</code></li>\n<li>embedding 할 때, 문맥에 따라 가변적으로 vector를 만든다. 즉, 맥락화된 단어 임베딩</li>\n</ul>\n</li>\n<li>\n<p>출력층을 <strong>'softmax'</strong>를 사용해서 <strong>계산량이 많다.</strong></p>\n<ul>\n<li>softmax를 사용하는 이유: one-hot 인코딩 위해서</li>\n<li>그런데 softmax를 사용하기 위해선 전체 단어를 0~1사이의 값으로 표현하기 위해 전부 계산을 진행하는데, 이때 전체 단어가 3만 개 등지가 넘어가는 정도로 큰 vocab일 땐 계산량이 많다.</li>\n<li>\n<p>해결 방법: <code class=\"language-text\">Skip-Gram Negative Sampling(SGNS)</code> </p>\n<ul>\n<li>SGNS는 sigmoid 사용</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>OOV</strong>(Out Of Vocbulary)</p>\n<ul>\n<li>\n<p>해결방법: <code class=\"language-text\">FastText</code></p>\n<ul>\n<li>빈도수가 적은 단어에 대해서도 OOV 문제에 해결 가능성이 높다</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>문서 전체</strong>에 대해선 고려 못한다.</p>\n<ul>\n<li>\n<p>해결방법: <code class=\"language-text\">GloVe</code></p>\n<ul>\n<li>빈도기반(TF-IDF) + 학습기반(Embedding) 방법 혼용</li>\n<li>TF-IDF: 문서 전체에 대한 통계를 사용하지만, 단어별 의미는 고려하지 못한다는 단점과\nWord2Vec: 주변 단어만을 사용하기 때문에 문서 전체에 대해서는 고려하지 못한다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<br>\n</br>\n<ul>\n<li>\n<h3 id=\"code\" style=\"position:relative;\"><a href=\"#code\" aria-label=\"code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>CODE</h3>\n<ul>\n<li>SGNS + CNN</li>\n<li>소설 alice in wonderland에 사용된 단어들을 2차원 feature로 vector화 한다.</li>\n<li>\n<h4 id=\"그림으로-먼저-보기\" style=\"position:relative;\"><a href=\"#%EA%B7%B8%EB%A6%BC%EC%9C%BC%EB%A1%9C-%EB%A8%BC%EC%A0%80-%EB%B3%B4%EA%B8%B0\" aria-label=\"그림으로 먼저 보기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>그림으로 먼저 보기:</h4>\n</li>\n<li><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e2ae678d533b5ae6fd54220b6ff52e2b/67a79/image-20200729173124866.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 19.594594594594593%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsSAAALEgHS3X78AAAA+ElEQVQY0x2PW1OCUBSF+f+/o5ceohkbE5kmEUVTyLgqyU3IEIlQSzOrcXX2eViz5qy19zf7CMtsCtsawJz0UORzlOsQlC0SB2niYrfJkC99VGWMsgiYR6irBV5Sj2fUrVcBtnXKXXj2DbQlEe2WCH3UQRSY8Gc6lE4TqnKLiaHipnEJY6zwub4qoddtQZaucX/XgHh1AZW9Ceaww4Sf7xLnvxqnY4nfUwXPHcEyNQw0GTNvjEe9iynLtnXGO9cectFMEln42q+46ML3txjC4TPH/uMV5MdDwb+dxDZfCOdPcJ0hh8ahxYAPsFlOQK0v837DQASkXdI/lBUZycinSd8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200729173124866\"\n        title=\"image-20200729173124866\"\n        src=\"/static/e2ae678d533b5ae6fd54220b6ff52e2b/fcda8/image-20200729173124866.png\"\n        srcset=\"/static/e2ae678d533b5ae6fd54220b6ff52e2b/12f09/image-20200729173124866.png 148w,\n/static/e2ae678d533b5ae6fd54220b6ff52e2b/e4a3f/image-20200729173124866.png 295w,\n/static/e2ae678d533b5ae6fd54220b6ff52e2b/fcda8/image-20200729173124866.png 590w,\n/static/e2ae678d533b5ae6fd54220b6ff52e2b/efc66/image-20200729173124866.png 885w,\n/static/e2ae678d533b5ae6fd54220b6ff52e2b/c83ae/image-20200729173124866.png 1180w,\n/static/e2ae678d533b5ae6fd54220b6ff52e2b/67a79/image-20200729173124866.png 1408w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></li>\n<li><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ccb65c3805db2af1ac6841a18f8b9db4/58bb7/image-20200729173147483.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50.67567567567568%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAAB90lEQVQozz1S204aURTli/vUPvehSU00sUkv+uKlfWpKk4ZWmyoUtFhuIwiOwshlQGG4DiNXFURYPWs3w8OZs+ecfdZea6/tGQ/raDUKGPZrYOzYZTSsK1yXdUzuWsC8p+4sPKh4ct/GfOZI3v24KYtn04fO8szD5N2dDQT8PhhZDdtb77Hy+qXs5dI59ve8SKfCCBz6cJYMI6kdYTaxhUD/tgqrZqBi6gIqgPzcjRrLirxg8v/dQii4J4BaIoRSIY2T8AHMYgY95xrNeh6FfAqJWBCzaVdwhGHtJgurmpMHZJE3kshexBWDGwCDZTtYEPO+gFEqQcxiWpjyXwCZdPjrm0jW4iG8e7uGN+sr2NxYR+bsRM7+KlZfvZ/kcSIeRDQSUArasNumKGFxx65I7GGzF0+3iskQWAyEETCSf97RJDKngqKSF4sG8MP3WfrdaZXk7rf/uwCSpWc0sHBlnIqrOZWUu0xAz0RxqceUhKbIotPcZ1NbFe9JMbff42FDgLjElEGvip/KST0TwVFoH2urr/Di+TN4v3yU6pTNVTHPxSAyTSX/SF8JQhVGTgNxGEsPnx4dtbqy0wgmMqZhlJI6PZb+xVTvWHh3+4MUoSrObDTiX/ZTAMfKHTrk0ndHyR1ml8mjmj8ywaIvQCze7Zioq1l03/0Dq5TPzEUOPbUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200729173147483\"\n        title=\"image-20200729173147483\"\n        src=\"/static/ccb65c3805db2af1ac6841a18f8b9db4/fcda8/image-20200729173147483.png\"\n        srcset=\"/static/ccb65c3805db2af1ac6841a18f8b9db4/12f09/image-20200729173147483.png 148w,\n/static/ccb65c3805db2af1ac6841a18f8b9db4/e4a3f/image-20200729173147483.png 295w,\n/static/ccb65c3805db2af1ac6841a18f8b9db4/fcda8/image-20200729173147483.png 590w,\n/static/ccb65c3805db2af1ac6841a18f8b9db4/efc66/image-20200729173147483.png 885w,\n/static/ccb65c3805db2af1ac6841a18f8b9db4/58bb7/image-20200729173147483.png 985w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></li>\n</ul>\n<blockquote>\n<p>네트워크에는 center값 넣음</p>\n</blockquote>\n<ul>\n<li><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 194px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c1b36aa628decfe62bb4348373fbc550/2bf95/image-20200729173218124.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 274.3243243243243%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAA3CAYAAAAMsWqVAAAACXBIWXMAAAsSAAALEgHS3X78AAAGNElEQVRYw51X+29UVRBeSfgT+MXAD4aHEGIwhAClyLu2gBJDQAWVmoABoRioRAP8gJgo+EAsiMYnGFBBhAQIGhTEII8U5V0KllLa7qOlpd22u31tH5/nm7Nz9+52t7t6k8k999xz556Zb+abOZ721lpQOtruo6XJ54yBkNxbwwGgpwm93UHnnd7DLQEjfmeO4uGDChWKklAAp08dQ6jZbxR3YUfR+yj6eKsZt5p3NbKms92uo1KOVYeHSlqavCKND+6hqbEa3ZEHyMubhbc3b8T5c79i8OCHceXyn/JBc7Ba1oaafWZtFYINVTK2OnzwtIVrQeF2m4Ne+WtX5wP4vLcwadIEjBgxDL+fPCa7o3lc1xaukTstoNg5K3Em809cHOmoB69FixbioQEDUBsoE5+qv1SSmuwGheZ0RxpQUX4Nzz07H7Nn5+DTXdswdOgjKL5wyijtcHaXEhS3QvoPvc34++IfGDv2cZSWFMtOD/74LYYPH4ZlS5fIrnQ3VNYvyjSZk/Qhfdbb3SgfAGF4q0px9swJY15dFOU6UU7ROTGZSlSCDZWClqLHO5Grqy1HqzGvp6vBQZg/okUUjlWHR/9C31AJQ4a7Q68NZvQEBST6trerUXav6zXcOFY9HrWfigK+21i8eCEO7N+Nv4pP48qlM7h+9Zzx6Wnx62XzXHH3mqAab3ISlFX27P4MT+ZMR86s6Zg5c5qRqZgxYwpyzFx29kQUFCxHpLM+NSjxuey15qJTTKQpYlrQGw3cGoTjUA6kR1nN6GirFbPoP5shDPi69Cjbv9gXRNWaEXDM4U9sjleJBTrP9UxVTVf9zqMLKDbR/XHKiCrNJ4X1GJQZRvqemUVx63BMJlJUoM/0FwP6k50fYML4cXhqbh5O/HLYzLXIu5QmJ4KizzZDIpg/fx5WLF+K3V/vwqSs8fJxLJfTouxz6InmVVeWYOTIEcaHlfBVl2LatCfEJbqjjFBub7UvmSFl/1zGahN3ZJkvPi/CggXPyK4tZaUIbI01Ch2sacV7jChCmDIlG/t/+EbiVHlT89etw6P5aMOj0hkrSZBxNm1aL1lDtBNLACVWArx9Tdaxze8G1Bi2nvf0HJSXXRHC4C76DexUKLulI5oxquw/o+xewBRkwPNdX4UZ1GVd7KY1gsIgB5odpTGFgejaJPTlNpkfSjk18Zefvxhz5uTi5G9HpOZoXYmZHNPhoKyFXgs2EQfaULh2FebOzcX27VsxOTtLAlvX0BXxhd6b3Icaf3X372LIkMEov3NV6goJljtKmynJUo8hUnL9gmHvGWan7Vi18hW8tnq59DqqJFFhkrCJB4WLmdPnzp7AqFGPyi6VyTMq9IlhQ+ezHCxbli8dBHcXivY3/zts6C9WOjfLpGnnvC5kYyjzTnpvEQLw27HM+/pp57wxttF2znkO26LE4sQfkRR0Xayd8znti8M2yUxWVr5fc8fWE8mSNhmrS1KanAiKpf4OaYML1xaY8Ani9cICFK5ZBb9pQhmfrNEZouyNKmxHbu5MHDuyX5SQqdkWHzXPtpMNZNbOUSHNpS+mTp1sSsAladKzTHEaOHAgfj5+UMzvt+rF2jk/GuorpOM6dfIoxo0bK3NEks3S4UP7MHr0KGz78F0ppQSp33aO0lB/T0zaUfSe+fAdGTeZDzduWIcN69dhzJjHsNPUadKZO1T6tHOJHSxN0IMO+fDAgT1484010uJRWXsfk/tBWcfu5pwcCHQ7BKtkmnEuJwqzQYPevT6jQu/OYy34eiaJz+WU7VysPbM11i87UZq6dfMiym5fQqepetqs63rN91g757cKdSFpn3+prLgh1M8iv3XLW4IuyZa57T4FuNs5nfO4TZCDj8mS7/d9ZYr7bGmWNpsD5KBBg7D9oy1yCFKfZVTobWHqxto1K0VB6c1i5C9ZJLmcNXG8nKwYTjHGTlPorcIIXnrxeTkNuK+8vJzomTmSsqb0QZmg0GSWze/2fim7LblxAS/nv2AqXpbUFLKNBn/ado7xxqJ06Ke9uH7tvNAYW7hXVyyVIy5PUtxRX4KNa+csrWs7xzDgmY4fECR7dg45Xb/NYV+Sds5K0sB2px3bDCrOOLDTpZ77pB/flSUH5V8CWqFwKmebQAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200729173218124\"\n        title=\"image-20200729173218124\"\n        src=\"/static/c1b36aa628decfe62bb4348373fbc550/2bf95/image-20200729173218124.png\"\n        srcset=\"/static/c1b36aa628decfe62bb4348373fbc550/12f09/image-20200729173218124.png 148w,\n/static/c1b36aa628decfe62bb4348373fbc550/2bf95/image-20200729173218124.png 194w\"\n        sizes=\"(max-width: 194px) 100vw, 194px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></li>\n<li><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/763557835bbbd81a0f4281d6942a0d27/fe8a7/image-20200729173227834.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 63.51351351351351%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAACsUlEQVQ4y22T2W8bVRTG/T/zQnlGPPFQoEECIVFKWVSJtuoS2sYkTd3EdeIsdrzF9nibfR9nxs64of1x7iTQFnj4dOdq5v7O9517prTMbM4XLovUEtn/aHm1ZqJUZPgOA8th7DiMRH3LJYht8oV1ddYqzpSyMwPf77HMHBT8bylYvrBhZQnA5ad6yJ3DkMetgO9rIQ+agQAvz2SpWl3SM4uSbh3w8vBH2egFNJNKeWbiRTbfVcPi8P1GwPXtmG93Q9YqEdWhB3+avM1N3uQuZ/Ep0+mmPAswS03iSCt0Xrh0uDj3iRKXb3ZCbu2F3DsOCkefbsRce5zwy0HCo1ZCpe/TNnz2hxpPG032RiElBVA9VDClIBxwqm0wtjTK3Uhi+vwsgJp8/LTt8cNezI1tky+2RjxsetysRXwprj9eT/l8K1ZAu2iocvr6PGCsv6KnbTIz9qV/Nk4U0NNH7PYnZFlInmpMZtv4TgXd3OdJJ2HtZcxn5ZiPHs4vgZc37ZDOdYH9wbPdG5j2MYZ1yGarTWtQ5sVJhc5sQnewLkX36Grb2G6L1UKMLHRpkUNbdympuKHEHM+q1Ft3OTldZ/foNpbT5M0q4XnrmEbnLrXuDq1JlyTqCbBGrXkHXQqqVZtWucgDLpbmu8jqlvOlz3BSEYdrEucAz2+zdXJIufoV9yq36RsjbLtOf/yCRvcRQXBamJkn06sJsd9FVloJcGpI9cavzMw6SaxxMJpRPbpFtbPDyNYZTZ/THZbZqH5NFA2l7+EHM/we0Cpue0dmUjl0vQ5vX8+Lkbhf/oTfa7/hJgvSZMBR50FxccXf8Z6hD4CXL52ilwOJFMlcrpYO9bFHz5iKUwM79DhLNNYr13klKfKl9x9o6d8VVAQVvZhP2XuRI/+xRzx3i/1CpHqWJOP/dfgXmUGsxutts6EAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200729173227834\"\n        title=\"image-20200729173227834\"\n        src=\"/static/763557835bbbd81a0f4281d6942a0d27/fcda8/image-20200729173227834.png\"\n        srcset=\"/static/763557835bbbd81a0f4281d6942a0d27/12f09/image-20200729173227834.png 148w,\n/static/763557835bbbd81a0f4281d6942a0d27/e4a3f/image-20200729173227834.png 295w,\n/static/763557835bbbd81a0f4281d6942a0d27/fcda8/image-20200729173227834.png 590w,\n/static/763557835bbbd81a0f4281d6942a0d27/efc66/image-20200729173227834.png 885w,\n/static/763557835bbbd81a0f4281d6942a0d27/c83ae/image-20200729173227834.png 1180w,\n/static/763557835bbbd81a0f4281d6942a0d27/fe8a7/image-20200729173227834.png 1223w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></li>\n</ul>\n<blockquote>\n<ul>\n<li>x값인 7을 input 했을 때 output이 y값으로 8이 나올 때의 네트워크다.</li>\n<li>2개 뉴런으로 줄였을 때의 latent layer를 전체 학습 후 따로 빼내고,</li>\n<li>이때 나온 x좌표와 y좌표로 2D상의 plt에 그림으로 나타내면, 맥락 상 가까운 의미를 가진 단어들끼리 뭉쳐져 있음을 확인할 수 있다.</li>\n</ul>\n</blockquote>\n<ul>\n<li><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 195px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6941bf1b821cc04d5334f80060030eb0/cb9a0/image-20200729173233038.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 280.4054054054054%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,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'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200729173233038\"\n        title=\"image-20200729173233038\"\n        src=\"/static/6941bf1b821cc04d5334f80060030eb0/cb9a0/image-20200729173233038.png\"\n        srcset=\"/static/6941bf1b821cc04d5334f80060030eb0/12f09/image-20200729173233038.png 148w,\n/static/6941bf1b821cc04d5334f80060030eb0/cb9a0/image-20200729173233038.png 195w\"\n        sizes=\"(max-width: 195px) 100vw, 195px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></li>\n</ul>\n</br>\n</li>\n</ul>\n<h3 id=\"code-1\" style=\"position:relative;\"><a href=\"#code-1\" aria-label=\"code 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>code</h3>\n<h4 id=\"step-1\" style=\"position:relative;\"><a href=\"#step-1\" aria-label=\"step 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>STEP 1</h4>\n<ul>\n<li>\n<p>패키지 불러오기</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>preprocessing <span class=\"token keyword\">import</span> OneHotEncoder\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n<span class=\"token keyword\">import</span> nltk\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>corpus <span class=\"token keyword\">import</span> stopwords\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>stem <span class=\"token keyword\">import</span> WordNetLemmatizer\n<span class=\"token keyword\">import</span> string\n<span class=\"token keyword\">from</span> nltk <span class=\"token keyword\">import</span> pos_tag\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>stem <span class=\"token keyword\">import</span> PorterStemmer\n<span class=\"token keyword\">import</span> collections\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Input<span class=\"token punctuation\">,</span> Dense<span class=\"token punctuation\">,</span> Dropout\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Model</code></pre></div>\n</li>\n</ul>\n  <br>\n<ul>\n<li>\n<p>전처리</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">preprocessing</span><span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token comment\"># 한 line(sentence)가 입력됨 </span>\n    \n    <span class=\"token comment\"># step1. 특문 제거</span>\n    text2 <span class=\"token operator\">=</span> <span class=\"token string\">\"\"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">\" \"</span> <span class=\"token keyword\">if</span> ch <span class=\"token keyword\">in</span> string<span class=\"token punctuation\">.</span>punctuation <span class=\"token keyword\">else</span> ch <span class=\"token keyword\">for</span> ch <span class=\"token keyword\">in</span> text<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> \n    <span class=\"token comment\"># for ch in text: 한 sentence에서 하나의 character를 보고, string.punctuation:[!@#$% 등]을 공백처리('')=제거 함  </span>\n    tokens <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>text2<span class=\"token punctuation\">)</span>\n    tokens <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> tokens<span class=\"token punctuation\">]</span> <span class=\"token comment\"># 위 제거에서 살아남은 것들만 .lower() = 소문자로 바꿔서 word에 넣어줌 </span>\n\n\t<span class=\"token comment\"># step2. 불용어 처리(제거)</span>\n\tstopwds <span class=\"token operator\">=</span> stopwords<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span><span class=\"token string\">'english'</span><span class=\"token punctuation\">)</span>\n\ttokens <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>token <span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> tokens <span class=\"token keyword\">if</span> token <span class=\"token keyword\">not</span> <span class=\"token keyword\">in</span> stopwds<span class=\"token punctuation\">]</span> <span class=\"token comment\"># stopword에 없는 것만 token 변수에 저장 </span>\n\n\t<span class=\"token comment\"># step3. 단어의 철자가 3개 이상인 것만 저장 </span>\n\ttokens <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>word <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> tokens <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span><span class=\"token operator\">>=</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span> \n\n\t<span class=\"token comment\"># step4. stemmer: 어간(prefix) 추출(어미(surffix) 제거)  ex: goes -> go / going -> go</span>\n\tstemmer <span class=\"token operator\">=</span> PorterStemmer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\ttokens <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>stemmer<span class=\"token punctuation\">.</span>stem<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> tokens<span class=\"token punctuation\">]</span>\n\n\t<span class=\"token comment\"># step5. 단어의 품사 태깅(tagging)</span>\n\ttagged_corpus <span class=\"token operator\">=</span> pos_tag<span class=\"token punctuation\">(</span>tokens<span class=\"token punctuation\">)</span> <span class=\"token comment\"># ex: (alic, NNP), (love, VB)</span>\n\n\tNoun_tags <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'NN'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'NNP'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'NNPS'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'NNS'</span><span class=\"token punctuation\">]</span>\n\tVerb_tags <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'VB'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'VBD'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'VBG'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'VBN'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'VBP'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'VBZ'</span><span class=\"token punctuation\">]</span>\n\n\t<span class=\"token comment\"># 단어의 원형(표제어,Lemma)을 표시한다 </span>\n\t<span class=\"token comment\">## 표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어' 정도의 의미. 동사와 형용사의 활용형 (surfacial form) 을 분석</span>\n\t<span class=\"token comment\">## 참고: https://wikidocs.net/21707</span>\n\t<span class=\"token comment\">## 걍 형용사/동사를 사전형 단어로 만들었다 생각하기.... </span>\n\t<span class=\"token comment\"># ex: belives -> (stemmer)believe(믿다) // belives -> (lemmatizer)belief(믿음) </span>\n\t<span class=\"token comment\"># (cooking, N) -> cooking / (cooking, V) -> cook</span>\n\t<span class=\"token comment\">## 한국어 예시:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"\n    lemmatize 함수를 쉽게 만들 수 있습니다. \n    띄어쓰기가 지켜진 단어가 입력되었을 때 Komoran 을 이용하여 형태소 분석을 한 뒤, \n    VV 나 VA 태그를 가진 단어에 '-다'를 붙입니다. \n    단, '쉬고싶다' 와 같은 복합 용언도 '쉬다' 로 복원됩니다.\n    출처: https://lovit.github.io/nlp/2019/01/22/trained_kor_lemmatizer/\n    \"\"\"</span>\n\tlemmatizer <span class=\"token operator\">=</span> WordNetLemmatizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\t\n\t<span class=\"token comment\"># 품사에 따라 단어의 lemma가 달라진다 </span>\n\t<span class=\"token comment\"># (cooking, N) -> cooking / (cooking, V) -> cook</span>\n\t<span class=\"token keyword\">def</span> <span class=\"token function\">prat_lemmatize</span><span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">,</span>tag<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    \t<span class=\"token keyword\">if</span> tag <span class=\"token keyword\">in</span> Noun_tags<span class=\"token punctuation\">:</span>\n        \t<span class=\"token keyword\">return</span> lemmatizer<span class=\"token punctuation\">.</span>lemmatize<span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">,</span><span class=\"token string\">'n'</span><span class=\"token punctuation\">)</span>\n    \t<span class=\"token keyword\">elif</span> tag <span class=\"token keyword\">in</span> Verb_tags<span class=\"token punctuation\">:</span>\n        \t<span class=\"token keyword\">return</span> lemmatizer<span class=\"token punctuation\">.</span>lemmatize<span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">,</span><span class=\"token string\">'v'</span><span class=\"token punctuation\">)</span>\n    \t<span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        \t<span class=\"token keyword\">return</span> lemmatizer<span class=\"token punctuation\">.</span>lemmatize<span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">,</span><span class=\"token string\">'n'</span><span class=\"token punctuation\">)</span>\n\n\tpre_proc_text <span class=\"token operator\">=</span>  <span class=\"token string\">\" \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>prat_lemmatize<span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">,</span>tag<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> token<span class=\"token punctuation\">,</span>tag <span class=\"token keyword\">in</span> tagged_corpus<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>      \n    \n    <span class=\"token keyword\">return</span> pre_proc_text</code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>소설 alice in wonderland를 읽어온다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">lines <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\nfin <span class=\"token operator\">=</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"./dataset/alice_in_wonderland.txt\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"r\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> fin<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>line<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span> \n        <span class=\"token keyword\">continue</span> <span class=\"token comment\"># 소설 txt내 엔터 없애기</span>\n    lines<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>preprocessing<span class=\"token punctuation\">(</span>line<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nfin<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>단어들이 사용된 횟수를 카운트 한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">counter <span class=\"token operator\">=</span> collections<span class=\"token punctuation\">.</span>Counter<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> lines<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>line<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n      counter<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+=</span> <span class=\"token number\">1</span></code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>사전을 구축한다.</p>\n<ul>\n<li>가장 많이 사용된 단어를 1번으로 시작해서 번호를 부여한다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">word2idx <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>w<span class=\"token punctuation\">:</span><span class=\"token punctuation\">(</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span>w<span class=\"token punctuation\">,</span>_<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>counter<span class=\"token punctuation\">.</span>most_common<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span> <span class=\"token comment\"># ex: [(apple:50), (cat: 43), ...]</span>\nidx2word <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>v<span class=\"token punctuation\">:</span>k <span class=\"token keyword\">for</span> k<span class=\"token punctuation\">,</span>v <span class=\"token keyword\">in</span> word2idx<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span> <span class=\"token comment\"># ex: [(50: apple), (43: cat), ...]</span></code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>Trigram으로 학습 데이터를 생성한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">xs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>     <span class=\"token comment\"># 입력 데이터</span>\nys <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>     <span class=\"token comment\"># 출력 데이터</span>\n<span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> lines<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># 사전에 부여된 번호로 단어들을 표시한다.</span>\n    <span class=\"token comment\">## 각 문장을 tokenize해서 소문자로 바꾸고 word2idx로 변환 </span>\n    embedding <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>word2idx<span class=\"token punctuation\">[</span>w<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>line<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token comment\"># word2idx: value값인 index번호가 나옴 </span>\n    \n    \n    <span class=\"token comment\"># Trigram으로 주변 단어들을 묶는다.</span>\n    <span class=\"token comment\">## .trigrams(=3)만큼 끊어서 연속된 문장으로 묶기 ex: triples = [(1,2,3), (3,5,3), ...]</span>\n    triples <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>nltk<span class=\"token punctuation\">.</span>trigrams<span class=\"token punctuation\">(</span>embedding<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    \n    \n    <span class=\"token comment\"># 왼쪽 단어, 중간 단어, 오른쪽 단어로 분리한다. </span>\n    w_lefts <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> triples<span class=\"token punctuation\">]</span>   <span class=\"token comment\"># [1, 2, ...8]</span>\n    w_centers <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> triples<span class=\"token punctuation\">]</span> <span class=\"token comment\"># [2, 8, ...13]</span>\n    w_rights <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> triples<span class=\"token punctuation\">]</span>  <span class=\"token comment\"># [8, 13, ...7]</span>\n    \n    <span class=\"token comment\"># 입력 (xs)      출력 (xy)</span>\n    <span class=\"token comment\"># ---------    -----------</span>\n    <span class=\"token comment\"># 1. 중간 단어 --> 왼쪽 단어</span>\n    <span class=\"token comment\"># 2. 중간 단어 --> 오른쪽 단어</span>\n    xs<span class=\"token punctuation\">.</span>extend<span class=\"token punctuation\">(</span>w_centers<span class=\"token punctuation\">)</span>\n    ys<span class=\"token punctuation\">.</span>extend<span class=\"token punctuation\">(</span>w_lefts<span class=\"token punctuation\">)</span>\n    xs<span class=\"token punctuation\">.</span>extend<span class=\"token punctuation\">(</span>w_centers<span class=\"token punctuation\">)</span>\n    ys<span class=\"token punctuation\">.</span>extend<span class=\"token punctuation\">(</span>w_rights<span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n</li>\n<li>\n<p>학습 데이터를 one-hot 형태로 바꾸고, 학습용과 시험용으로 분리한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">vocab_size <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>word2idx<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span>  <span class=\"token comment\"># 사전의 크기 # vocab_size = 1787 # + 1 해줘야 밑에 ohe 할 때, vocab 끝까지 전부를 ohe 할 수 있음 </span>\n\nohe <span class=\"token operator\">=</span> OneHotEncoder<span class=\"token punctuation\">(</span>categories <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>vocab_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># ohe = OneHotEncoder(categories=[range(0, 1787)])</span>\nX <span class=\"token operator\">=</span> ohe<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>xs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>todense<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># .todense = .toarray()와 동일함: 결과를 배열 형태로 변환 </span>\nY <span class=\"token operator\">=</span> ohe<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>ys<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>todense<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>X.shape = (13868, 1787) / y.shape = (13868, 1787)</p>\n</blockquote>\n</li>\n</ul>\n  </br>\n<h4 id=\"step-2-학습용시험용-data로-분리\" style=\"position:relative;\"><a href=\"#step-2-%ED%95%99%EC%8A%B5%EC%9A%A9%EC%8B%9C%ED%97%98%EC%9A%A9-data%EB%A1%9C-%EB%B6%84%EB%A6%AC\" aria-label=\"step 2 학습용시험용 data로 분리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>STEP 2. 학습용/시험용 data로 분리</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Xtrain<span class=\"token punctuation\">,</span> Xtest<span class=\"token punctuation\">,</span> Ytrain<span class=\"token punctuation\">,</span> Ytest<span class=\"token punctuation\">,</span> xstr<span class=\"token punctuation\">,</span> xsts <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> Y<span class=\"token punctuation\">,</span> xs<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">)</span> \n<span class=\"token comment\"># xs를 쓴 이유? => 뒤에서 가까운 단어끼리 그림(plt) 그릴 때 쓰려고</span></code></pre></div>\n<blockquote>\n<p>shape 참고 > </p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>np.array(xs).shape<br />Out[19]: (13868,)</td>\n<td></td>\n</tr>\n<tr>\n<td>np.array(xstr).shape<br/>Out[20]: (11094,)</td>\n<td>np.array(xsts).shape<br/>Out[21]: (2774,)</td>\n</tr>\n<tr>\n<td>np.array(Xtrain).shape<br/>Out[22]: (11094, 1787)</td>\n<td>np.array(Xtest).shape<br/>Out[23]: (2774, 1787)</td>\n</tr>\n<tr>\n<td>np.array(Ytrain).shape<br/>Out[24]: (11094, 1787)</td>\n<td>np.array(Ytest).shape<br/>Out[25]: (2774, 1787)</td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n<br>\n<ul>\n<li>\n<p>딥러닝 모델을 생성한다. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">BATCH_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">128</span>\nNUM_EPOCHS <span class=\"token operator\">=</span> <span class=\"token number\">20</span>\n\ninput_layer <span class=\"token operator\">=</span> Input<span class=\"token punctuation\">(</span>shape <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>Xtrain<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"input\"</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># shape = batch(None) 빼고 y feature의 shape만 넣어주면 됨 </span>\nfirst_layer <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span><span class=\"token number\">300</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> name <span class=\"token operator\">=</span> <span class=\"token string\">\"first\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>input_layer<span class=\"token punctuation\">)</span>\nfirst_dropout <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"firstdout\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>first_layer<span class=\"token punctuation\">)</span>\nsecond_layer <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"second\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>first_dropout<span class=\"token punctuation\">)</span>\nthird_layer <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span><span class=\"token number\">300</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"third\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>second_layer<span class=\"token punctuation\">)</span>\nthird_dropout <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span><span class=\"token string\">\"thirdout\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>third_layer<span class=\"token punctuation\">)</span>\nfourth_layer <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span>Ytrain<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">,</span> name <span class=\"token operator\">=</span> <span class=\"token string\">\"fourth\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>third_dropout<span class=\"token punctuation\">)</span>\n                  <span class=\"token comment\"># Ytrain.shape[1] = Xtrain의 shape과 동일해야 함 </span>\n                  <span class=\"token comment\"># activation='softmax': one-hot이 출력되기 때문에 softmax여야 함 </span>\nmodel <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span>input_layer<span class=\"token punctuation\">,</span> fourth_layer<span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer <span class=\"token operator\">=</span> <span class=\"token string\">\"rmsprop\"</span><span class=\"token punctuation\">,</span> loss<span class=\"token operator\">=</span><span class=\"token string\">\"categorical_crossentropy\"</span><span class=\"token punctuation\">)</span> </code></pre></div>\n<blockquote>\n<p>loss=\"<code class=\"language-text\">categorical_crossentropy</code>\": 만약 one-hot이 아니라, 숫자(vocab의 index)가 출력된다면, loss=\"<code class=\"language-text\">sparse_categorical_crossentropy</code>\"</p>\n</blockquote>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>학습</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">hist <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>Xtrain<span class=\"token punctuation\">,</span> Ytrain<span class=\"token punctuation\">,</span> \n               batch_size<span class=\"token operator\">=</span>BATCH_SIZE<span class=\"token punctuation\">,</span>\n               epochs<span class=\"token operator\">=</span>NUM_EPOCHS<span class=\"token punctuation\">,</span>\n           validation_data <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>Xtest<span class=\"token punctuation\">,</span> Ytest<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>Loss history를 그린다</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">plt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>hist<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">'Train loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>hist<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> label <span class=\"token operator\">=</span> <span class=\"token string\">'Test loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">\"Loss history\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">\"epoch\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">\"loss\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 386px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d218a14080ede6bf5ba44784e86ebc88/7bc0b/image-20200729170655602.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAABx0lEQVQ4y5VUWY7bMAzN/c83/QiQaYEkxiReZFmWvGh7JSknjdugRQUQJC3qcfch54wYo1AI4Q9a1xXX6xXn8xnH4xGXywXee7mLMSDRu5QyURI6MGB9v6NTPYx1mKYJzhX+kPu+F+q6TqjcOzS9wb1VqMhhVVVo27YAmkGjG0Y4n/E/Z7Uaurvj4+Mbvn9+QilVAFmwHMnkwfq/CYVLmlnKEknmyEvKTVNScx5pC/JtrIyUIgHFojNfzPOaS3RgQWstiprIOHq66YFZI28crqNviniLbFuAKJPOcrYNsvkSu4kj5GJ/3W6YCdDoFsHUgHfIfkIOC8nEqZsi55e4RU8l8BR+RciAdV1jnamj8wJNddxl+UIl67wH3mq6S5lBeQz4u7ILOiK70BzGtIP+W6OegKw0W1Oe2RCQmT0Br+RghVuDdPN9n94AclMegDnvH76CM41zwEQOXoF2Xf49wodRehhvXGaOZC7FSNSOCxSNGWdgyIlbI6y1pYZ8eLh5dYZheHJ2NI6jNI0dim4GWtUbcVo7mg5F9f9xqVB3PdnaEiGfZVkkZF56fsyLzmDM53mWXT2dTlSCKCD8M+Fm8o/CWbYrw/4ThNNEsrEMjG4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200729170655602\"\n        title=\"image-20200729170655602\"\n        src=\"/static/d218a14080ede6bf5ba44784e86ebc88/7bc0b/image-20200729170655602.png\"\n        srcset=\"/static/d218a14080ede6bf5ba44784e86ebc88/12f09/image-20200729170655602.png 148w,\n/static/d218a14080ede6bf5ba44784e86ebc88/e4a3f/image-20200729170655602.png 295w,\n/static/d218a14080ede6bf5ba44784e86ebc88/7bc0b/image-20200729170655602.png 386w\"\n        sizes=\"(max-width: 386px) 100vw, 386px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n</blockquote>\n</li>\n</ul>\n</br>\n<h4 id=\"step3-단어들끼리의-거리를-그림으로-나타내는-code\" style=\"position:relative;\"><a href=\"#step3-%EB%8B%A8%EC%96%B4%EB%93%A4%EB%81%BC%EB%A6%AC%EC%9D%98-%EA%B1%B0%EB%A6%AC%EB%A5%BC-%EA%B7%B8%EB%A6%BC%EC%9C%BC%EB%A1%9C-%EB%82%98%ED%83%80%EB%82%B4%EB%8A%94-code\" aria-label=\"step3 단어들끼리의 거리를 그림으로 나타내는 code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>STEP3. 단어들끼리의 거리를 그림으로 나타내는 code</h4>\n<ul>\n<li>\n<p>Word2Vec 수치 확인</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Extracting Encoder section of the Model for prediction of latent variables</span>\n<span class=\"token comment\"># 학습이 완료된 후 중간(hidden layer)의 결과 확인: = Word2Vec layer확인. </span>\n<span class=\"token comment\"># (word2vec: word를 vec(수치)로 표현. 저번 수업에서 w의 값을 '.get_weight()'해서 확인했을 때의 값이 나올 듯)</span>\nencoder <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span>input_layer<span class=\"token punctuation\">,</span> second_layer<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Predicting latent variables with extracted Encoder model</span>\nreduced_X <span class=\"token operator\">=</span> encoder<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>Xtest<span class=\"token punctuation\">)</span> <span class=\"token comment\"># Xtest 넣은 것처럼 임의의 단어를 입력하면 reduced_X = 해당 단어의 Word2Vec형태로 출력됨 </span>\n\n<span class=\"token comment\"># 시험 데이터의 단어들에 대한 2차원 latent feature(word2vec 만드는 layer)인 reduced_X를 데이터 프레임(표)으로 정리한다.</span>\nfinal_pdframe <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>reduced _X<span class=\"token punctuation\">)</span>\nfinal_pdframe<span class=\"token punctuation\">.</span>columns <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"xaxis\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"yaxis\"</span><span class=\"token punctuation\">]</span>\nfinal_pdframe<span class=\"token punctuation\">[</span><span class=\"token string\">\"word_indx\"</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> xsts <span class=\"token comment\"># test 용이므로 train/test split 할 때 같이 나눴던 xstr, xsts 중 y값인 xsts 사용 </span>\nfinal_pdframe<span class=\"token punctuation\">[</span><span class=\"token string\">\"word\"</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> final_pdframe<span class=\"token punctuation\">[</span><span class=\"token string\">\"word_indx\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>idx2word<span class=\"token punctuation\">)</span> <span class=\"token comment\"># index를 word로 변환함 </span>\n\n<span class=\"token comment\"># 데이터 프레임에서 100개를 샘플링한다.</span>\nrows <span class=\"token operator\">=</span> final_pdframe<span class=\"token punctuation\">.</span>sample<span class=\"token punctuation\">(</span>n <span class=\"token operator\">=</span> <span class=\"token number\">100</span><span class=\"token punctuation\">)</span>\nlabels <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>rows<span class=\"token punctuation\">[</span><span class=\"token string\">\"word\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nxvals <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>rows<span class=\"token punctuation\">[</span><span class=\"token string\">\"xaxis\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nyvals <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>rows<span class=\"token punctuation\">[</span><span class=\"token string\">\"yaxis\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>[final_pdframe] >\nOut[26]:\nxaxis     yaxis  word_indx    word\n0     0.301799  0.000000         25    take\n1     0.590210  0.810300        468    pick\n2     0.672298  0.000000          1     say\n3     0.408792  0.520896          9    know\n4     0.387678  0.605502         30    much\n...       ...        ...     ...\n2769  1.309759  0.851837         27    mock\n2770  0.000000  0.423953        622  master\n2771  0.196061  0.299570         83    good\n2772  0.000000  0.024289       1516  deserv\n2773  0.470771  0.550808        497    plan</p>\n<p>[2774 rows x 4 columns]</p>\n</blockquote>\n</li>\n<li>\n<p>샘플링된 100개 단어를 2차원 공간상에 배치</p>\n<ul>\n<li>거리가 가까운 단어들은 서로 관련이 높은 것</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">15</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  \n\n<span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span> label <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  x <span class=\"token operator\">=</span> xvals<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n  y <span class=\"token operator\">=</span> yvals<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n  plt<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>annotate<span class=\"token punctuation\">(</span>label<span class=\"token punctuation\">,</span>xy<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> xytext<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> textcoords<span class=\"token operator\">=</span><span class=\"token string\">'offset points'</span><span class=\"token punctuation\">,</span>\n               ha<span class=\"token operator\">=</span><span class=\"token string\">'right'</span><span class=\"token punctuation\">,</span> va<span class=\"token operator\">=</span><span class=\"token string\">'bottom'</span><span class=\"token punctuation\">,</span> fontsize<span class=\"token operator\">=</span><span class=\"token number\">15</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">\"Dimension 1\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">\"Dimension 2\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/20b059306301966b4ef44eeeda1740be/b04e4/image-20200729170642147.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96.62162162162163%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAACT0lEQVQ4y31Ui46bMBDk/3/o1B+o1PbUqqrurrpLiaLLowRCeNpgjLczTiBwSbqSY4jX69mZMYExRrTW0nWd8Jlzi7muKmnb1v83zFwrilyUqsf8jyPgT1XXMoTBZmutlFkmt6LvrTjXy70IiqKQKE7E4cU5J6pCcbzwoFop6WwnGXKcu2xi3r0RMGGz24pBC9Ngm8e8kALjff2OM5x0psMws8JXCIkkSvaim0aOaSotZoYCupenZ3BZy2a9wlxKnue+aIOcqipmaMeC1vby5/eLNOAxRcGqrCRJDn5Ro2gSR9LqxvO8XC0lhyjxPvbi3CzY970sw1BeXxdeDEbTGsnyFE32vtUCyMK3hV8jAB6qtZq1PXJIrh4fv0mOJIvi/JM0PD/9kvR4Rtpoz+kQCsWiOIbi7orLgH76+uWzJ1vVlZjmdLIBT6vNFp68iEVeuZldcd+00Ihwt9vJzx/fZbfdSl0rCHNClcGH5HSIHOIkSTIWaYD4EEUy+MmbH+OkMmzD0/cg29TlScWSKl4MfET7U0N7pOR8QDlyiM1/YewxESNcvEmRXdDxalLx4ZmiTGPGIV/aiVl7LvZ2toFe1Eqfr15/Qsa2IRbfZxyy1e1mA/5w4e2lEBERzT0kfCzL3N/t6Zr3oYViCgh0047ZHgkG77rtzpvETYzsbrdMAdab9ZX8Q0ztMf0IfMydGTtchTBze3PT/74ut9aDsizl4eETrlp5BZ/8HtLs7rfPwPT5+TbNVOZGKs32OYiagnCm2TmTy+H/4SvP3GHmGuv8A8cHz8RbbzCsAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200729170642147\"\n        title=\"image-20200729170642147\"\n        src=\"/static/20b059306301966b4ef44eeeda1740be/fcda8/image-20200729170642147.png\"\n        srcset=\"/static/20b059306301966b4ef44eeeda1740be/12f09/image-20200729170642147.png 148w,\n/static/20b059306301966b4ef44eeeda1740be/e4a3f/image-20200729170642147.png 295w,\n/static/20b059306301966b4ef44eeeda1740be/fcda8/image-20200729170642147.png 590w,\n/static/20b059306301966b4ef44eeeda1740be/efc66/image-20200729170642147.png 885w,\n/static/20b059306301966b4ef44eeeda1740be/b04e4/image-20200729170642147.png 888w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n</blockquote>\n</li>\n</ul>\n<br>\n</br>\n<h2 id=\"code-classlanguage-textskip-gram-negative-samplingcodesgns\" style=\"position:relative;\"><a href=\"#code-classlanguage-textskip-gram-negative-samplingcodesgns\" aria-label=\"code classlanguage textskip gram negative samplingcodesgns permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Skip-Gram Negative Sampling</code>(SGNS)</h2>\n<ul>\n<li>\n<p>Skip-Gram의 softmax를 활용했기 때문에 계산량이 많다는 단점을 sigmoid 사용하여 보완함</p>\n<ul>\n<li>값이 0~1사이 값이 아니라, 0 아니면 1인 이진 분류로 나옴 </li>\n<li>따라서 계산량 감소 </li>\n</ul>\n<br>\n</li>\n<li>\n<p>방법:</p>\n<p><code class=\"language-text\">Skip-Gram Negative Sampling</code>: </p>\n<ol>\n<li>n-gram으로 선택한 단어 쌍에는 label = 1을 부여하고, 랜덤하게 선택한 단어 쌍에는 label = 0을 부여해서 이진 분류 </li>\n</ol>\n<blockquote>\n<p>N-gram으로 선택한 단어 쌍은 서로 연관된 단어로 인식됨 </p>\n</blockquote>\n<ol start=\"2\">\n<li>2개의 input에 각각의 input, target 값 입력</li>\n<li>각각 vector 값 계산</li>\n<li>두 값 concat(or dot or add) </li>\n<li>sigmoid 계산하여</li>\n<li>label(0 or 1) 값이 나오게 </li>\n<li>학습이 완료된 후에는 아래의 왼쪽 네트워크에 특정 단어를 입력하면, 그 단어에 대한 word vector를 얻을 수 있다.</li>\n</ol>\n</br>\n</li>\n</ul>\n<h3 id=\"skip-gram과-skip-gram-negative-sampling-차이\" style=\"position:relative;\"><a href=\"#skip-gram%EA%B3%BC-skip-gram-negative-sampling-%EC%B0%A8%EC%9D%B4\" aria-label=\"skip gram과 skip gram negative sampling 차이 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>skip-gram과 skip-Gram Negative Sampling 차이</h3>\n<table>\n<thead>\n<tr>\n<th><code class=\"language-text\">Skip-Gram</code></th>\n<th><code class=\"language-text\">Skip-Gram Negative Sampling</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>input 1개<br />input: input data<br />output: target data</td>\n<td>input 2개<br />input[1] : input data<br />input[2] : target data<br />output: label</td>\n</tr>\n<tr>\n<td>label 無</td>\n<td>label 有: 1 or 0으로 이루어져 있음(이진분류)<br />n-gram으로 선택한 단어 쌍에는 label = 1 <br />랜덤하게 선택한 단어 쌍에는 label = 0</td>\n</tr>\n<tr>\n<td>출력층: <strong>softmax</strong> 사용하여 0~1사이의 값 <br /> loss='categorical_crossentropy'<br />따라서 argmax() 함</td>\n<td>출력층: <strong>sigmoid</strong> 사용하여 이진분류<br />loss=\"binary_crossentropy\"</td>\n</tr>\n<tr>\n<td>거리 연산(cosine 등)을 하지 않는다. <br />latent layer에서 벡터 연산을 통해 나온 x,y 좌표로 그림을 그리던가 해서 <br />맥락 상 비슷한 의미를 가진 단어들을 찾아낼 수 있다.</td>\n<td>거리 연산을 할 수 있다. <br />두 개의 input 값에서 나온 vector 값을 하나로 합칠 때 dot 함수를 쓰면 거리 연산을 하는 것과 같다. 이때 cosine 거리 함수를 쓸 수도 있다. <br />그런데, concate 이나 add 함수를 쓰면 거리 계산을 못한다.</td>\n</tr>\n</tbody>\n</table>\n<p>  <br><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4cb19ef4ae5b50325f5f31ef0682dbd2/4f1dd/image-20200729151904052.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAABoElEQVQoz21SDW+sIBD0///F16Rpc8ediiLgF6hMd9baXpNHsizgOszMUoUQcL/fcbvd8Hw+EWPUmOcZy7L8N16/cc2YpgnbtqFKKcE5h77vNXddp2uesyDnrGvma/96dhwHSimaGRUnay3atv2Ja0/GBOe6aRrNvPC6lHVkRkVky1Ht+64/UjbDGAPzMKjrWkG899iPHUc5wFoSCC4o6LZvuidzflNATmldMY0jxpfIq0hMm+ayFwmgbCItH/DO68Wsu4A4KL3itO4Ja96Rk8SSgd8aZTC4AXEUWXmBQCujiz0l/wGknFtt8HE36K2HswNWMpZCGs8iAtZvNdy7w9Se5wRjU+jdMAx6dnoogKZ5wohnyyQF06pS6NFltJCC/bQw/wxiG/8wIhi9Z4OUIY0l4Ie8Q+/kDfqza+zodSsl+ugRp4g1rfrjBTh/d5nAPx5244BHa2Fbj7Sk8029mM3hwqAyT9ZFm1Gk8yHOCkZVKjmnLDePCPOI3gf0IaKT3IpvVsLJ3gmDhzA2j0Yev9T07HIttnhpTqcd7/mMRMkX00dRy2tk9R0AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200729151904052\"\n        title=\"image-20200729151904052\"\n        src=\"/static/4cb19ef4ae5b50325f5f31ef0682dbd2/fcda8/image-20200729151904052.png\"\n        srcset=\"/static/4cb19ef4ae5b50325f5f31ef0682dbd2/12f09/image-20200729151904052.png 148w,\n/static/4cb19ef4ae5b50325f5f31ef0682dbd2/e4a3f/image-20200729151904052.png 295w,\n/static/4cb19ef4ae5b50325f5f31ef0682dbd2/fcda8/image-20200729151904052.png 590w,\n/static/4cb19ef4ae5b50325f5f31ef0682dbd2/efc66/image-20200729151904052.png 885w,\n/static/4cb19ef4ae5b50325f5f31ef0682dbd2/c83ae/image-20200729151904052.png 1180w,\n/static/4cb19ef4ae5b50325f5f31ef0682dbd2/4f1dd/image-20200729151904052.png 1567w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n</br>\n<h3 id=\"sgns의-embedding-활용\" style=\"position:relative;\"><a href=\"#sgns%EC%9D%98-embedding-%ED%99%9C%EC%9A%A9\" aria-label=\"sgns의 embedding 활용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SGNS의 Embedding 활용</h3>\n<ol>\n<li>raw data <strong>전처리</strong></li>\n<li>Trigram으로 <strong>학습할 data 생성</strong></li>\n<li>\n<p><strong>긍정(1), 부정(0) data 생성:SGNS용 학습 데이터를 생성</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">rand_word <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>randint<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>word2idx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>xs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nx_pos <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>vstack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>xs<span class=\"token punctuation\">,</span> ys<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>T\nx_neg <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>vstack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>xs<span class=\"token punctuation\">,</span> rand_word<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>T\n\ny_pos <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span>x_pos<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\ny_neg <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>x_neg<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x_total <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>vstack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>x_pos<span class=\"token punctuation\">,</span> x_neg<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\ny_total <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>vstack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>y_pos<span class=\"token punctuation\">,</span> y_neg<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nX <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>hstack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>x_total<span class=\"token punctuation\">,</span> y_total<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nnp<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>shuffle<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n<li>SGNS 모델 <strong>빌드</strong></li>\n<li><code class=\"language-text\">embedding</code>, <code class=\"language-text\">dot</code>, <code class=\"language-text\">reshape</code>, <code class=\"language-text\">sigmoid</code>, <code class=\"language-text\">binary_crossentropy</code> 등</li>\n<li>SGNS 모델 <strong>학습</strong></li>\n<li>SGNS의 Embedding 모델 만들고, <strong>그 모델의 가중치(w)만 따로 빼서 저장</strong>함</li>\n<li>여기까지가 범용 목적의 SGNS의 Embedding을 만든 절차.</li>\n<li>아래부턴 불특정 word data에 SGNS로 학습한 Embedding 기법을 적용해보는 것임</li>\n<li>raw data로 train/test <strong>data split</strong></li>\n<li>활용할 CNN 모델 빌드(complie 까지)</li>\n<li>CNN 모델 학습 <strong>전</strong>에 SGNS의 Embedding의 가중치(w) load(불러오기)</li>\n<li>CNN 모델 fit 할 때, SKNS에서 학습한 W를 적용: <code class=\"language-text\">model.layers[1].set_weights(We)</code></li>\n<li>plt 그리거나 성능 확인</li>\n<li>\n<p>성능 확인: </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>x_test<span class=\"token punctuation\">)</span>\ny_pred <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>where<span class=\"token punctuation\">(</span>y_pred <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"Test accuracy:\"</span><span class=\"token punctuation\">,</span> accuracy_score<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> y_pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ol>\n<br>\n<ul>\n<li>\n<p>Google's trained Word2Vec model:</p>\n<ul>\n<li>SGNS 방식</li>\n<li>Pre-trained 방식</li>\n<li>문서 → Vector화(수치화) → 일반 DL로 바로 학습 가능</li>\n</ul>\n</li>\n</ul>\n<br>\n</br>\n<h1 id=\"응용-및-발전에-있어-궁금한-점\" style=\"position:relative;\"><a href=\"#%EC%9D%91%EC%9A%A9-%EB%B0%8F-%EB%B0%9C%EC%A0%84%EC%97%90-%EC%9E%88%EC%96%B4-%EA%B6%81%EA%B8%88%ED%95%9C-%EC%A0%90\" aria-label=\"응용 및 발전에 있어 궁금한 점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>응용 및 발전에 있어 궁금한 점</h1>\n<ul>\n<li>\n<p>Word2Vec의 code 中 빈도순으로 index를 부여했었다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">word2idx <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>w<span class=\"token punctuation\">:</span><span class=\"token punctuation\">(</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span>w<span class=\"token punctuation\">,</span>_<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>counter<span class=\"token punctuation\">.</span>most_common<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span> </code></pre></div>\n</li>\n<li>\n<p>그런데 바로 다음, trigram으로 학습 데이터를 구성할 땐</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">embedding <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>word2idx<span class=\"token punctuation\">[</span>w<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>line<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>정말 단순히 word2idx를 단어 찾는 용도로만 썼다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">triples <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>nltk<span class=\"token punctuation\">.</span>trigrams<span class=\"token punctuation\">(</span>embedding<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n<li>\n<p>만약, 위 embedding을 sort 해서 idx number를 재정렬하거나, pre-processing 단계에 embedding을 넣는다. </p>\n<p>그리고 CNN, LSTM 모델을 돌린다면, 빈도가 비슷한 단어들끼리 묶일 것이다.</p>\n<p>그럼 단어의 중요도 순으로 거리를 측정할 텐데, 그럼 index number 1인 단어를 찾고, 그 단어와 다른 단어 사이의 거리를 계산해 특정 점수 구간 이외의 것들을 따로 모아둔다면?</p>\n</li>\n<li>\n<p>예를 들어 가장 많이 나온 단어 happy와 다른 단어들 사이의 거리를 측정하고, 0.5 이하의 코사인 유사도인 것들을 따로 빼서 Another_vocab에 모아둔다. 기존 vocab의 것들은 해당 Document, Sentence의 핵심 keyword 들일 거고, 주인공들이겠지(경우에 따라선 필요가 없는 단어일수도 있겠다.)</p>\n<p>이렇게 다른 문서도 이러한 process를 진행해 Another<em>vocab</em>2를 만든다.</p>\n<p>그 다음 Another<em>vocab와 Another</em>vocab_2의 cosine 유사도를 다시 구해 axis로 통합했을 때의 유사도는 해당 문서 사이의 겹치는 단어들 수치겠지.</p>\n</li>\n<li>이걸 100년치 신문 data에 년별로 적용한다면, 1988년 신문과 2020년 신문이 유사도가 높을 때, 1988년 및 2020년의 국민들의 관심사가 일치한다고 볼 수 있지 않을까?</li>\n</ul>\n</br>\n</br>\n</br>\n</br>\n<ul>\n<li>\n<p>참고: </p>\n<blockquote>\n<p> 아마추어 퀀트, blog.naver.com/chunjein</p>\n</blockquote>\n<blockquote>\n<p>코드 출처: 크리슈나 바브사 외. 2019.01.31. 자연어 처리 쿡북 with 파이썬 [파이썬으로 NLP를 구현하는 60여 가지 레시피]. 에이콘</p>\n</blockquote>\n</li>\n</ul>","excerpt":"NLP & DL 특수 목적이 아닌, 범용적(일반적)으로 쓰일 Word Embedding을 만든다. embedding의 방법 따라서 문장 속 단어의 맥락(의미)를 파악할 줄 안다. 즉, semantic…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_3/#nlp--dl\">NLP &#x26; DL</a></p>\n<ul>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_3/#code-classlanguage-textword2veccode\"><code class=\"language-text\">Word2Vec</code></a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_3/#code-1\">code</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/NLP%EC%9D%91%EC%9A%A9_3/#code-classlanguage-textskip-gram-negative-samplingcodesgns\"><code class=\"language-text\">Skip-Gram Negative Sampling</code>(SGNS)</a></p>\n<ul>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_3/#skip-gram%EA%B3%BC-skip-gram-negative-sampling-%EC%B0%A8%EC%9D%B4\">skip-gram과 skip-Gram Negative Sampling 차이</a></li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_3/#sgns%EC%9D%98-embedding-%ED%99%9C%EC%9A%A9\">SGNS의 Embedding 활용</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/NLP%EC%9D%91%EC%9A%A9_3/#%EC%9D%91%EC%9A%A9-%EB%B0%8F-%EB%B0%9C%EC%A0%84%EC%97%90-%EC%9E%88%EC%96%B4-%EA%B6%81%EA%B8%88%ED%95%9C-%EC%A0%90\">응용 및 발전에 있어 궁금한 점</a></li>\n</ul>","fields":{"slug":"/NLP응용_3/"},"frontmatter":{"title":"NLP Word2Vec/SGNS","date":"Jul 29, 2020","tags":["NLP","Word2Vec","SGNS"],"keywords":["JyneeEarth","jynee"],"update":"Aug 16, 2020"}}},"pageContext":{"slug":"/NLP응용_3/","series":[{"slug":"/NLP응용_1/","title":"NLP 편집거리/주제식별/자연어분석","num":1},{"slug":"/NLP응용_2/","title":"NLP Embedding","num":2},{"slug":"/NLP응용_3/","title":"NLP Word2Vec/SGNS","num":3},{"slug":"/NLP응용_4/","title":"NLP Ask Me Anything","num":4}],"lastmod":"2020-08-16"}},"staticQueryHashes":["3649515864","694178885"]}