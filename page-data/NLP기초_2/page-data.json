{"componentChunkName":"component---src-templates-post-tsx","path":"/NLP기초_2/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"nlp\" style=\"position:relative;\"><a href=\"#nlp\" aria-label=\"nlp permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NLP</h1>\n<ul>\n<li>품사 태깅 원리</li>\n<li>HMM</li>\n</ul>\n<br>\n<br>\n<h2 id=\"품사-태깅\" style=\"position:relative;\"><a href=\"#%ED%92%88%EC%82%AC-%ED%83%9C%EA%B9%85\" aria-label=\"품사 태깅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>품사 태깅</h2>\n<ul>\n<li>\n<p><strong><code class=\"language-text\">품사 태깅</code></strong>: 문장의 N, V, ad, av 판별</p>\n<ul>\n<li>문장만 보고 품사를 붙여주는 기계:  <strong><code class=\"language-text\">pos tagger</code></strong></li>\n</ul>\n</li>\n<li>\n<p>문맥 = '문장 내' 주변 단어 = <strong><code class=\"language-text\">Context</code></strong></p>\n<ul>\n<li>현재 NLP 상에선 문장 간, 절 간 Context는 불가</li>\n</ul>\n</li>\n<li>\n<p>\"NLP 분석 시, 몇 개의 Context를 창조할 것인가?\"</p>\n<ul>\n<li><strong><code class=\"language-text\">n-gram</code></strong>:</li>\n<li>1개: <strong><code class=\"language-text\">unigram</code></strong></li>\n<li>\n<p>2개: **<code class=\"language-text\">Bigram</code> **</p>\n<ul>\n<li>ex: (I love) , (love you)</li>\n</ul>\n</li>\n<li>3개: <strong><code class=\"language-text\">Trigram</code></strong></li>\n<li>4개: ...</li>\n</ul>\n</li>\n<li>\n<p>분석할 문장의 올바른 품사를 결정하기 위해선(올바른 tagger 기계를 만들기 위해선) 사전에 올바른 품사가 정의된 문서 코퍼스(말뭉치)가 있어야 한다.</p>\n<ul>\n<li>nltk: 영어용</li>\n<li>konlpy: 한글용 </li>\n</ul>\n</li>\n</ul>\n<br>\n<h2 id=\"tagging-거치는-원리\" style=\"position:relative;\"><a href=\"#tagging-%EA%B1%B0%EC%B9%98%EB%8A%94-%EC%9B%90%EB%A6%AC\" aria-label=\"tagging 거치는 원리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Tagging 거치는 원리</h2>\n<ol>\n<li>사람이 학습 문서에 품사를 태깅해 놓았음: Tagged Corpora  <code class=\"language-text\">trainX</code></li>\n<li>학습  <code class=\"language-text\">model.fit</code></li>\n<li>모델 파라미터  <code class=\"language-text\">machine</code></li>\n<li>POS Tagger 완성 </li>\n<li>추후 input text 입력 시  <code class=\"language-text\">test X</code></li>\n<li>POS Tagger 거치면 <code class=\"language-text\">model.predict(testX)</code></li>\n<li>Tagging 돼서 출력됨 </li>\n</ol>\n<br>\n<br>\n<h2 id=\"hmm\" style=\"position:relative;\"><a href=\"#hmm\" aria-label=\"hmm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>HMM</h2>\n<ul>\n<li>히든 마코프 모델(HMM): sequence를 분석</li>\n<li>\n<p>1차 Markov Chain:</p>\n<p>: 현재 상태는 직전 상태에만 의존한다</p>\n</li>\n<li>\n<p>2차 Markov Chain:</p>\n<p>: 현재 상태는 전전 상태에만 의존한다<br></p>\n</li>\n<li>\n<p><code class=\"language-text\">Hidden Markov Model(HMM)</code></p>\n<ul>\n<li>관측데이터(주가, 수익률, 거래량 ,변동성, 등...)에 직접 나타나지 않는 히든 상태(Hidden State)가 있다</li>\n<li>이때, <strong>HMM은 관찰 데이터를 가지고 Hidden 상태를 추론하는 것</strong></li>\n<li>MLE 개념 사용</li>\n<li>용어 정리: </li>\n<li><code class=\"language-text\">초기상태</code> = <code class=\"language-text\">초기확률</code></li>\n<li>Hidden State에서 행동 변화가 일어날 확률 <code class=\"language-text\">천이확률</code></li>\n<li>상태 변화가 일어나는 확률: <code class=\"language-text\">출력확률</code></li>\n<li>알고리즘 정리:</li>\n<li><code class=\"language-text\">Forward 알고리즘</code>: X가 나올 확률 계산</li>\n<li>\n<p><code class=\"language-text\">Viterbi decoding 알고리즘</code>: Z 추정</p>\n<ul>\n<li><strong><code class=\"language-text\">Forward 알고리즘</code>은 '확률' 계산이고, <code class=\"language-text\">Viterbi decoding 알고리즘</code>은 '시퀀스' 추정임</strong></li>\n</ul>\n</li>\n<li>\n<p><code class=\"language-text\">Baum Welch 알고리즘</code>: Z 추정</p>\n<ul>\n<li><strong><code class=\"language-text\">Viterbi 알고리즘</code> 과의 차이점: <code class=\"language-text\">Baum Welch 알고리즘</code>는 사전에 주어진 게 X 밖에 없음</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<br>\n<h4 id=\"code-classlanguage-textforward-알고리즘code\" style=\"position:relative;\"><a href=\"#code-classlanguage-textforward-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98code\" aria-label=\"code classlanguage textforward 알고리즘code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Forward 알고리즘</code></h4>\n<ul>\n<li>\n<p>Evaluation Question 문제에서 활용</p>\n<ol>\n<li><code class=\"language-text\">초기 확률(초기 상태)</code>, </li>\n<li><code class=\"language-text\">Transition 확률(천이확률)</code>, </li>\n<li><code class=\"language-text\">Emission 확률(출력확률)</code>이 주어졌을 때, </li>\n<li>관측 데이터가 발생할 <strong>확률</strong>을 <code class=\"language-text\">Forward 알고리즘</code>으로 계산(추정)한다.</li>\n<li><strong>Forward알고리즘은 '확률' 계산이고, Viterbi decoding 알고리즘은 '시퀀스' 추정임</strong>\t</li>\n</ol>\n</li>\n</ul>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">from</span> hmmlearn <span class=\"token keyword\">import</span> hmm</code></pre></div>\n<br>\n<ul>\n<li>히든 상태 정의</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">states <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"Rainy\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"Sunny\"</span><span class=\"token punctuation\">]</span>\nnState <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>states<span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<ul>\n<li>관측 데이터 정의</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">observations <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"Walk\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"Shop\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"Clean\"</span><span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># nObervation = len(observations)</span></code></pre></div>\n<br>\n<ul>\n<li>HMM 모델 빌드</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> hmm<span class=\"token punctuation\">.</span>MultinomialHMM<span class=\"token punctuation\">(</span>n_components<span class=\"token operator\">=</span>nState<span class=\"token punctuation\">)</span> <span class=\"token comment\"># n_components = 2개 </span>\nmodel<span class=\"token punctuation\">.</span>startprob_ <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 초기확률(상태)</span>\nmodel<span class=\"token punctuation\">.</span>transmat_ <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0.4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.6</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 천이확률 Transition </span>\nmodel<span class=\"token punctuation\">.</span>emissionprob_ <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0.6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 출력확률 Emission</span></code></pre></div>\n<blockquote>\n<p> Multinomial(다항분포): 여러 개의 값을 가질 수 있는 독립 확률변수들에 대한 확률분포</p>\n</blockquote>\n<br>\n<ul>\n<li>\n<p><code class=\"language-text\">X</code>: 관측 데이터 시퀀스(Observations Sequence) </p>\n<ul>\n<li>''이렇게 X가 나오도록 Z 값'' 계산하라 中 ''이렇게 X가 나오도록'' 담당 </li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">X <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>T  <span class=\"token comment\"># Walk(0) -> Clean(2) -> Shop(1)</span></code></pre></div>\n<br>\n<ul>\n<li>\n<p><code class=\"language-text\">Forward 알고리즘</code>: <code class=\"language-text\">&#39;.score()&#39;</code></p>\n<ul>\n<li>x가 관측될 likely probability(가능성, 확률) 계산</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">logL <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>score<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span> <span class=\"token comment\"># Forward 알고리즘. sequnce 값이 커지면 확률값이 굉장히 작아져 '0.00..' 등으로 나오니까 defaulf로 log 함수를 취해줌 </span>\np <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>logL<span class=\"token punctuation\">)</span> <span class=\"token comment\">#log를 exp 씌워주면 일반 확률로 변환됨 </span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nProbability of [Walk, Clean, Shop] = %.4f%s\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>p<span class=\"token operator\">*</span><span class=\"token number\">100</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'%'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>Probability of [Walk, Clean, Shop] = 3.1038%</p>\n</blockquote>\n<br>\n<h4 id=\"code-classlanguage-textviterbi-알고리즘code\" style=\"position:relative;\"><a href=\"#code-classlanguage-textviterbi-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98code\" aria-label=\"code classlanguage textviterbi 알고리즘code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Viterbi 알고리즘</code></h4>\n<ul>\n<li>\n<p>Decoding Question 에서 활용</p>\n<ol>\n<li><code class=\"language-text\">초기 확률(초기 상태)</code>, </li>\n<li><code class=\"language-text\">Transition 확률(천이확률)</code>, </li>\n<li><code class=\"language-text\">Emission 확률(출력확률)</code>,</li>\n<li><code class=\"language-text\">관측 데이터 시퀀스(X)</code>가 주어졌을 때, </li>\n<li><strong><code class=\"language-text\">히든 상태의 시퀀스(Z)</code></strong> 을    <code class=\"language-text\">Viterbi decoding 알고리즘</code>으로 계산(추정)한다.</li>\n<li><strong>Forward알고리즘은 '확률' 계산이고, Viterbi decoding 알고리즘은 '시퀀스' 추정임</strong></li>\n</ol>\n</li>\n</ul>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">from</span> hmmlearn <span class=\"token keyword\">import</span> hmm</code></pre></div>\n<br>\n<ul>\n<li>히든 상태 정의</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">states <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"Rainy\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"Sunny\"</span><span class=\"token punctuation\">]</span>\nnState <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>states<span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<ul>\n<li>관측 데이터 정의</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">observations <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"Walk\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"Shop\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"Clean\"</span><span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># nObervation = len(observations)</span></code></pre></div>\n<br>\n<ul>\n<li>HMM 모델 빌드</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> hmm<span class=\"token punctuation\">.</span>MultinomialHMM<span class=\"token punctuation\">(</span>n_components<span class=\"token operator\">=</span>nState<span class=\"token punctuation\">)</span> <span class=\"token comment\"># n_components = 2개 </span>\nmodel<span class=\"token punctuation\">.</span>startprob_ <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 초기확률(상태)</span>\nmodel<span class=\"token punctuation\">.</span>transmat_ <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0.4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.6</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 천이확률 Transition </span>\nmodel<span class=\"token punctuation\">.</span>emissionprob_ <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0.6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 출력확률 Emission</span></code></pre></div>\n<blockquote>\n<p> Multinomial(다항분포): 여러 개의 값을 가질 수 있는 독립 확률변수들에 대한 확률분포</p>\n</blockquote>\n<br>\n<ul>\n<li>\n<p><code class=\"language-text\">X</code>: 관측 데이터 시퀀스(Observations Sequence) </p>\n<ul>\n<li>''이렇게 X가 나오도록 Z 값'' 계산하라 中 <code class=\"language-text\">이렇게 X가 나오도록</code> 담당 </li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">X <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>T <span class=\"token comment\"># walk -> clean -> shop -> walk</span></code></pre></div>\n<br>\n<ul>\n<li>\n<p><code class=\"language-text\">Viterbi 알고리즘</code>: <code class=\"language-text\">&#39;.decode( , algorithm=&quot;viterbi&quot;)&#39;</code></p>\n<ul>\n<li>Z가 관측될 likely probability(가능성, 확률) 계산</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">logprob<span class=\"token punctuation\">,</span> Z <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> algorithm<span class=\"token operator\">=</span><span class=\"token string\">\"viterbi\"</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 여기서 Z는 Z가 될 확률값</span></code></pre></div>\n<br>\n<ul>\n<li>결과 출력</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n  Obervation Sequence :\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\", \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> x<span class=\"token punctuation\">:</span> observations<span class=\"token punctuation\">[</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> X<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Hidden State Sequence :\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\", \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> x<span class=\"token punctuation\">:</span> states<span class=\"token punctuation\">[</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> Z<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Probability = %.6f\"</span> <span class=\"token operator\">%</span> np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>logprob<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>Obervation Sequence : Walk, Walk, Shop, Shop, Walk, Walk, Walk, Walk, Walk, Walk, Walk, Clean, Walk, ...</p>\n<p>Hidden State Sequence : Sunny, Sunny, Sunny, Sunny, Sunny, Sunny, Sunny, Sunny, Sunny, Sunny, Sunny, Rainy, ...</p>\n<p>Probability = 0.000000</p>\n</blockquote>\n<br>\n<h4 id=\"code-classlanguage-textbaum-welch-알고리즘code\" style=\"position:relative;\"><a href=\"#code-classlanguage-textbaum-welch-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98code\" aria-label=\"code classlanguage textbaum welch 알고리즘code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Baum Welch 알고리즘</code></h4>\n<ul>\n<li>\n<p>X만 주어진 경우: <code class=\"language-text\">Learning Question</code> 문제</p>\n<ol>\n<li><code class=\"language-text\">초기 확률(초기 상태)</code>, </li>\n<li><code class=\"language-text\">Transition 확률(천이확률)</code>, </li>\n<li><code class=\"language-text\">Emission 확률(출력확률)</code>을 추정,</li>\n</ol>\n<p> 1-3 까지 <code class=\"language-text\">Baum Welch 알고리즘</code></p>\n<ol start=\"4\">\n<li><code class=\"language-text\">히든 데이터 시퀀스(Z)</code>까지 찾아낸다.</li>\n</ol>\n<p> 4는 <code class=\"language-text\">Viterbi 알고리즘</code>까지 쓴다면</p>\n</li>\n<li>\n<p>활용: 어떤 사람의 행위를 통해 초기 상태와 천이 확률, 그리고 출력 확률을 먼저 추정한 후 Z를 추정한다 </p>\n<ul>\n<li>관찰만으로 전부 추정하는 알고리즘 </li>\n<li>아래 code 내에선 정확도도 꽤 괜찮은 편</li>\n</ul>\n</li>\n</ul>\n<br>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">from</span> hmmlearn <span class=\"token keyword\">import</span> hmm\nnp<span class=\"token punctuation\">.</span>set_printoptions<span class=\"token punctuation\">(</span>precision<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># np.set_printoptions: numpy float 출력옵션 변경. 소수점 몇자리까지만 보고 싶은 경우</span></code></pre></div>\n<br>\n<ul>\n<li>나무랑 w(가중치) 세팅</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">nState <span class=\"token operator\">=</span> <span class=\"token number\">2</span>\npStart <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0.6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.4</span><span class=\"token punctuation\">]</span>\npTran <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\npEmit <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0.6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span></code></pre></div>\n<blockquote>\n<p>해당 code는 추후 <code class=\"language-text\">Baum Welch</code>을 통해 나온 결과값과의 정확도를 보기 위한 것으로, Data를 임의로 설정해주는 부분임에 유의</p>\n</blockquote>\n<blockquote>\n<p>걍 가짜로 X data, Z data 만들어내는 부분</p>\n</blockquote>\n<br>\n<ul>\n<li>주어진 확률 분포대로 관측 데이터 시퀀스를 생성한다. </li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 히든 상태 선택. 확률 = [0.6, 0.4]</span>\ns <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>multinomial<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> pStart<span class=\"token punctuation\">,</span> size<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># {1, pStart=[0.6, 0.4]} 3개 중 가장 큰 수(np.argmax) 따라서 s = 1</span>\nX <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>      <span class=\"token comment\"># Obervation 시퀀스</span>\nZ <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>      <span class=\"token comment\"># 히든 상태 시퀀스</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">5000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># Walk, Shop, Clean ?</span>\n    a <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>multinomial<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> pEmit<span class=\"token punctuation\">[</span>s<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> size<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># pEmit[s] = [0.6, 0.3, 0.1]</span>\n    X<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span>\n    Z<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n    \n    <span class=\"token comment\"># 히든 상태 천이</span>\n    s <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>multinomial<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> pTran<span class=\"token punctuation\">[</span>s<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> size<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nX <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\nX <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nZ <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>Z<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>따라서 현재는 X만 아는 상태 </p>\n</blockquote>\n<blockquote>\n<p>Q. Z는 왜 만드는 것?? 바움 알고리즘은 X만 가지고 예측하는 건데???\nA: 지금 있는 기본 data x랑 나중에 model 만든 거를 합치면 predict z가 나오는데(yHat) 그거랑 찐 z랑 비교하려고</p>\n</blockquote>\n<br>\n<ul>\n<li>\n<p><code class=\"language-text\">Forward 알고리즘</code> -> <code class=\"language-text\">Baum Welch 알고리즘</code> 사용</p>\n<ul>\n<li>Step 1) <code class=\"language-text\">Forward 알고리즘</code> 활용: Observation 시퀀스만을 이용하여, 초기 확률, Transition, Emmision 확률을 추정한다</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">zHat <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>Z<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nminprob <span class=\"token operator\">=</span> <span class=\"token number\">999999999</span> <span class=\"token comment\">#3의 큰 수로 줘버림</span>\n<span class=\"token keyword\">for</span> k <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  model <span class=\"token operator\">=</span> hmm<span class=\"token punctuation\">.</span>MultinomialHMM<span class=\"token punctuation\">(</span>n_components<span class=\"token operator\">=</span>nState<span class=\"token punctuation\">,</span> tol<span class=\"token operator\">=</span><span class=\"token number\">0.0001</span><span class=\"token punctuation\">,</span> n_iter<span class=\"token operator\">=</span><span class=\"token number\">10000</span><span class=\"token punctuation\">)</span>\n  model <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 가짜 data인 x로 학습(fit)</span>\n  predZ <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\n  logprob <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>model<span class=\"token punctuation\">.</span>score<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span> <span class=\"token comment\"># forword 알고리즘 # 원래 값이 음수가 나와서 앞에 '-' 붙여줌으로서 양수로 변환</span></code></pre></div>\n<blockquote>\n<p>logprob = -6349.458034174618</p>\n<p>**EM 알고리즘은 local optimum에 빠질 수 있으므로, 5번 반복하여 로그 우도값이 가장 작은 결과를 채택한다.\n(그게 가장 큰 결과가 되니까 작은 결과 채택).</p>\n</blockquote>\n<br>\n<ul>\n<li>Step 2) <code class=\"language-text\">Baum Welch 알고리즘</code> 활용 : Z를 추정</li>\n<li><strong><code class=\"language-text\">Viterbi 알고리즘</code> 과의 차이점: <code class=\"language-text\">Baum Welch 알고리즘</code>는 사전에 주어진 게 X 밖에 없음</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">  <span class=\"token keyword\">if</span> logprob <span class=\"token operator\">&lt;</span> minprob<span class=\"token punctuation\">:</span>\n      zHat <span class=\"token operator\">=</span> predZ\n      T <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>transmat_\n      E <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>emissionprob_\n      minprob <span class=\"token operator\">=</span> logprob\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"k = %d, logprob = %.2f\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>k<span class=\"token punctuation\">,</span> logprob<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>k = 4, logprob = -6349.46</p>\n</blockquote>\n<br>\n</li>\n<li><code class=\"language-text\">찐 Data 세팅 단계</code>에서 생성한 <code class=\"language-text\">Z</code>와 위 알고리즘들을 통해 추정한 <code class=\"language-text\">zHat</code>의 <strong>정확도를 측정</strong>한다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">accuracy <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>Z <span class=\"token operator\">==</span> zHat<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>Z<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> accuracy <span class=\"token operator\">&lt;</span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">:</span> <span class=\"token comment\"># 정확도가 0.5보다 작다면 순서를 바꿔주는 부분 </span>\n    T <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>fliplr<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>flipud<span class=\"token punctuation\">(</span>T<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># np.fliplr: 좌우 순서 변경</span>\n    E <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>flipud<span class=\"token punctuation\">(</span>E<span class=\"token punctuation\">)</span> <span class=\"token comment\"># np.flipud: 상하 순서 변경</span>\n    zHat <span class=\"token operator\">=</span> <span class=\"token number\">1</span> <span class=\"token operator\">-</span> zHat\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"flipped\"</span><span class=\"token punctuation\">)</span>\n    \naccuracy <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>Z <span class=\"token operator\">==</span> zHat<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>Z<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\naccuracy = %.2f %s\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>accuracy <span class=\"token operator\">*</span> <span class=\"token number\">100</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'%'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>accuracy = 76.78 %</p>\n</blockquote>\n<br>\n<ul>\n<li>추정 결과를 출력한다</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nlog prob = %.2f\"</span> <span class=\"token operator\">%</span> minprob<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nstart prob :\\n\"</span><span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">.</span>startprob_<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\ntrans prob :\\n\"</span><span class=\"token punctuation\">,</span>T<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nemiss prob :\\n\"</span><span class=\"token punctuation\">,</span> E<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\niteration = \"</span><span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">.</span>monitor_<span class=\"token punctuation\">.</span><span class=\"token builtin\">iter</span><span class=\"token punctuation\">)</span> </code></pre></div>\n<blockquote>\n<ul>\n<li>log prob = 5339.32</li>\n<li>start prob :\n[6.93e-72 1.00e+00]</li>\n<li>trans prob :\n[[0.74 0.26]\n[0.15 0.85]]</li>\n<li>emiss prob :\n[[0.11 0.41 0.48]\n[0.58 0.3  0.12]]</li>\n<li>iteration =  246</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>model.monitor<em>.iter: for문 몇 번 돌렸단 뜻. 여기서 위에 \"model = hmm.MultinomialHMM(n</em>components=nState, tol=0.0001, <strong>n_iter=10000</strong>)\" 라 설정했는데 model.monitor.iter가 10000이라 뜨면 값을 못 찾았다는 거라 20000정도로도 늘려보아야 함 </p>\n</blockquote>\n<br>\n<br>\n<h2 id=\"hmm-참고\" style=\"position:relative;\"><a href=\"#hmm-%EC%B0%B8%EA%B3%A0\" aria-label=\"hmm 참고 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>HMM 참고</h2>\n<ul>\n<li>\n<p>Bigram POS tagging과 시험 데이터를 이용한 평가.</p>\n<ul>\n<li><code class=\"language-text\">trade-off between accuracy and coverage</code></li>\n<li>Bigram에서는 만약 NNS VBG 시퀀스가 학습 데이터에 없다면,</li>\n<li>P(VBG|NNS)=0, <code class=\"language-text\">*해석: VBG 안에 NNS가 없음</code></li>\n<li>sparse problem 이므로</li>\n<li>그 이후의 모든 시퀀스에 악영향을 미쳐 평가 결과가 낮다.</li>\n<li>N-gram의 N이 클수록 accuracy는 낮지만 문맥의 coverage는 좋다.  따라서 학습용 데이터를 늘리면 약간 개선되기는 함.</li>\n</ul>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>N-Gram tagging - Combining Tagger (Backoff Tagger)</p>\n<ul>\n<li>Bigram tagging을 시도하고 P(tag2 | tag1) = 0 (tag1 tag2 시퀀스가 없으면)이면, Unigram을 적용한다 P(tag2).</li>\n<li>만약 이것도 없으면 default tag를 적용한다. </li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">t0 <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>DefaultTagger<span class=\"token punctuation\">(</span><span class=\"token string\">'NN'</span><span class=\"token punctuation\">)</span> \nt1 <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>UnigramTagger<span class=\"token punctuation\">(</span>train_sents<span class=\"token punctuation\">,</span> backoff <span class=\"token operator\">=</span> t0<span class=\"token punctuation\">)</span> \nt2 <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>BigramTagger<span class=\"token punctuation\">(</span>train_sents<span class=\"token punctuation\">,</span> backoff <span class=\"token operator\">=</span> t1<span class=\"token punctuation\">)</span> \nt2<span class=\"token punctuation\">.</span>evaluate<span class=\"token punctuation\">(</span>test_sents<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>참고 : </p>\n<p><strong><code class=\"language-text\">nltk.pos_tag()</code></strong>는 PerceptronTagger로 Penn Treebank (Wall Street Journal) <strong>데이터를 사전에 학습</strong>해 놓은 것을 사용한다. </p>\n<p>반면에 UngramTagger나 BigramTagger는 사전에 학습해 놓은 것을 사용하는 것이 아니라 직접 학습해서 사용하는 것이다.</p>\n</blockquote>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>품사 태깅 : N-Gram tagging - Unknown word</p>\n<ul>\n<li>Tagger가 학습 데이터에서 경험하지 못한 단어를 보면 어떻게 태깅해야 하나?</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">text <span class=\"token operator\">=</span> <span class=\"token string\">\"I go to school in the klaldkf\"</span> \ntoken <span class=\"token operator\">=</span> text<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> </code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>unigram_tagger<span class=\"token punctuation\">.</span>tag<span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>[('I', 'PPSS'), ('go', 'VB'), ('to', 'TO'), ('school', 'NN'), ('in', 'IN'), ('the', 'AT'), ('klaldkf', None)]</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>bigram_tagger<span class=\"token punctuation\">.</span>tag<span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> </code></pre></div>\n<blockquote>\n<p>[('I', 'PPSS'), ('go', 'VB'), ('to', 'TO'), <strong>('school', None), ('in', None), ('the', None), ('klaldkf', None)</strong>]</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>nltk<span class=\"token punctuation\">.</span>pos_tag<span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>[('I', 'PRP'), ('go', 'VBP'), ('to', 'TO'), ('school', 'NN'), ('in', 'IN'), ('the', 'DT'), ('klaldkf', 'NN')]</p>\n</blockquote>\n<br>\n<ul>\n<li><code class=\"language-text\">Unigram Tagger</code>는 unknown word에만 영향을 미침. </li>\n<li><strong><code class=\"language-text\">Bigram Tagger</code></strong>는 unknown word가 다른 단어에도 영향을 미침.</li>\n<li><code class=\"language-text\">nltk.pos_tag()</code>는 unknown word를 명사로 태깅하고 있음.</li>\n<li>단, <code class=\"language-text\">Unigram과 Bigram</code>은 충분한 데이터로 학습한 결과가 아니며, <code class=\"language-text\">nltk.pos_tag()</code>은 충분한 데이터로 사전에 학습된 것임.</li>\n</ul>\n</li>\n</ul>\n<br>\n<ul>\n<li>시험 데이터로 태깅 성능을 측정할 때는 <code class=\"language-text\">nltk.ConfusionMatrix</code>를 이용한다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Confusion Matrix </span>\ntest_tags <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>tag <span class=\"token keyword\">for</span> sent <span class=\"token keyword\">in</span> brown<span class=\"token punctuation\">.</span>sents<span class=\"token punctuation\">(</span>categories<span class=\"token operator\">=</span><span class=\"token string\">'editorial'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">,</span> tag<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> t2<span class=\"token punctuation\">.</span>tag<span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> \ngold_tags <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>tag <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">,</span> tag<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> brown<span class=\"token punctuation\">.</span>tagged_words<span class=\"token punctuation\">(</span>categories<span class=\"token operator\">=</span><span class=\"token string\">'editorial'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> \ncm <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>ConfusionMatrix<span class=\"token punctuation\">(</span>gold_tags<span class=\"token punctuation\">,</span> test_tags<span class=\"token punctuation\">)</span> \ncm<span class=\"token punctuation\">[</span><span class=\"token string\">'NN'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'NN'</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>cm<span class=\"token punctuation\">.</span>pretty_format<span class=\"token punctuation\">(</span>truncate<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> sort_by_count<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<br>\n<br>\n<br>\n<ul>\n<li>\n<p>참고: </p>\n<blockquote>\n<ul>\n<li>아마추어 퀀트, blog.naver.com/chunjein</li>\n<li>코드 출처: 크리슈나 바브사 외. 2019.01.31. 자연어 처리 쿡북 with 파이썬 [파이썬으로 NLP를 구현하는 60여 가지 레시피]. 에이콘</li>\n</ul>\n</blockquote>\n</li>\n</ul>","excerpt":"NLP 품사 태깅 원리 HMM 품사 태깅 : 문장의 N, V, ad, av 판별 문장만 보고 품사를 붙여주는 기계:   문맥 = '문장 내' 주변 단어 =  현재 NLP 상에선 문장 간, 절 간 Context는 불가 \"NLP…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/NLP%EA%B8%B0%EC%B4%88_2/#nlp\">NLP</a></p>\n<ul>\n<li><a href=\"/NLP%EA%B8%B0%EC%B4%88_2/#%ED%92%88%EC%82%AC-%ED%83%9C%EA%B9%85\">품사 태깅</a></li>\n<li><a href=\"/NLP%EA%B8%B0%EC%B4%88_2/#tagging-%EA%B1%B0%EC%B9%98%EB%8A%94-%EC%9B%90%EB%A6%AC\">Tagging 거치는 원리</a></li>\n<li><a href=\"/NLP%EA%B8%B0%EC%B4%88_2/#hmm\">HMM</a></li>\n<li><a href=\"/NLP%EA%B8%B0%EC%B4%88_2/#hmm-%EC%B0%B8%EA%B3%A0\">HMM 참고</a></li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/NLP기초_2/"},"frontmatter":{"title":"(NLP 기초) 품사 태깅","date":"Jul 16, 2020","tags":["NLP","기초"],"keywords":["JyneeEarth","jynee"],"update":"Aug 16, 2020"}}},"pageContext":{"slug":"/NLP기초_2/","series":[{"slug":"/NLP기초_2/","title":"(NLP 기초) 품사 태깅","num":2},{"slug":"/NLP기초_3/","title":"(NLP 기초) 문서 정보 추출","num":3},{"slug":"/NLP기초_4/","title":"(NLP 기초) 문장 구조 분석","num":4}],"lastmod":"2020-08-16"}},"staticQueryHashes":["3649515864","694178885"]}