{"componentChunkName":"component---src-templates-post-tsx","path":"/NLP_Transfomer/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"nlp\" style=\"position:relative;\"><a href=\"#nlp\" aria-label=\"nlp permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NLP</h1>\n<br>\n<br>\n<h2 id=\"code-classlanguage-texttransformercode\" style=\"position:relative;\"><a href=\"#code-classlanguage-texttransformercode\" aria-label=\"code classlanguage texttransformercode permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">Transformer</code></h2>\n<ul>\n<li>CNN, RNN 대신 Self-Attention을 사용하는 모델</li>\n</ul>\n<blockquote>\n<p>Transformer는 RNN, LSTM없이 time 시퀀스 역할을 하는 모델입니다. <strong>RNN, LSTM 셀을 일체 사용하지 않았으나, 자체만으로 time 시퀀스 역할을 해줄 수 있는 굉장히 novel한 논문입니다.</strong></p>\n<p>일반적인 Seq2Seq-Attention 모델에서의 번역 태스크의 문제는 원본 언어(Source Language), 번역된 언어(Target Language)간의 어느정도 대응 여부는 어텐션을 통해 찾을 수 있었으나, <strong>각 자신의 언어만에 대해서는 관계를 나타낼수 없었습니다.</strong> 예를 들면 <code class=\"language-text\">I love tiger but it is scare</code>와 <code class=\"language-text\">나는 호랑이를 좋아하지만 그것(호랑이)는 무섭다</code> 사이의 관계는 어텐션을 통해 매칭이 가능했지만 <code class=\"language-text\">it</code><strong>이 무엇을 나타내는지?</strong>와 같은 문제는 기존 Encoder-Decoder 기반의 어텐션 메커니즘에서는 찾을 수 없었습니다.</p>\n<ul>\n<li>출처: <a href=\"https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">platfarm tech team</a></li>\n</ul>\n</blockquote>\n<p><img src=\"https://miro.medium.com/max/619/1*FYsBESLkDO9yNtu0z0mhPw.png\" alt=\"Image for post\"></p>\n<ul>\n<li>\n<p>순서:</p>\n<ol>\n<li>Seq 처리: '<code class=\"language-text\">positional encoding</code>' (순서정보를 입력하면 저장되고 공식에 맞춰 벡터를 수치화 시키는 곳)</li>\n</ol>\n<blockquote>\n<p> 상대적/절대적인 정보를 넣어야 함 <br></p>\n</blockquote>\n<ol start=\"2\">\n<li>그걸 (input) encoder의 attention으로 전달 <br></li>\n<li>2의 결과를 FNN을 전달(단순 linear prejection(차원축소) 위함) <br></li>\n<li>3의 결과를 Decoder의 attention으로 전달<br></li>\n<li>(<code class=\"language-text\">fine Tuning</code>) input Decoder에서 입력한 순서 정보(positional encoding) 출력<br></li>\n<li>5의 결과를 attention으로 전달<br></li>\n<li>4의 결과와 합침<br></li>\n<li>FNN으로 전달(linear, softmax 거침)<br></li>\n<li>그 결과물을 일부(?) 다시 1로 전달<br></li>\n</ol>\n</li>\n<li><code class=\"language-text\">attention</code>: 입력된 값을 가중치로 조정하여 목적에 맞게 바꿔줌 </li>\n<li><code class=\"language-text\">self attention</code>: 입력값인 Q와 transposed K를 내적한 후, Scaled with Softmax 한다. 이는 곧 Self-Attention 시키는 것이고, 이를 통해 attention score 얻을 수 있다.</li>\n<li>\n<p><code class=\"language-text\">Masked</code>: Self-Attention시 자신의 time step 이후 word는 가려 Self-Attention 되는 것을 막는 역할. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token triple-quoted-string string\">'''\n# make mask like this.\n0 1 1\n0 0 1\n0 0 0\n'''</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">get_attn_subsequent_mask</span><span class=\"token punctuation\">(</span>seq<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\nattn_shape <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>seq<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> seq<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> seq<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\nsubsequent_mask <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>triu<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span>attn_shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> k<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nsubsequent_mask <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>from_numpy<span class=\"token punctuation\">(</span>subsequent_mask<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>byte<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">return</span> subsequent_mask</code></pre></div>\n</li>\n<li><code class=\"language-text\">d_model</code> : 임베딩을 하기 위한 차원으로 보통 512를 사용. embedding vector table. <br></li>\n<li><code class=\"language-text\">attention value</code>: 수치화된 table에서 중요한 정보를 골라 가중치를 줄 수 있음<br></li>\n</ul>\n<p><br><br></p>\n<h3 id=\"encoder\" style=\"position:relative;\"><a href=\"#encoder\" aria-label=\"encoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Encoder</h3>\n<table>\n<thead>\n<tr>\n<th><strong><code class=\"language-text\">Positional Encoding</code></strong></th>\n<th><strong><code class=\"language-text\">Multi-Head Attention</code></strong></th>\n<th><strong><code class=\"language-text\">Scaled-Dot Product Attention</code></strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><img src=\"https://miro.medium.com/max/329/1*4HJt3iD5tbtf9wZFuDrM-Q.png\" alt=\"Image for post\"><br /></td>\n<td><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 576px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 129.7297297297297%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAaCAYAAAC3g3x9AAAACXBIWXMAAAsSAAALEgHS3X78AAAD3ElEQVRIx51V+VMaSRTmD93aX/cf2fxiKmajkngsKIRzOAdQjmGAUYZR7sNwlCbgERCMKUtBxYDf9rQOCmKS2q561T1vXn/9vff6vVZhyri7uxutM5nMVP1LQzUJpGwqVypIppJwul1IZdKoVKu/Baqaxuz29hYhPoxEegdichv8Vgy+wMb/A7y7X4DjObDrHpgZC2wuB4Jc8JkXvwScZDAcAhwXJYyHvx3HqS5/+9aBGPehkI+AC5lRLESwLfnx/fvZL0FVT90YDgdUWSqlUCmtoLq7ir2KHuWSFonNN2jUqw/MB6M9T0Mgz6ppMTk+akCI6pCIm+DzqBEKrCAU/BedTvNFZgqGSlnc3Nyg1+sS6eHq+hoXF130+z9gMJggSTu4vOzhmugVG0W63S7VjxjKi/39fXg9QfBcHFxw80EExCIS3s4ugHX5EQmLT/5tEsYCnaO8hHr98BGwUqlhblaHlfcuLM7b8GGOobMsC29NmJ1Zg/qdBep/LFhasFOR/2kW3ffzEoNctoj+bf/e5YSYRIAtIhVvgrVtw6ANQrvkBWMUwBhiVFwWEU5LHHqNH8bVENyMBCl2gDj/BW9mlqHRrOGkfUILQiUlUuB8ZaTjLexsfoXIN5CIHmDDnSMAQQJEwMwiAdnGJrdPQOqQoofIiG3wG1X8+cdfyGazJMaXOD8/h6pU/ETYhOFxxGHWB8Hat7DB7sBuiUCv9WD5PQPtshNG3Tq8ThE+VwJOawx2cxRrKyzWff7xezgY/EDvqou4uIVXr/5GabdIgvwZzdZXmtHXr2fgJ3Vcr3+h+uvrHgLke27+HQqFHHVTFuX60Sy32x24WBYa7SoKpSJqe3s4OjpCLpfDvFqNqCBgt1xG4+CAXO4GrAwDk8VCbWVptVqPWZY3GswmaPU6GK1mmBgrXB4WdqcTGt0aVj/qYXXYYbYxCIRCWNKsQGc0EDsLjETWA340Go1HwFQ6DSmTRKqQhZROghMimFuYp+0ru1ugLSyR2oHO8BF2hwNhIYp8uUTbmje4AcZmQ/ey+wgo0xXiW2C9JAHk9KgQI7GqI5vPIbYl4MPyItyEcSabQbPZBB+NUFvNmhbhCD9ydzgcjnebYrFIqsXzrEZXtVpShhdjOlEUwYf5Z3VMGSrI8un1h1gMBoPRifl8HkO5ExGR9fLYI0lrt9tjzEZZVtCPj4+poWKk6JPJ5AhI2Vyr1egtmGQ31m1OT0/HjBR9lTxOk4CHh4fUflqzHQHKLpydjXfkq6sr2okmYyUDyiH6KaAceLkenxr1+32cnJw8c6vT6bzMcFI5ufln+mlvy39rL2AuubDZywAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"image-20200822105845609\" title=\"image-20200822105845609\" src=\"/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png\" srcset=\"/static/c924934dafc5a3ca5fd9cd80c3820dd0/12f09/image-20200822105845609.png 148w,\n/static/c924934dafc5a3ca5fd9cd80c3820dd0/e4a3f/image-20200822105845609.png 295w,\n/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png 576w\" sizes=\"(max-width: 576px) 100vw, 576px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span><br /></td>\n<td><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 355px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/526ee/image-20200815215109073.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 112.16216216216218%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAYAAADAQbwGAAAACXBIWXMAAAsSAAALEgHS3X78AAAC8ElEQVQ4y5VUW1PaUBDmv/ehfagPbWdqH5xOp4oFsVULeOEiNCAkQAIBBLkngCBFuQhyh6/nBIMIanFnFsKe5cu3Z79dDYgNBgPU63Xc3t6i2Wzi+rqKGv3dbKBcLqNJ4jc3N2i322jftVGr1ZT8RqOhxMbjMYXBZDKBhj70uh0IPA+W4xAI8AgGBYjhCNKpJOIXCcTicXCsF5KcR6FQQCgYRDQSgSAI4IUger3eAyD9eMro281mM/x+P2xWK1iWRTgcRiqZgt1uB8MwaLVaMyAVRzMfoK7Sp2VFo1FkMmmk0yl4vR7kpByq1b+4uIgjFAop16P+/1HJ84DUVNB+f4BYLAFJKuDk2IZ4LInYeYK8rD6rZL7CWckUYN5Ho5GSIPAR6L+7sbFuxPqHn9j4/BuGTQ+OzS7lfDQaL4Fq8IJdFsrQbe3D5fDh1HIGN+PH7o4JYfF8VsliDzRUFrRb2WwWoihCykkIBHmccg7YPDZo97TwiB64eAZ3nTbpskyk0lpiNgOkYCaTCVbSSYvFAuYPA8PBLg5DJ9i266G16KBzGLDPGiETsMpV5VEzlhhGiJ7cbjecTidKpdK0w/UaOB8HlvgPvQ4C0SXrY9HtdVEsFpUBeJYhnRIqzH6/r0zMcDKG38LAu2VEcM8O3mBB+MABcdeGZCiKYunyZcDFAE0JMSzOdo7g+WWFSz/99hHPEMn8F3BegzOBEs/JaYTCHpiPDIgneOQvZeVMluVHE/IswwdRTzXoY/dxan0Lp/0DTg7fEFGzSlwi0/IqwMlkKtZKRUIm7YfbZSKSEkijKveA0mqA8yOXSefgsAZgPWaxvWmEyykStj6ywmqKElZuigro9fBkzExYe/cNH99r8WlNh69fTMhliQ4rV0tL4VlANYHKx+tlkUplyO4TwXE+snWyylk+n3+lbO6TqIDpXVHwcrmEbrc7i3c6ndUZqknValVZ79Tq9cYMhLIbDoerM1xMVO/1qd33lD3LcNXnRfsHVU52tbegVB4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200815215109073\"\n        title=\"image-20200815215109073\"\n        src=\"/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/526ee/image-20200815215109073.png\"\n        srcset=\"/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/12f09/image-20200815215109073.png 148w,\n/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/e4a3f/image-20200815215109073.png 295w,\n/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/526ee/image-20200815215109073.png 355w\"\n        sizes=\"(max-width: 355px) 100vw, 355px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td>여기서 Mask(opt.) 부분은 <br />Decoder의 Masked MultiHead Attention과 <br />다른 Masking이고 Optional 한 부분. <br />단순히 아래와 같은 코드를 통해 <br />입력 dimension(=512) 중 word가 <br />아닌 경우를 구분하는 역할</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>\n<p><code class=\"language-text\">Encoder</code> 기능 설명</p>\n<table>\n<thead>\n<tr>\n<th>이름</th>\n<th>기능</th>\n<th>설명</th>\n<th>매커니즘</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong><code class=\"language-text\">Positional Encoding</code></strong><img src=\"https://miro.medium.com/max/329/1*4HJt3iD5tbtf9wZFuDrM-Q.png\" alt=\"Image for post\"></td>\n<td>벡터로된 위치정보에다가 공식을 사용해서 <strong>'서로 간격이 일정하도록&#x26;고르게 분포 하도록 반영</strong>하면, 딥러닝이 이 정보를 분석해 어떤 pattern을 찾는다.<br />*<code class=\"language-text\">서로 간격이 일정하도록 고르게 분포</code> : 모든 벡터가 거리 값이 같다.(아래 추가 설명 참고)</td>\n<td>문장은 일반적인 임베딩과 <code class=\"language-text\">Positional Encoding</code>을 <br /><strong>더하여</strong> Encoder의 Layer로 들어간다</td>\n<td></td>\n</tr>\n<tr>\n<td><strong><code class=\"language-text\">Multi-Head Attention</code></strong><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 576px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 129.7297297297297%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAaCAYAAAC3g3x9AAAACXBIWXMAAAsSAAALEgHS3X78AAAD3ElEQVRIx51V+VMaSRTmD93aX/cf2fxiKmajkngsKIRzOAdQjmGAUYZR7sNwlCbgERCMKUtBxYDf9rQOCmKS2q561T1vXn/9vff6vVZhyri7uxutM5nMVP1LQzUJpGwqVypIppJwul1IZdKoVKu/Baqaxuz29hYhPoxEegdichv8Vgy+wMb/A7y7X4DjObDrHpgZC2wuB4Jc8JkXvwScZDAcAhwXJYyHvx3HqS5/+9aBGPehkI+AC5lRLESwLfnx/fvZL0FVT90YDgdUWSqlUCmtoLq7ir2KHuWSFonNN2jUqw/MB6M9T0Mgz6ppMTk+akCI6pCIm+DzqBEKrCAU/BedTvNFZgqGSlnc3Nyg1+sS6eHq+hoXF130+z9gMJggSTu4vOzhmugVG0W63S7VjxjKi/39fXg9QfBcHFxw80EExCIS3s4ugHX5EQmLT/5tEsYCnaO8hHr98BGwUqlhblaHlfcuLM7b8GGOobMsC29NmJ1Zg/qdBep/LFhasFOR/2kW3ffzEoNctoj+bf/e5YSYRIAtIhVvgrVtw6ANQrvkBWMUwBhiVFwWEU5LHHqNH8bVENyMBCl2gDj/BW9mlqHRrOGkfUILQiUlUuB8ZaTjLexsfoXIN5CIHmDDnSMAQQJEwMwiAdnGJrdPQOqQoofIiG3wG1X8+cdfyGazJMaXOD8/h6pU/ETYhOFxxGHWB8Hat7DB7sBuiUCv9WD5PQPtshNG3Tq8ThE+VwJOawx2cxRrKyzWff7xezgY/EDvqou4uIVXr/5GabdIgvwZzdZXmtHXr2fgJ3Vcr3+h+uvrHgLke27+HQqFHHVTFuX60Sy32x24WBYa7SoKpSJqe3s4OjpCLpfDvFqNqCBgt1xG4+CAXO4GrAwDk8VCbWVptVqPWZY3GswmaPU6GK1mmBgrXB4WdqcTGt0aVj/qYXXYYbYxCIRCWNKsQGc0EDsLjETWA340Go1HwFQ6DSmTRKqQhZROghMimFuYp+0ru1ugLSyR2oHO8BF2hwNhIYp8uUTbmje4AcZmQ/ey+wgo0xXiW2C9JAHk9KgQI7GqI5vPIbYl4MPyItyEcSabQbPZBB+NUFvNmhbhCD9ydzgcjnebYrFIqsXzrEZXtVpShhdjOlEUwYf5Z3VMGSrI8un1h1gMBoPRifl8HkO5ExGR9fLYI0lrt9tjzEZZVtCPj4+poWKk6JPJ5AhI2Vyr1egtmGQ31m1OT0/HjBR9lTxOk4CHh4fUflqzHQHKLpydjXfkq6sr2okmYyUDyiH6KaAceLkenxr1+32cnJw8c6vT6bzMcFI5ufln+mlvy39rL2AuubDZywAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"image-20200822105845609\" title=\"image-20200822105845609\" src=\"/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png\" srcset=\"/static/c924934dafc5a3ca5fd9cd80c3820dd0/12f09/image-20200822105845609.png 148w,\n/static/c924934dafc5a3ca5fd9cd80c3820dd0/e4a3f/image-20200822105845609.png 295w,\n/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png 576w\" sizes=\"(max-width: 576px) 100vw, 576px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span></td>\n<td>1문장을 여러 head로 Self-Attention 시킴<br /></td>\n<td>”Je suis étudiant”라는 문장의 임베딩 벡터가 512차원(<code class=\"language-text\">d_model</code>)이라면 8개 <code class=\"language-text\">head</code>로 나눠 64개의 벡터를 한 <code class=\"language-text\">Scaled Dot Attention</code>이 맡아 처리하는 것. <br /><strong>이는 동일한 문장도 8명(8 heads)이 각각의 관점에서 보고 추후에 합치는 과정</strong>이라고도 볼 수 있다.<br />*<code class=\"language-text\">d_model</code>은 임베딩을 하기 위한 차원으로 보통 512를 사용하고, <code class=\"language-text\">d_k</code>와 <code class=\"language-text\">d_v</code>는 64를 사용. <br /></td>\n<td><img src=\"https://miro.medium.com/max/2015/1*lsMX7fYhk55VCavim7GiMg.png\" alt=\"Image for post\"><br />동일한 [batch<em>size x len</em>q x d_model] shape의 동일한 <code class=\"language-text\">Q, K, V</code>를 만들어 [d<em>model, d</em>model] <code class=\"language-text\">linear</code>를 곱한 후, <br /><strong>임베딩 차원을 8등분하여 <code class=\"language-text\">Scaled Dot Product Atttention</code>으로 넘겨준다.<br /></strong>그리고 multi-head attention 그림에서의 h는 <code class=\"language-text\">n_heads</code>(number of head)를 나타내며 보통 8을 사용. 이는 64 * 8 = 512이기 때문.</td>\n</tr>\n<tr>\n<td><strong><code class=\"language-text\">Scaled-Dot Product Attention</code></strong></td>\n<td><code class=\"language-text\">Self-Attention</code>이 일어나는 부분.<br /></td>\n<td>- 한 <code class=\"language-text\">head</code>당 <br />(input 값) : Q(64), K(64), V(64)씩 가져가게 되는데, dimension(=512) 중 word가 아닌 경우를 구분하는 역할.<br />- 처음의 Q를 ResNet의 <code class=\"language-text\">Residual Shortcut</code>와 같은 컨셉으로 더해준다. 이는 층이 깊어지는 것을 더 잘 학습 시키기 위함</td>\n<td><img src=\"https://miro.medium.com/max/1433/1*Mj7Bdi0xVPPIqGbfu7cbSw.png\" alt=\"Image for post\"><br /><em><code class=\"language-text\">Self-Attention</code>></em><br />1. Q와 transposed K 사이를 내적하여 어텐션을 Softmax를 통해 구하고,<br/>2. 그 후에 V를 내적하여 중요한 부분(Attention)을 더 살린다.<br />3. 이렇게 8개의 head(여러 관점)으로 본 것을 다시 <code class=\"language-text\">concate</code>하고<br />4. <code class=\"language-text\">PoswiseFeedForwardNet</code>로 보냄</td>\n</tr>\n<tr>\n<td><strong><code class=\"language-text\">PoswiseFeedForwardNet</code></strong></td>\n<td>각 <code class=\"language-text\">head</code>가 만들어낸 <code class=\"language-text\">Self-Attention</code>을 치우치지 않게 <strong>균등하게 섞는 역할</strong></td>\n<td>FNN</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p> 출처: <a href=\"https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">platfarm tech team</a></p>\n</blockquote>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>추가 설명</p>\n<ul>\n<li><code class=\"language-text\">positional encoding</code>:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">PE <span class=\"token operator\">=</span> positional_encoding<span class=\"token punctuation\">(</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># emb_size = dmodel = 2차원으로 표현 </span></code></pre></div>\n<ul>\n<li>\n<blockquote>\n<p>(5,6) 값을 변경해도 발산하지 않고 뱅글뱅글 돌면서 각 벡터들의 간격이 일정하게 유지된다.</p>\n<table>\n<thead>\n<tr>\n<th>positional_encoding(50,2)</th>\n<th>positional_encoding(50,5)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 498px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7d4dbcc8bf619669b1221fe18db80425/79e1b/image-20200813105755022.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 93.24324324324323%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAACgklEQVQ4y5VUaW/TQBDN//8RSHxAiELDIRVUVQhVqqqWpk2htIVSSGLn9n2u78fMOnYdp0Ew0mht7+zzm9k303FdF0EQIAzDNY8jgTSOkNDq+QFcL5DvWRLJvWasEAKMk2UZOrqu4zEz/BjnIwuHdxreX83QvZjg4GaBvmLBCpON+DiOkaYpOr7vI89zFEUhN5Isx8kvHd1zFYc/lrie2BjqvvS+YuLgeo5d2rsk4NLKc1EUlYCmaRBg+VEkGbGYE6MprCDGNlu4ghirEjwlMvIspS0BXc+TDNk+3s4lYGU5sV7zvFzZMnp+2x9j/9tsnWFVw6Ee4GVPlYEVmEyI1qZXYFVGT45+QzVD5GmChAG9FcN3lxPcTJ21A02Q9nsVc/RTwyeqdUGAZcq2Bc2L8OxkJP/YZtW29t7CEXjTn8BwfBQ5yUYEPgaaRx/HG2lus+Z+SqroXowx0Z0S0LFMKYm9r9P1VIF/AsyoXLsEqGorwJAY6p6QDOM0/2+GLC8GXFpeCWgRwyTN8Jo+KkawccOPgcmYVTZ8kXyhrEPZetyDFIXPAwMfrmY1YPEX2TR/tHOm4JTOUpMjSViHhiE3Qrrh56ejuqWKlZDbgFWdK8m86ilI6FscRw+9nGalXO7mLp4eD9AbmltTrez4XpNlqlq0bj3TNEs2DV1x8/MlfZ85sMO4BuPnL6pFrFSpCsOP6vEQReKBYdsyiridudin5mfRvjhTpe8QENf5fulvyKoeX8vlsh6Sggcmr+RFGtPgDTE3XSna27EORbPlkM1pT6zi5DAmMNu2S0BG1zQNzNSyLHBvG3RRjuPKoMD34Do2QlrZHceh6eyBS8VgvDIwY7D9ASBwtnbJgcnKAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200813105755022\"\n        title=\"image-20200813105755022\"\n        src=\"/static/7d4dbcc8bf619669b1221fe18db80425/79e1b/image-20200813105755022.png\"\n        srcset=\"/static/7d4dbcc8bf619669b1221fe18db80425/12f09/image-20200813105755022.png 148w,\n/static/7d4dbcc8bf619669b1221fe18db80425/e4a3f/image-20200813105755022.png 295w,\n/static/7d4dbcc8bf619669b1221fe18db80425/79e1b/image-20200813105755022.png 498w\"\n        sizes=\"(max-width: 498px) 100vw, 498px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></td>\n<td><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 498px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/eb3b62e262ed51728b09ac5cfe9f457a/79e1b/image-20200813145235361.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 93.24324324324323%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAAC+ElEQVQ4y11UCW7cMBDL///WAkmTNmiaY7O3L8mSrMPHTknZCooKELxeazgccjR31hixbhA3DJJikBiCTCnmPaYg8/bsrRPvvcxjlBC8KONkxPmA/+zgxQBnmia569pW/DjL08lIbZOYOMm+8/KJ/dEM8nwx8lYPclRBXmuXv+2Vl8pEeamcPJ+N7Fov45hWQOucyO2GAC9/KosDvdzvlVz6ID5NeZ/wbd8N0rkkjY3yeNDygDPapxz3faekA+NlnuWuBUMgig1TBqoRwNX7UXaNQ9Ao07xIRBWf7SAvV5tBuQn2ChKs7NwZuS0AZO0EbJCdHwkUEHw1QdSQMqBPK9gB5bo45XeeYTXclGMuJTdNkxm9Qg8e5iIzAnORxeNRZ0Zl2TDmalgVpaHOnmYS0FkrBlr8PPUSp1kGaMbs3B8APmkPiW85GTdBdmDLxQqY/AqDLl8layVXZDmAyRnBFJ6LjN7hKoPKIttvH53My+0LUA8plxxj3FwGwwQ2780abFAOA1kK1whDCE6X+eQ7z5z1+r11MbfYfxreMgDLYRB7q7DgovNP0JFOF2ZvYM93JqehUwEMuCEXlMrMvyFuYaY3t8mGunKR1Wk7y8WzdL8F4Jg2QNV1ORNbgtmKuwuMoPjdpiFbpQc42ZVuYNIHNLnCM6V/NFyWRX6hn8iG2RlEYLprEUyzCkgG30plk/Mcr6G2A3Dgcl2vfZjZgDoB2EIE4A2hriyz9OECbanpW20zEMs96tXlOfch7vINDNlL538C+WSSIgFNotNs8tJKvE0VNhmzsWfe5a5rs4YvV5M/7gHCvizA1K7cWZowL2sbUXP+5vB4PPZSKbuWzHnGoPsDp4yGlgZTx+VR9QMHObqeLzazZRUcaSyVRpxQKnuwHWjK5nJdVXJpe2l7TBYMUWoRQN84n/+L29B1GKIUvtY2XzMO5RHOVspIQIzp9QqYux3N7dGPGteQrnOkWWvyIU4jpVR+8t47/D9Ad6W6PME1vpFABWJcfwENdrYAv52GawAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20200813145235361\"\n        title=\"image-20200813145235361\"\n        src=\"/static/eb3b62e262ed51728b09ac5cfe9f457a/79e1b/image-20200813145235361.png\"\n        srcset=\"/static/eb3b62e262ed51728b09ac5cfe9f457a/12f09/image-20200813145235361.png 148w,\n/static/eb3b62e262ed51728b09ac5cfe9f457a/e4a3f/image-20200813145235361.png 295w,\n/static/eb3b62e262ed51728b09ac5cfe9f457a/79e1b/image-20200813145235361.png 498w\"\n        sizes=\"(max-width: 498px) 100vw, 498px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<br>\n<br>\n<h3 id=\"decoder\" style=\"position:relative;\"><a href=\"#decoder\" aria-label=\"decoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decoder</h3>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>설명</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><br /><img src=\"https://miro.medium.com/max/440/1*q8Q-3t9ej6SPrCwTN4wmrA.png\" alt=\"Image for post\"></td>\n<td>인코더와 동일하지만, <code class=\"language-text\">Self-Attention</code>시, <br /><strong><code class=\"language-text\">&#39;Masked&#39;-Multi-Head Attention</code></strong>을 쓴다는 점이 다르다.<br />* <strong>Masked</strong>를 쓰는 이유: Self-Attention시 자신의 time step 이후의 word는 가려서 Self-Attention 되는 것을 막는 역할.</td>\n<td></td>\n</tr>\n<tr>\n<td><img src=\"https://miro.medium.com/max/440/1*64GKtTstp4zi3CEj5Y91iw.png\" alt=\"Image for post\"></td>\n<td>노란색 Box에서 왼쪽 2개 화살표가 Encoder의 K,V <br />오른쪽 화살표가 Self-Attention을 거친 Decoder의 Q</td>\n<td><img src=\"https://miro.medium.com/max/779/1*y_oOzc5s7I6urwrXiIcQAA.png\" alt=\"Image for post\"><br />이렇게 나온 값을 일반적인Teacher Forcing을 통해 학습</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<ul>\n<li>출처 및 참고: <a href=\"https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">platfarm tech team</a></li>\n</ul>\n</blockquote>\n<br>\n<br>\n<h3 id=\"inference추론\" style=\"position:relative;\"><a href=\"#inference%EC%B6%94%EB%A1%A0\" aria-label=\"inference추론 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Inference(추론)</h3>\n<table>\n<thead>\n<tr>\n<th>전체 Inference</th>\n<th>Encoder Inference</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Inference시 Encoder의 들어오는 문장(번역할 문장)은 정확히 알지만, Decoder에 들어오는 문장(번역되어지는 문장)은 알지 못한다. <br />시작 표시를 나타내는 &#x3C;S>를 사용해서 Seq2Seq와 동일하게 Inference 한다.</td>\n<td>inference word가 &#x3C;E> (end point)이면<br />inference를 멈춘다.</td>\n</tr>\n<tr>\n<td><img src=\"https://miro.medium.com/max/946/1*JMTk-0Ky6fZh2RNFd8mghQ.png\" alt=\"Image for post\"></td>\n<td><img src=\"https://miro.medium.com/max/1554/1*mmNgbfouHqgTQDY1vEAYNw.png\" alt=\"Image for post\"></td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<ul>\n<li>출처: <a href=\"https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">platfarm tech team</a></li>\n</ul>\n</blockquote>\n<br>\n<br>\n<h3 id=\"chatbot-code\" style=\"position:relative;\"><a href=\"#chatbot-code\" aria-label=\"chatbot code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chatbot Code</h3>\n<ul>\n<li>\n<p>(2020-08-14) (data set)를 transformer 모델에 학습한 챗봇 만들기 미니 프로젝트</p>\n<p>Created by jynee &#x26; <a href=\"https://github.com/molo6379\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">molo6379</a> &#x26; <a href=\"https://github.com/hayjee\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">hh</a> &#x26; <a href=\"https://github.com/Jude0124\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Dabi</a> </p>\n<p><a href=\"https://github.com/CyberZHG/keras-transformer\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Zhao HG</a> keras transformer 코드 응용</p>\n<br>\n</li>\n<li>데이터 입력</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> keras_transformer <span class=\"token keyword\">import</span> get_model<span class=\"token punctuation\">,</span> decode\n<span class=\"token keyword\">import</span> pickle\n<span class=\"token keyword\">import</span> warnings\nwarnings<span class=\"token punctuation\">.</span>filterwarnings<span class=\"token punctuation\">(</span><span class=\"token string\">'ignore'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'tensorflow'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 단어 목록 dict를 읽어온다.</span>\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'./dataset/6-1.vocabulary.pickle'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'rb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    word2idx<span class=\"token punctuation\">,</span>  idx2word <span class=\"token operator\">=</span> pickle<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">)</span>\n    \n<span class=\"token comment\"># 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 읽어온다.</span>\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'./dataset/6-1.train_data.pickle'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'rb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    trainXE<span class=\"token punctuation\">,</span> trainXD<span class=\"token punctuation\">,</span> trainYD <span class=\"token operator\">=</span> pickle<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">)</span>\n\t\n<span class=\"token comment\"># 평가 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.</span>\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'./dataset/6-1.eval_data.pickle'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'rb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    testXE<span class=\"token punctuation\">,</span> testXD<span class=\"token punctuation\">,</span> testYD <span class=\"token operator\">=</span> pickle<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<ul>\n<li>model 빌드</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> get_model<span class=\"token punctuation\">(</span>\n    token_num<span class=\"token operator\">=</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>word2idx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>word2idx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    embed_dim<span class=\"token operator\">=</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span>\n    encoder_num<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span>\n    decoder_num<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span>\n    head_num<span class=\"token operator\">=</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span>\n    hidden_dim<span class=\"token operator\">=</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span>\n    dropout_rate<span class=\"token operator\">=</span><span class=\"token number\">0.05</span><span class=\"token punctuation\">,</span>\n    use_same_embed<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># Use different embeddings for different languages</span>\n<span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span><span class=\"token string\">'adam'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'sparse_categorical_crossentropy'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<ul>\n<li>model load or fit(학습)</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">LOAD_MODEL <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n<span class=\"token keyword\">if</span> LOAD_MODEL<span class=\"token punctuation\">:</span>\n    MODEL_PATH <span class=\"token operator\">=</span> <span class=\"token string\">'./dataset/transformer.h5'</span>\n    model<span class=\"token punctuation\">.</span>load_weights<span class=\"token punctuation\">(</span>MODEL_PATH<span class=\"token punctuation\">)</span>\n    \n<span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n    model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>\n    x<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>trainXE<span class=\"token punctuation\">,</span> trainXD<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    y<span class=\"token operator\">=</span>trainYD<span class=\"token punctuation\">,</span>\n    epochs<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n    batch_size<span class=\"token operator\">=</span><span class=\"token number\">32</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>save_weights<span class=\"token punctuation\">(</span>MODEL_PATH<span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<ul>\n<li>predict 함수 정의</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">ivec_to_word</span><span class=\"token punctuation\">(</span>q_idx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    decoded <span class=\"token operator\">=</span> decode<span class=\"token punctuation\">(</span>\n        model<span class=\"token punctuation\">,</span>\n        q_idx<span class=\"token punctuation\">,</span>\n        start_token<span class=\"token operator\">=</span>word2idx<span class=\"token punctuation\">[</span><span class=\"token string\">'&lt;START>'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        end_token<span class=\"token operator\">=</span>word2idx<span class=\"token punctuation\">[</span><span class=\"token string\">'&lt;END>'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        pad_token<span class=\"token operator\">=</span>word2idx<span class=\"token punctuation\">[</span><span class=\"token string\">'&lt;PADDING>'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n    decoded <span class=\"token operator\">=</span> <span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> x<span class=\"token punctuation\">:</span> idx2word<span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> decoded<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> decoded</code></pre></div>\n<br>\n<ul>\n<li>chatbot </li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">MAX_SEQUENCE_LEN <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">chatting</span><span class=\"token punctuation\">(</span>n<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        question <span class=\"token operator\">=</span> <span class=\"token builtin\">input</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Q: '</span><span class=\"token punctuation\">)</span>\n        \n        <span class=\"token keyword\">if</span>  question <span class=\"token operator\">==</span> <span class=\"token string\">'quit'</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">break</span>\n        \n        q_idx <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> question<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">' '</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> x <span class=\"token keyword\">in</span> word2idx<span class=\"token punctuation\">:</span>\n                q_idx<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>word2idx<span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n                q_idx<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>word2idx<span class=\"token punctuation\">[</span><span class=\"token string\">'&lt;UNKNOWN>'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># out-of-vocabulary (OOV)</span>\n        \n        <span class=\"token comment\"># &lt;PADDING>을 삽입한다.</span>\n        <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>q_idx<span class=\"token punctuation\">)</span> <span class=\"token operator\">&lt;</span> MAX_SEQUENCE_LEN<span class=\"token punctuation\">:</span>\n            q_idx<span class=\"token punctuation\">.</span>extend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>word2idx<span class=\"token punctuation\">[</span><span class=\"token string\">'&lt;PADDING>'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span>MAX_SEQUENCE_LEN <span class=\"token operator\">-</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>q_idx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            q_idx <span class=\"token operator\">=</span> q_idx<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">:</span>MAX_SEQUENCE_LEN<span class=\"token punctuation\">]</span>\n        \n        answer <span class=\"token operator\">=</span> ivec_to_word<span class=\"token punctuation\">(</span>q_idx<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'A: '</span><span class=\"token punctuation\">,</span> answer<span class=\"token punctuation\">)</span>\n\nchatting<span class=\"token punctuation\">(</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<br>\n<br>\n<br>\n<h2 id=\"units-of-words\" style=\"position:relative;\"><a href=\"#units-of-words\" aria-label=\"units of words permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>units of words</h2>\n<ul>\n<li>\n<p>등장 배경:</p>\n<ul>\n<li>FastText를 사용한다면,</li>\n<li>\n<p>n-gram character 방식<br></p>\n<p><em>단점 ></em></p>\n<ul>\n<li>word2vec의 oov 문제를 해결하지만, <strong>collision 문제 발생</strong><br></li>\n</ul>\n</li>\n<li>hash 사용한다면, hash-table 크기를 작게 잡으면 collision 발생</li>\n</ul>\n<blockquote>\n<p>참고: DB에서는,</p>\n<ol>\n<li>Collision 문제를 해결하기 위해서 <strong>별도의 overflow page를 사용</strong>한다</li>\n<li>overflow page가 늘어나면 검색 속도가 떨어진다</li>\n<li>\n<p>overflow paee가 임계치를 초과하면 <strong>hash-table을 늘려서</strong> DB를 재구성</p>\n<p>=> \"Reorganization\" 따라서 기존의 <strong>단어들의 위치가 변경</strong>된다.</p>\n<ul>\n<li>NLP 분석 시엔 Reorganization하면 단어들의 위치가 변경되므로 다시 학습시켜야 한다. 따라서 근본적인 해결책이 되진 못한다.<br></li>\n</ul>\n</li>\n</ol>\n</blockquote>\n<br>\n</li>\n<li>\n<p><code class=\"language-text\">Word Piece(WPM)</code>, <strong><code class=\"language-text\">Sentence Piece(SPM)</code></strong></p>\n<ul>\n<li>NLP에서 Sub-word 구축할 때 사용</li>\n<li>형태소 분석과 달리 언어의 특성에 구애 받지 않아 아무 언어나(한글/영어 등) 사용이 가능하다.</li>\n<li>빈도가 높은 문자열들은 하나의 <code class=\"language-text\">unit</code>으로 취급하여 사전에 등록해 사용한다.</li>\n<li>'unit': 의미를 가진 문자는 아니고 음절이라기엔 두 음절을 하나로 보니 unit이란 이름을 사용한다.</li>\n<li>\n<blockquote>\n<p>구글은 자신들의 구글 번역기에서 WPM이 어떻게 수행되는지에 대해서 기술했다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">WPM을 수행하기 이전의 문장<span class=\"token punctuation\">:</span>\nJet makers feud over seat width <span class=\"token keyword\">with</span> big orders at stake\nWPM을 수행한 결과<span class=\"token punctuation\">(</span>wordpieces<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">:</span>\n_J et _makers _fe ud _over _seat _width _with _big _orders _at _stake</code></pre></div>\n<p>Jet는 J와 et로 나누어졌으며, feud는 fe와 ud로 나누어진 것을 볼 수 있다. WPM은 입력 문장에서 기존에 존재하던 띄어쓰기는 언더바로 치환하고, 단어는 내부단어(subword)로 통계에 기반하여 띄어쓰기로 분리한다.</p>\n<p>기존의 띄어쓰기를 언더바로 치환하는 이유는 <strong>차후 다시 문장 복원을 위한 장치</strong>이다. WPM의 결과로 나온 문장을 보면, 기존에 없던 띄어쓰기가 추가되어 내부 단어(subwords)들을 구분하는 구분자 역할을 하고 있으므로 본래의 띄어쓰기를 언더바로 치환해놓지 않으면, 기존 문장으로 복원할 수가 없다.</p>\n<p>WPM이 수행된 결과로부터 다시 수행 전의 결과로 돌리는 방법은 현재 있는 띄어쓰기를 전부 삭제하여 내부 단어들을 다시 하나의 단어로 연결시키고, 언더바를 다시 띄어쓰기로 바꾸면 된다.</p>\n<ul>\n<li>출처: <a href=\"https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-5%EC%9D%BC%EC%B0%A8-%EB%8B%A8%EC%96%B4-%EB%B6%84%EB%A6%AC-60b59f681eb7\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">정민수</a></li>\n</ul>\n</blockquote>\n</li>\n<li>pre-trained 된 <a href=\"https://github.com/SKTBrain/KoBERT\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">SKTBrain의 KoBert</a>를  fine tuning으로 사용하면 쉽게 SentencePiece를 만들 수 있다.</li>\n</ul>\n<br>\n<ul>\n<li>WPM, SPM</li>\n<li>SPM은 GOOGLE이 C++로 만들어서 속도가 빠르다.</li>\n<li>\n<p>(단점) 조사도 쪼개서 보기 때문에 챗봇 만들 때 답변으로 잘못된 조사가 붙여 나올 수 있다.</p>\n<ul>\n<li>ex: 번역 api 사용 시 부자연스러운 조사</li>\n<li>개선 방법: (1) 데이터 양을 늘린다. (2) BERT를 사용한다.</li>\n</ul>\n<blockquote>\n<p>[나는 학교에 간다]</p>\n<ol>\n<li>단어와 단어를 구분하는 단어의 띄어쓰기에만 언더바 붙이기 \"나는_학교에_간다\"</li>\n<li>'_학교' → 사전에 등록</li>\n<li>'_나' → 사전에 등록</li>\n<li>'는' → 사전에 등록</li>\n<li>'간다' → 사전에 등록</li>\n<li>['_학교', '_나', '는', '간다']</li>\n<li>[5, 4, 61,12]</li>\n</ol>\n</blockquote>\n</li>\n</ul>\n<br>\n</li>\n<li>\n<p><code class=\"language-text\">BPE</code></p>\n<ul>\n<li>데이터 압축기술.</li>\n</ul>\n<blockquote>\n<p>구글의 WPM에는 BPE(Byte Pair Encoding) 알고리즘이 사용되었다.</p>\n<p>BPE 알고리즘의 기본 원리는 가장 많이 등장한 문자열에 대하여 병합하는 작업을 반복하는 데, 원하는 단어 집합의 크기. 즉, 단어의 갯수가 될 때까지 이 작업을 반복한다.</p>\n<ul>\n<li>출처: <a href=\"https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-5%EC%9D%BC%EC%B0%A8-%EB%8B%A8%EC%96%B4-%EB%B6%84%EB%A6%AC-60b59f681eb7\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">정민수</a></li>\n</ul>\n</blockquote>\n<br>\n<blockquote>\n<p> 예> </p>\n<p> [abcabtabsta]</p>\n<ol>\n<li>'ab' → 사전에 등록</li>\n<li>'abc' → 사전에 등록</li>\n<li>'r' → 사전에 등록</li>\n<li>'t' → 사전에 등록</li>\n<li>'u' → 사전에 등록</li>\n</ol>\n</blockquote>\n</li>\n</ul>\n<br>\n<p><br><br></p>\n<br>\n<ul>\n<li>\n<p>참고:</p>\n<blockquote>\n<ul>\n<li>아마추어 퀀트, blog.naver.com/chunjein</li>\n<li>platfarm tech team. 2019.05.11. \"어텐션 메커니즘과 transfomer(self-attention)\". <a href=\"https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://medium.com/platfarm/어텐션-메커니즘과-transfomer-self-attention-842498fd3225</a></li>\n<li>Zhao HG. \"keras-bert\". <a href=\"https://github.com/CyberZHG/keras-bert\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/CyberZHG/keras-bert</a></li>\n<li>정민수. 2019.06.07. \"자연어처리(NLP) 5일차 (단어 분리)\". <a href=\"https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-5%EC%9D%BC%EC%B0%A8-%EB%8B%A8%EC%96%B4-%EB%B6%84%EB%A6%AC-60b59f681eb7\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-5%EC%9D%BC%EC%B0%A8-%EB%8B%A8%EC%96%B4-%EB%B6%84%EB%A6%AC-60b59f681eb7</a>. @omicro03</li>\n</ul>\n</blockquote>\n</li>\n</ul>","excerpt":"NLP  CNN, RNN 대신 Self-Attention을 사용하는 모델 Transformer는 RNN, LSTM없이 time 시퀀스 역할을 하는 모델입니다. RNN, LSTM 셀을 일체 사용하지 않았으나, 자체만으로 time…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/NLP_Transfomer/#nlp\">NLP</a></p>\n<ul>\n<li>\n<p><a href=\"/NLP_Transfomer/#code-classlanguage-texttransformercode\"><code class=\"language-text\">Transformer</code></a></p>\n<ul>\n<li><a href=\"/NLP_Transfomer/#encoder\">Encoder</a></li>\n<li><a href=\"/NLP_Transfomer/#decoder\">Decoder</a></li>\n<li><a href=\"/NLP_Transfomer/#inference%EC%B6%94%EB%A1%A0\">Inference(추론)</a></li>\n<li><a href=\"/NLP_Transfomer/#chatbot-code\">Chatbot Code</a></li>\n</ul>\n</li>\n<li><a href=\"/NLP_Transfomer/#units-of-words\">units of words</a></li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/NLP_Transfomer/"},"frontmatter":{"title":"NLP Transformer & SentecePiece","date":"Aug 15, 2020","tags":["NLP","transformer","SentencePiece"],"keywords":["JyneeEarth","jynee"],"update":"Aug 20, 2020"}}},"pageContext":{"slug":"/NLP_Transfomer/","series":[],"lastmod":"2020-08-20"}},"staticQueryHashes":["3649515864","694178885"]}