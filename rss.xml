<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[JyneeEarth | Feed]]></title><description><![CDATA[jynee's lab]]></description><link>https://jynee.github.io/tags#1st</link><generator>GatsbyJS</generator><lastBuildDate>Tue, 25 Aug 2020 01:06:36 GMT</lastBuildDate><item><title><![CDATA[NLP BERT]]></title><description><![CDATA[NLP  Bidirectional Encoder Representations Form Transformer transformer의 encoder만 사용한다.  Pre-traning과 fine-tuning으로 활용한다. 즉, transformer…]]></description><link>https://jynee.github.io/tags#1st/NLP_BERT_1/</link><guid isPermaLink="false">https://jynee.github.io/tags#1st/NLP_BERT_1/</guid><pubDate>Tue, 18 Aug 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;nlp&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#nlp&quot; aria-label=&quot;nlp permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP&lt;/h1&gt;
&lt;h2 id=&quot;code-classlanguage-textbertcode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textbertcode&quot; aria-label=&quot;code classlanguage textbertcode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;BERT&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bidirectional Encoder Representations Form Transformer&lt;/li&gt;
&lt;li&gt;transformer의 encoder만 사용한다. &lt;/li&gt;
&lt;li&gt;Pre-traning과 fine-tuning으로 활용한다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;즉, transformer의 encoder을 사전학습(UL) 시킨 언어 모델로, AE처럼 목적에 맞게 활용할 수 있도록 해둔 것이다. &lt;strong&gt;특정 과제의 성능을 더 좋게 할 수 있는 언어 모델이다.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;transformer의 encoder 출력 = BERT의 출력&lt;/li&gt;
&lt;li&gt;transformer의 encoder 부분만 Pre-traning 하여 사용, &lt;strong&gt;실제 사용할 때는 W만 빼내는 fine-tuning 방법으로 응용&lt;/strong&gt;한다. &lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;BERT등장 이전에는 데이터의 전처리 임베딩을 Word2Vec, GloVe, Fasttext 방식을 많이 사용했지만, 요즘의 고성능을 내는 대부분의 모델에서 BERT를 많이 사용하고 있다고 합니다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;BERT는 이미 총3.3억 단어(BookCorpus + Wikipedia Data)의 거대한 코퍼스를 정제하고, 임베딩하여 학습시킨 모델이 있다. 따라서 새로운 단어를 추가하거나 기타 등등등의 이유로 필요할 때 BERT 기법을 적용하기 위해 작동 원리를 배운다. &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AR(Autoregressive) : 과거 데이터로부터 현재 데이터를 추정할 수 있다. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;참고: 기존의 ELMO나 GPT는 left to right or right to left Language Model을 사용하여 pre-training을 하지만, BERT는 이와 다르게 2가지의 새로운 unsupervised prediction task로 pre-training을 수행합니다.&lt;/p&gt;
&lt;p&gt;출처: &lt;a href=&quot;https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;mino-park7. &quot;BERT 논문정리&quot;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&quot;알고리즘-원리&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9B%90%EB%A6%AC&quot; aria-label=&quot;알고리즘 원리 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;알고리즘 원리&lt;/h3&gt;
&lt;h4 id=&quot;input-단계&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#input-%EB%8B%A8%EA%B3%84&quot; aria-label=&quot;input 단계 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Input 단계&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.kakaocdn.net/dn/WFCfe/btqBWZ40Gmc/6FkuwsAGN9e7Uudmi03k4k/img.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;br&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;BERT는 아래 세가지 임베딩을 합치고, Layer에 정규화와 Dropout을 적용하여 입력값으로 사용한다.&lt;/em&gt;
출처: &lt;a href=&quot;https://ebbnflow.tistory.com/151&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;ebb and flow&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Token Embedding&lt;/code&gt;&lt;/strong&gt; &gt;&lt;/p&gt;
&lt;p&gt;&quot;&lt;em&gt;[&lt;code class=&quot;language-text&quot;&gt;Word Piece&lt;/code&gt;]&lt;/em&gt;. 임베딩 방식 사용한다. 즉, 각 Char(문자) 단위로 임베딩한다.&quot;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SubWord&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 1. 자주 등장하면서 가장 긴 길이로 단어를 분리한다.&lt;/p&gt;
&lt;p&gt;Step 2. 자주 등장하지 않았던 단어도 분리한 후 &apos;OOV&apos;처리하여 모델링의 성능을 저하했던 &apos;OOV&apos;문제도 해결한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;두 문장이 들어왔다는 걸 알려준다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 3. 두 문장을 구분한단 의미에 구분자 [SEP]를 넣어 분리한다. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pre-trained 일 경우 input을 2개 문장씩 넣어주고, fine-tuning 시 분류할 목적이라면 input에 문장을 하나만 넣어준다.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;모든 sentence의 첫번째 token은 언제나 &lt;code class=&quot;language-text&quot;&gt;[CLS]&lt;/code&gt;(special classification token) 입니다. 이 &lt;code class=&quot;language-text&quot;&gt;[CLS]&lt;/code&gt; token은 transformer 전체층을 다 거치고 나면 token sequence의 결합된 의미를 가지게 되는데, 여기에 &lt;strong&gt;간단한 classifier를 붙이면 단일 문장, 또는 연속된 문장의 classification을 쉽게 할 수 있게 됩니다&lt;/strong&gt;. 만약 classification task가 아니라면 이 token은 무시하면 됩니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;출처: &lt;a href=&quot;https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;mino-park7. &quot;BERT 논문정리&quot;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Segment Embedding&lt;/code&gt;&lt;/strong&gt; &gt;&lt;/p&gt;
&lt;p&gt;&quot;&lt;em&gt;[&lt;code class=&quot;language-text&quot;&gt;Sentence Embedding&lt;/code&gt;]&lt;/em&gt;. 문장 순서 정보가 있다. 즉, 의미 있는 여러 subword(내부 단어)로 임베딩 한다.&quot;&lt;/p&gt;
&lt;p&gt;Step 1. 위 Step 3에서 구분한 구분자로 두 문장을 하나의 Segment로 지정하여 입력한다. 토큰 시킨 단어들을 다시 하나의 문장으로 만든다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;첫 번째 문장 속 단어는 전부 1로, 두 번째 문장 속 단어는 전부 2로 임베딩한다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;BERT에서는 이 한 세그먼트를 512 sub-word 길이로 제한하는데, 한국어는 보통 20 sub-word가 한 문장을 이룬다고 하며 대부분의 문장은 60 sub-word가 넘지 않는다고 하니 BERT를 사용할 때, 하나의 세그먼트에 128로 제한하여도 충분히 학습이 가능하다고 합니다.&lt;/p&gt;
&lt;p&gt;출처: &lt;a href=&quot;https://ebbnflow.tistory.com/151&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;ebb and flow&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Position Embedding&lt;/code&gt;&lt;/strong&gt; &gt;&lt;/p&gt;
&lt;p&gt;&quot;&lt;em&gt;[Position Embedding]&lt;/em&gt;. 단어 순서 정보&quot;&lt;/p&gt;
&lt;p&gt;Step 1. 순서정보를 입력하면 저장 + 공식에 맞춰 벡터화(수치화) 시킨다. Token 순서 대로 인코딩 한다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h4 id=&quot;pre-training-단계&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#pre-training-%EB%8B%A8%EA%B3%84&quot; aria-label=&quot;pre training 단계 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Training 단계&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;데이터들을 임베딩하여 훈련시킬 데이터를 모두 인코딩 하였으면, 사전훈련을 시킬 단계입니다. 기존의 방법들은 보통 문장을 왼쪽에서 오른쪽으로 학습하여 다음 단어를 예측하는 방식이거나, 예측할 단어의 좌우 문맥을 고려하여 예측하는 방식을 사용합니다.&lt;/p&gt;
&lt;p&gt;하지만 BERT는 언어의 특성을 잘 학습하도록,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;MLM(Masked Language Model)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;NSP(Next Sentence Prediction)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;위 두가지 방식을 사용합니다. &lt;/p&gt;
&lt;p&gt;논문에서는 기존(좌우로 학습하는 모델)방식과 비교하여 MLM방식이 더 좋은 성능을 보여주고 있다고 말합니다!&lt;/p&gt;
&lt;p&gt;출처: &lt;a href=&quot;https://ebbnflow.tistory.com/151&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;ebb and flow&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h5 id=&quot;task-1-mlm&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#task-1-mlm&quot; aria-label=&quot;task 1 mlm permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Task 1. MLM&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;MLM: Masked LM&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;양방향+단방향을 concatnate 한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이를 통하여 LM의 left-to-right을 통하여 문장 전체를 predict하는 방법론과는 달리, &lt;strong&gt;[MASK] token 만을 predict&lt;/strong&gt;하는 pre-training task를 수행.&lt;/li&gt;
&lt;li&gt;많은 training step이 필요하지만, 보통 LM보다 훨씬 빠르고 좋은 성능을 낸다. &lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&quot;denoising--mask&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#denoising--mask&quot; aria-label=&quot;denoising  mask permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Denoising ~ [mask]&lt;/h6&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;AE에서는 학습 시, 미리 잡음을 섞어 추후에 입력될지 모르는 잡음을 제거(Denoising)할 수 있도록 모델링을 해둔다. BERT에서도 [mask] 개념을 사용하여 Denoising AE와 같은 역할을 수행하도록 학습한다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I love you → I [mask] you&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[mask]: noise 처리된 단어를 [mask] 처리함&lt;/li&gt;
&lt;li&gt;이때 주변단어를 이용하여 [mask]된 단어가 나오도록 알아맞추게 학습한다.&lt;/li&gt;
&lt;li&gt;Pre-traning: [mask] 사용&lt;/li&gt;
&lt;li&gt;Fine-tuning: [mask] 안 사용&lt;/li&gt;
&lt;li&gt;해당 token을 맞추어 내는 task를 수행하면서, BERT는 문맥을 파악하는 능력이 생긴다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;80% 단어(token)을 [MASK]로 바꾼다. eg., my dog is hairy -&gt; my dog is [MASK]&lt;/li&gt;
&lt;li&gt;10% token을 random word로 바꾼다. eg., my dog is hariy -&gt; my dog is apple&lt;/li&gt;
&lt;li&gt;10%는 원래 token로 그대로 둔다. 이는 실제 관측된 단어를 bias해주기 위해 실시한다.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;TransFormer의 Encoder에서도 사용한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;h5 id=&quot;task-2-nsp&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#task-2-nsp&quot; aria-label=&quot;task 2 nsp permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Task 2. NSP&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;문장과 문장 간의 관계&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;두 문장 학습&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NSP: Next Sentence prediction&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;input 단계에서 붙여진 [CLS], [SEP]를 확인한다. 그리고 두 문장을 이어 붙이곤, 이게 원래의 corpus에서도 바로 이어 붙여져 있던 문장인지를 맞추는 binarized next sentence prediction task를 수행한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;연속된 2문장인 A문장과 B문장을 확인하고, A문장 뒤에 B문장이 이어서 나오는 구나,를 파악한다&lt;/li&gt;
&lt;li&gt;Special token: [CLS], [SEP]이 추가되어  있는 점이 Transformer와의 차이점이다.&lt;/li&gt;
&lt;li&gt;[CLS]: 문장 시작&lt;/li&gt;
&lt;li&gt;[SEP]: 단어 구분자&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;원리: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A 다음 B가 나오면 IsNEXT로 분류한다.&lt;/li&gt;
&lt;li&gt;A 다음 엉뚱하게 C가 나오면 NotNext로 분류한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&quot;주의&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EC%A3%BC%EC%9D%98&quot; aria-label=&quot;주의 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;주의&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Transformer에서는 만든 SentencePiece를 입력값에 넣을 수 있었지만, BERT는 안된다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input layer에서 자동으로 SentencePiece 처리를 해준다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code&lt;a href=&quot;https://github.com/CyberZHG/keras-bert&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Zhao HG keras-bert&lt;/a&gt; 中 &lt;strong&gt;.encode()&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;ids&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; segments &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;encode&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;text&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; max_len&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;SEQ_LEN&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Finetuning의 경우, pre-traing 당시 학습 안 된 단어는 OOV로 자동 처리한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&quot;code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code&quot; aria-label=&quot;code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CODE&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/SKTBrain/KoBERT&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;SKTBrain. &quot;KoBERT&quot;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/CyberZHG/keras-bert&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Zhao HG. &quot;keras-bert&quot;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&quot;활용&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%ED%99%9C%EC%9A%A9&quot; aria-label=&quot;활용 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;활용&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;BERT 활용 방법: 대량의 코퍼스를 BERT 모델에 넣어 학습하고, BERT 출력 부분에 추가적인 모델(RNN, CNN 등의 ML/DL 모델)을 쌓아 목적을 수행한다. &lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;BERT를 사용하지 않은 일반 모델링 &gt; &lt;/p&gt;
&lt;p&gt;분류를 원하는 데이터 → LSTM, CNN 등의 머신러닝 모델 → 분류&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT를 사용한 모델링 &gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;관련 대량 코퍼스 → BERT&lt;/strong&gt; →  분류를 원하는 데이터 → LSTM, CNN 등의 머신러닝 모델 → 분류&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;이때 DNN을 이용하였을 때와 CNN등 과 같은 복잡한 모델을 이용하였을 때의 성능 차이가 거의 없다고 알려져 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/c51fdaa01f671e9b5dcbcf982601927f/ace37/image-20200819161110447.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 97.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAAD+ElEQVQ4y02UW28aVxSF+dd9qVT1B1RVHlpVUdqnvlWJrcRxEmyDYYY7xsOYAWYAA2NuNtfhOly/7hljJ0c6gnP2Omufs/daEzgAZiZDp91hOJnRH45e52Dk+L/L5Qpv9Not9JjKaCr7g6cjZnrE9XGmUwLb3Z7QdZTPUYWI/shF+o5zVeGzImutzXksR9Xu+oT5UoWz0Dcu0yrB5DVRvUKi7KIaY74mVVK6IYSbDcmyg1raSHBJMN3jc7wt6zVJ0yVqLKl3JkK3x6iPiZZ2xMsbQprDJ6WJWnBIWS4RY4tWGRPYbLek8yYJzSQumS7jBUKxrGSrEC/0iWkNGq2Bf8NitU1Sr5IpVFGzOt/UAkpxSaI0E7yJbj54Tz6Qib8nFf6TVL5KOnlOTn1HJvKWRDZLXPmA/WD7hGUjQeLsJ/Twr+hXP6NlPskr1qSNNreXv1DQVQIesG6U0aNJxoMRlXwBI5bGiEujmg9UtALL2dwn7Lc6JD5KwmCI/GUEu2iykpI544mso348cDgcOGw3LCZjHntd5o7US9bubEpXujqTfTj4hIf9jp27YildHnQ7dFsSn83Yevj5TMq8f77hynUZjkb0ej3Gk4n/fzAcylMfmIoUXsZqtfJjo7Ekf3qiLVLrdrs4ztTH7V8Il8slup5H0zRMs0yz2aBpN2nYNs7sO6EriW3Z8+ZgOGC9XrNcuf75ldz8SPj8nO32wEoCh8PxtDRr8vTITMS7F62+Donv3A3TscPaXcvBtb+5XizxDvs1vDMtYrc33FpdUncWiTsNJRoicnrBxX8n1O6fu1yoVonGr1DDXwgHT8kINh0pkSu1CX95z73dIrCXQif1Gl9TZZSC6DDb5EzRCGYt4tac4O0DhdqzU1KFBhc3FpG7Olc5k7DWIV7xhL7gSmti1HpCuNugVacoJcQxK3HKIydhm5g4wtOYYqyx7KnvlJw1JVpEYge+JEd8CNWIGRNS5koctcVsTgh45cllgyRDb4jemCiJEMno3yQj/6Cm01xHTiiZRf+GuZsw2chvaIm/0NQ3JG9SXOY3KLf35JTfKVul5y5Xizrpy1Pu6y2KeY1c5Cs3Mq2yiZaKiR7bPuG9IZY8/QPj6l9uz99i3WVpDbc0Gi3SH9/J1+jhqMOJQ8+qMhd9DUVbo2aLQb0pXZZPVNNmK3LxZTOdC65B16pjFyycp4FfCncxp12usd9shdBzylERnlhXx8Pbw56uCH3uycEThuD2r5qCibil9/jIxHHY7b/Lyr/hYrGgXC5TFVl0Oh2eBNgTB3jCfSF7MUClUvGxpVKJWq3m4/v9/is28MLskXpz/0O2Vy17r/iBdD6f+x52j6/5Men/3V1r3iL9/hMAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200819161110447&quot;
        title=&quot;image-20200819161110447&quot;
        src=&quot;/static/c51fdaa01f671e9b5dcbcf982601927f/fcda8/image-20200819161110447.png&quot;
        srcset=&quot;/static/c51fdaa01f671e9b5dcbcf982601927f/12f09/image-20200819161110447.png 148w,
/static/c51fdaa01f671e9b5dcbcf982601927f/e4a3f/image-20200819161110447.png 295w,
/static/c51fdaa01f671e9b5dcbcf982601927f/fcda8/image-20200819161110447.png 590w,
/static/c51fdaa01f671e9b5dcbcf982601927f/ace37/image-20200819161110447.png 666w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;QQP&lt;/li&gt;
&lt;li&gt;Q&amp;#x26;A&lt;/li&gt;
&lt;li&gt;Classification&lt;/li&gt;
&lt;li&gt;Tagging&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;qqp-예시&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#qqp-%EC%98%88%EC%8B%9C&quot; aria-label=&quot;qqp 예시 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;QQP 예시&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;QQP : &lt;em&gt;&quot;Input된 Q1과 Q2가 얼마나 유사한가? 유사한 문장인가?&quot;&lt;/em&gt; 를 따지는 것.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;qa-예시&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#qa-%EC%98%88%EC%8B%9C&quot; aria-label=&quot;qa 예시 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;QA 예시&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;(챗봇) Qustion &amp;#x26; Answer에 활용한다.&lt;/li&gt;
&lt;li&gt;SQuAD 모델: &amp;#x3C;START&gt;와 &amp;#x3C;END&gt;가 나오도록 하는 모델&lt;/li&gt;
&lt;li&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;설명&lt;/th&gt;
&lt;th&gt;알고리즘 설명&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;지문이 주어지고, Where ~? 라는 질문을 받으면 Seoul Korea라고 y값을 내놓을 수 있게 지문의 위치를 strat = 13이고 end =14라고 학습한다.&lt;/td&gt;
&lt;td&gt;[CLS] Question [SEP] 지문 [SEP]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;질문에 답을 달아놓을 수 있게, 지문의 몇 번째 있는 단어가 정답인지에 대해 학습을 한다.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;stance-classification분류&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#stance-classification%EB%B6%84%EB%A5%98&quot; aria-label=&quot;stance classification분류 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stance Classification(분류)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;일반 LSTM은 문장 내에서 단어들의 흐름을 보지만, Stance classification은 여러 문장들의 서로의 &lt;strong&gt;관련성&lt;/strong&gt;을 보고 classification를 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h4 id=&quot;code--modelsummary&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code--modelsummary&quot; aria-label=&quot;code  modelsummary permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;code ~ model.summary()&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/CyberZHG/keras-bert&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Zhao HG&lt;/a&gt; 의 keras_BERT code 를 활용하였다.&lt;/li&gt;
&lt;li&gt;Pre_trained data는 &lt;a href=&quot;https://github.com/google-research/bert&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Google Research&lt;/a&gt;를 활용하였다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/a6942057a32f062b1491650a4ac02b48/c1b63/image-20200818172004055.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAABvUlEQVQoz3VS25LSUBDkI9S9IEIIEELI/ULIsiRZXdYqS33y/z+l7Z4Qqyj1oSuZSU5fZs5kmz3BSxsExTP87ASrk6NhGZQGN6yQHF+QNp9Rtm8ozxf4+Ql3ToAHN7zBJKp7hFVrB6JDh7HW+4akPgVW0YGCZ+vH/J4cexO5X+5vyFRPpOQlg0NzyXpLx+o5uwLTVQSHLvfl+UbYCQp8WOzM5b1wJZ+EVWckciAiOdqxVvxVeIC7r8xh9vQF+ekVFSM3L98Qk9gJa+KAOf95pLA51Aepi0QEiql6X7b2LhH1g6tDiZsg603zanApLtKPXorJEKFHytloERtGVSQ5Xe7LK6o/sxtEG6zZjxg1WseYEXJokeVmHPg2HRZgDoh1VNvw13HNzb5ZZM3ZIZm7zZCTIGZMV/MjLLIO66qIQIcXXMRI9omHNMM5n6PbxS5nP8V0k+AdCd6T9E4bHq+NnEl14ed4ZEOEqiUiAW1TRFpKXHeYcU5TRhQe/nVtRKjhz/3MHOmKeOlQy40glyLUhjUSOZx5yV/30BxW7VcUzxfk5qC3u1d339FdfuHY/UDT/ySGZ1C0Jjpnmv8R/gYTS1PKdRbdaQAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200818172004055&quot;
        title=&quot;image-20200818172004055&quot;
        src=&quot;/static/a6942057a32f062b1491650a4ac02b48/fcda8/image-20200818172004055.png&quot;
        srcset=&quot;/static/a6942057a32f062b1491650a4ac02b48/12f09/image-20200818172004055.png 148w,
/static/a6942057a32f062b1491650a4ac02b48/e4a3f/image-20200818172004055.png 295w,
/static/a6942057a32f062b1491650a4ac02b48/fcda8/image-20200818172004055.png 590w,
/static/a6942057a32f062b1491650a4ac02b48/efc66/image-20200818172004055.png 885w,
/static/a6942057a32f062b1491650a4ac02b48/c83ae/image-20200818172004055.png 1180w,
/static/a6942057a32f062b1491650a4ac02b48/c1b63/image-20200818172004055.png 1200w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;참고:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;아마추어 퀀트, blog.naver.com/chunjein&lt;/li&gt;
&lt;li&gt;mino-park7. 2018.10.12. &quot;BERT 논문정리&quot;. &lt;a href=&quot;https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w&lt;/a&gt;. Mino-Park7 NLP Blog&lt;/li&gt;
&lt;li&gt;ebb and flow. 2020. 2. 12. &quot;[BERT] BERT에 대해 쉽게 알아보기1 - BERT는 무엇인가, 동작 구조&quot;. &lt;a href=&quot;https://ebbnflow.tistory.com/151&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://ebbnflow.tistory.com/151&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zhao HG. &quot;keras-bert&quot;. &lt;a href=&quot;https://github.com/CyberZHG/keras-bert&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://github.com/CyberZHG/keras-bert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Google Research. &quot;2/128 (BERT-Tiny)&quot;. &lt;a href=&quot;https://github.com/google-research/bert&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://github.com/google-research/bert&lt;/a&gt;. bert&lt;/li&gt;
&lt;li&gt;SKTBrain. &quot;KoBERT&quot;. &lt;a href=&quot;https://github.com/SKTBrain/KoBERT&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://github.com/SKTBrain/KoBERT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[NLP Transformer & SentecePiece]]></title><description><![CDATA[NLP  CNN, RNN 대신 Self-Attention을 사용하는 모델 Transformer는 RNN, LSTM없이 time 시퀀스 역할을 하는 모델입니다. RNN, LSTM 셀을 일체 사용하지 않았으나, 자체만으로 time…]]></description><link>https://jynee.github.io/tags#1st/NLP_Transfomer/</link><guid isPermaLink="false">https://jynee.github.io/tags#1st/NLP_Transfomer/</guid><pubDate>Sat, 15 Aug 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;nlp&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#nlp&quot; aria-label=&quot;nlp permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP&lt;/h1&gt;
&lt;h2 id=&quot;code-classlanguage-texttransformercode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-texttransformercode&quot; aria-label=&quot;code classlanguage texttransformercode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;Transformer&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CNN, RNN 대신 Self-Attention을 사용하는 모델&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Transformer는 RNN, LSTM없이 time 시퀀스 역할을 하는 모델입니다. &lt;strong&gt;RNN, LSTM 셀을 일체 사용하지 않았으나, 자체만으로 time 시퀀스 역할을 해줄 수 있는 굉장히 novel한 논문입니다.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;일반적인 Seq2Seq-Attention 모델에서의 번역 태스크의 문제는 원본 언어(Source Language), 번역된 언어(Target Language)간의 어느정도 대응 여부는 어텐션을 통해 찾을 수 있었으나, &lt;strong&gt;각 자신의 언어만에 대해서는 관계를 나타낼수 없었습니다.&lt;/strong&gt; 예를 들면 &lt;code class=&quot;language-text&quot;&gt;I love tiger but it is scare&lt;/code&gt;와 &lt;code class=&quot;language-text&quot;&gt;나는 호랑이를 좋아하지만 그것(호랑이)는 무섭다&lt;/code&gt; 사이의 관계는 어텐션을 통해 매칭이 가능했지만 &lt;code class=&quot;language-text&quot;&gt;it&lt;/code&gt;&lt;strong&gt;이 무엇을 나타내는지?&lt;/strong&gt;와 같은 문제는 기존 Encoder-Decoder 기반의 어텐션 메커니즘에서는 찾을 수 없었습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;출처: &lt;a href=&quot;https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;platfarm tech team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/708170af4ef2002fbb1abeadbfa500ff/38cea/image-20200822112320942.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 121.62162162162163%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAYAAAD6S912AAAACXBIWXMAAAsSAAALEgHS3X78AAADXUlEQVQ4y41VCW/aSBjl93e1yl5SpWhTle6m6pYm2bYpMZASb2xzGmPwBeY0ECAcISWXAoG333gLCwHSjjTyePzNm/e977BnOp3iW5ONm5sbtNttXF1due+bbD14YszA2Bh+GaJRb6Df7c/3F7/Phud72PUHfUTUCD6d+CEqIlqd1kaWnqfY/Q94gWQ2DS58jKiSRKfX3czwe1xun3chSgr+fh8Af5pEuVKjbxOwz49BPZtYLa51XQV35MXB/jYCnBeSFEal4qxl6XkMMJlM3Dnfp3U8pcAnRvDSfwSfFAUfi8HU9fWAm7SYjYfxCKqRQ9hu4w2fwqdMFSnDhmUaTzNkrDqdDup1B82zGtIkvkaBME0TGrksiIfY23+Jd3s74LhDFArFJcDZcw54e3uLjJpFIHgA759beLX7E974niMpJ5GWJYQOtuB/+yP4Dz9Djn6GlbNXtF4CvKZKMDUNolnCbsLCKyGNP0QVUiwBU02gHPNBO9mFk3gHS40iq5sYjUYrOTsHvLu7QzaTgWaXEU5mEZRkyLkKMclBURTIqRTC/AlORQGGYSBnWbi/v3elWnLZFf7hARcXF7Btm7SLQxQ4SGIQuhYnTeuUNmkc+Xfge/sr9vd+QSj4EaVSeb3Ls83LywGJbyBy+gHv//oBH31b2Hv9jMS3kTfS0D7/DuHgN1j8CxiKAN2wvhWUG9fIMQRooW1kj19ADWyjVCwgZygoiF5kQjsoSl7kMhIMK/80INNQlmVKlQzCxxyOQxxSyTiq1SpVRQWqIoPzH+KUD5MsKZJB3+wyE/f6+hrFYpEEN5HL5aFpOuVll6S4xGBwiW6354Lk83k3N8/OztyAMP1XgtLtdkmrgttAG2Ro0aESsVqqGDpg04V5smNgzWbTPcMuXBtlxnI0HsMhMDsehyaKqKoZjGh/0OvBoXUsGIQViaJeLuOecpAVA8vFtRrORpXcMfh/kKLDmfAJxnSgR8wtQUCCC0DnebRqtY2d3bPSeemdMT2nup4uRpFmr9/Hw0IiP253K91m9hwOh26jWBSc6VwmV5nOk0eg/104Xe2Hc6Ov/XC2NybGfWLHAtAjPZne64gsAbZaLTfnWMXUG3U0Gg04juNO5mqnc05lWEOT7Nhejdbs1zon8ZXhv9ouB5kgmxfYAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200822112320942&quot;
        title=&quot;image-20200822112320942&quot;
        src=&quot;/static/708170af4ef2002fbb1abeadbfa500ff/fcda8/image-20200822112320942.png&quot;
        srcset=&quot;/static/708170af4ef2002fbb1abeadbfa500ff/12f09/image-20200822112320942.png 148w,
/static/708170af4ef2002fbb1abeadbfa500ff/e4a3f/image-20200822112320942.png 295w,
/static/708170af4ef2002fbb1abeadbfa500ff/fcda8/image-20200822112320942.png 590w,
/static/708170af4ef2002fbb1abeadbfa500ff/38cea/image-20200822112320942.png 678w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;순서:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Seq 처리: &apos;&lt;code class=&quot;language-text&quot;&gt;positional encoding&lt;/code&gt;&apos; (순서정보를 입력하면 저장되고 공식에 맞춰 벡터를 수치화 시키는 곳)&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt; 상대적/절대적인 정보를 넣어야 함 &lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;그걸 (input) encoder의 attention으로 전달 &lt;br&gt;&lt;/li&gt;
&lt;li&gt;2의 결과를 FNN을 전달(단순 linear prejection(차원축소) 위함) &lt;br&gt;&lt;/li&gt;
&lt;li&gt;3의 결과를 Decoder의 attention으로 전달&lt;br&gt;&lt;/li&gt;
&lt;li&gt;(&lt;code class=&quot;language-text&quot;&gt;fine Tuning&lt;/code&gt;) input Decoder에서 입력한 순서 정보(positional encoding) 출력&lt;br&gt;&lt;/li&gt;
&lt;li&gt;5의 결과를 attention으로 전달&lt;br&gt;&lt;/li&gt;
&lt;li&gt;4의 결과와 합침&lt;br&gt;&lt;/li&gt;
&lt;li&gt;FNN으로 전달(linear, softmax 거침)&lt;br&gt;&lt;/li&gt;
&lt;li&gt;그 결과물을 일부(?) 다시 1로 전달&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;attention&lt;/code&gt;: 입력된 값을 가중치로 조정하여 목적에 맞게 바꿔줌 &lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;self attention&lt;/code&gt;: 입력값인 Q와 transposed K를 내적한 후, Scaled with Softmax 한다. 이는 곧 Self-Attention 시키는 것이고, 이를 통해 attention score 얻을 수 있다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;Masked&lt;/code&gt;: Self-Attention시 자신의 time step 이후 word는 가려 Self-Attention 되는 것을 막는 역할. &lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token triple-quoted-string string&quot;&gt;&apos;&apos;&apos;
# make mask like this.
0 1 1
0 0 1
0 0 0
&apos;&apos;&apos;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;d_model&lt;/code&gt; : 임베딩을 하기 위한 차원으로 보통 512를 사용. embedding vector table. &lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;attention value&lt;/code&gt;: 수치화된 table에서 중요한 정보를 골라 가중치를 줄 수 있음&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&quot;encoder&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#encoder&quot; aria-label=&quot;encoder permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Encoder&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Positional Encoding&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Multi-Head Attention&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Scaled-Dot Product Attention&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 308px; &quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/922895bef02e2da96f7eb3bcd6309284/2ece4/image-20200822112025592.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 160.13513513513513%; position: relative; bottom: 0; left: 0; background-image: url(&amp;apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAYAAAASYli2AAAACXBIWXMAAAsSAAALEgHS3X78AAAFW0lEQVRIx41Wa09TWRTtbzMmfjHGDyYaZxKdMfH5wS/GzCSaGOMMkmEGhjcM4ntwEAS0LS30AeVSSlv6oC9KHzyUR4tAsWpbSu+ada4t1gLBk+ycc++5d9219157n6tCxZBlWbHSulAo7LHS/n5DtR9g+XzQWF9fx8bGhmJivbq6inw+vxewHOz9+/eYn5/H8vIy1tbWkEgklZetVivGx8fhdDpht9vhcrsRiUTg9/sPZvj27Vs4HA5ldrlc0Go1GB7WY3BQi5aWJoyOmhUzmYwElxCNRhEOh/cCihiJ4fF4EIvHCObBG/ULGM1N0A42wmRug26oAVpdPYYNrbDZO7nXg7ExK2ZmZg4G9Pl80Ov1WFlZxciIAX833aNV0X7HXw330Nj2B+qb76O5vRoabT/dn0IoFDoYcHp6GkaTiTEowOJwo9s1B2k1iy5rEG1aCf+O+2CYS0Ef30K/0YLJiUMYCkCD0YjtbAYmmxO6hRTMSx+gDr1DnzeGXppxcRO2NNBvlmBjHMOzs4cwJGCBUjBKE2gftuLxiAMN/UNoov3Z9RotahOeTwbxQq2Hy+lQ4q6q1F45oIhhKpViHFfgDwQxMxtRrLevD+FIFNFYHCFmdmFhQQFLp9NfGVYKWSTF6/VCoitCNgF/AH6fH25qTqPWMLQydnZ2+CKQ+ZxRNLlbKSWwXC6niFlciy9ubW1he3ubDJcRCPgwOxtiJv3QaF5TyGFEomGEZ4JkOK98dA9gJpNRUr+5uUmAgKJ+m82O3t4n1F0tevpq8EZbj4E3dXjVX4OB13UwW1oxqO/ClNP9bS3v57KIYTK5Bkkyo7nxDtpbf8OD9irU195GB+eHD6ppVayiXno0vZdhZVJElnd46bFb4B5shvG/Ooz2NMChacHIyzoYX9QiaOzAmL4bjin3dwAaDMjk8nDbTMhG+7AeeIW49SmS3m4s2J5h1d0NrBlgM7zEpGNq//b1jbAJmNvOw2E1YWa0E7PSM0j9jbBpWcOaNvhMDxVQk7oLznKGEAyLtgtIyQiXP338iBR73qPOTujZaaYcdnQ++AfjkgWjI2a0tbYiFospclIAFXepJ7nYiQtFpl72NrPFggSFneb+CuepYBAu3n/Z00s1zMBPL9ZEY6XcRF/8GsPyfpjNCkEizixbNBpYHj/GCFlIHR2QHj2C7skTPOX8jkCLbLbvPn2Cb24Onl2GwkW6U7h+HbkrV7Bz+TLyFy9i+9IlJE+eRO7oURSuXcMH7mWuXkX63DkEjhxB7MQJRM6cQYT343x/p3SmyM3NwI0b+ExGMboYZZcOkH6M7oSp/pXFRYQYoxBZJCj4HMXvGRoCzwZAsFKrgZs3gVu3lLCpZDLhgQE6iiW6kWTpiYNHtP5EIoFNrsOsmiTPlQmJbWpiAgME2Uh//KKKoskXLoDZIeDdu0BtLbKM2yK7yjwB47Qw15FkEsvM8iaT5SXDID8iXDPZbMhwFuu8QGW54qefIbPuVew5kG/fBhg3hfqdO8D9+0BNDVBVBVRXI/XrLwj8+AMWzp9H8NQpTB07hjjnxdOnkTh7FlvHjwMMl6zEsJRdxkdmC5fpkjw8DFmnQ4HGAwVzTU1IiVgP9GODWZ5ra8Pi02eQDcPKvofPF75URZkOi/KRy2RU0qSfrT1km0TE5YaHcQxy7TaPwMFKSgRCcFsnCJHfp1KEhISJD9BE+xdjgWeuRaNlzRohsQP1kuXYoA4W2ijvh7zTu71AhUPGbuMoWoHXVhsZ7fOrcgjgtz9N5f1S1G7lz1VpX/Vd7IpzlmW5tLS0e3BlM9k9zx3qcuURIQ5zwVDM4syp7Pb/A18t+UatYjBMAAAAAElFTkSuQmCC&amp;apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;image-20200822112025592&quot; title=&quot;image-20200822112025592&quot; src=&quot;/static/922895bef02e2da96f7eb3bcd6309284/2ece4/image-20200822112025592.png&quot; srcset=&quot;/static/922895bef02e2da96f7eb3bcd6309284/12f09/image-20200822112025592.png 148w,
/static/922895bef02e2da96f7eb3bcd6309284/e4a3f/image-20200822112025592.png 295w,
/static/922895bef02e2da96f7eb3bcd6309284/2ece4/image-20200822112025592.png 308w&quot; sizes=&quot;(max-width: 308px) 100vw, 308px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;br /&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 576px; &quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 129.7297297297297%; position: relative; bottom: 0; left: 0; background-image: url(&amp;apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAaCAYAAAC3g3x9AAAACXBIWXMAAAsSAAALEgHS3X78AAAD3ElEQVRIx51V+VMaSRTmD93aX/cf2fxiKmajkngsKIRzOAdQjmGAUYZR7sNwlCbgERCMKUtBxYDf9rQOCmKS2q561T1vXn/9vff6vVZhyri7uxutM5nMVP1LQzUJpGwqVypIppJwul1IZdKoVKu/Baqaxuz29hYhPoxEegdichv8Vgy+wMb/A7y7X4DjObDrHpgZC2wuB4Jc8JkXvwScZDAcAhwXJYyHvx3HqS5/+9aBGPehkI+AC5lRLESwLfnx/fvZL0FVT90YDgdUWSqlUCmtoLq7ir2KHuWSFonNN2jUqw/MB6M9T0Mgz6ppMTk+akCI6pCIm+DzqBEKrCAU/BedTvNFZgqGSlnc3Nyg1+sS6eHq+hoXF130+z9gMJggSTu4vOzhmugVG0W63S7VjxjKi/39fXg9QfBcHFxw80EExCIS3s4ugHX5EQmLT/5tEsYCnaO8hHr98BGwUqlhblaHlfcuLM7b8GGOobMsC29NmJ1Zg/qdBep/LFhasFOR/2kW3ffzEoNctoj+bf/e5YSYRIAtIhVvgrVtw6ANQrvkBWMUwBhiVFwWEU5LHHqNH8bVENyMBCl2gDj/BW9mlqHRrOGkfUILQiUlUuB8ZaTjLexsfoXIN5CIHmDDnSMAQQJEwMwiAdnGJrdPQOqQoofIiG3wG1X8+cdfyGazJMaXOD8/h6pU/ETYhOFxxGHWB8Hat7DB7sBuiUCv9WD5PQPtshNG3Tq8ThE+VwJOawx2cxRrKyzWff7xezgY/EDvqou4uIVXr/5GabdIgvwZzdZXmtHXr2fgJ3Vcr3+h+uvrHgLke27+HQqFHHVTFuX60Sy32x24WBYa7SoKpSJqe3s4OjpCLpfDvFqNqCBgt1xG4+CAXO4GrAwDk8VCbWVptVqPWZY3GswmaPU6GK1mmBgrXB4WdqcTGt0aVj/qYXXYYbYxCIRCWNKsQGc0EDsLjETWA340Go1HwFQ6DSmTRKqQhZROghMimFuYp+0ru1ugLSyR2oHO8BF2hwNhIYp8uUTbmje4AcZmQ/ey+wgo0xXiW2C9JAHk9KgQI7GqI5vPIbYl4MPyItyEcSabQbPZBB+NUFvNmhbhCD9ydzgcjnebYrFIqsXzrEZXtVpShhdjOlEUwYf5Z3VMGSrI8un1h1gMBoPRifl8HkO5ExGR9fLYI0lrt9tjzEZZVtCPj4+poWKk6JPJ5AhI2Vyr1egtmGQ31m1OT0/HjBR9lTxOk4CHh4fUflqzHQHKLpydjXfkq6sr2okmYyUDyiH6KaAceLkenxr1+32cnJw8c6vT6bzMcFI5ufln+mlvy39rL2AuubDZywAAAABJRU5ErkJggg==&amp;apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;image-20200822105845609&quot; title=&quot;image-20200822105845609&quot; src=&quot;/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png&quot; srcset=&quot;/static/c924934dafc5a3ca5fd9cd80c3820dd0/12f09/image-20200822105845609.png 148w,
/static/c924934dafc5a3ca5fd9cd80c3820dd0/e4a3f/image-20200822105845609.png 295w,
/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png 576w&quot; sizes=&quot;(max-width: 576px) 100vw, 576px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;br /&gt;&lt;/td&gt;
&lt;td&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 355px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/526ee/image-20200815215109073.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 112.16216216216218%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAYAAADAQbwGAAAACXBIWXMAAAsSAAALEgHS3X78AAAC8ElEQVQ4y5VUW1PaUBDmv/ehfagPbWdqH5xOp4oFsVULeOEiNCAkQAIBBLkngCBFuQhyh6/nBIMIanFnFsKe5cu3Z79dDYgNBgPU63Xc3t6i2Wzi+rqKGv3dbKBcLqNJ4jc3N2i322jftVGr1ZT8RqOhxMbjMYXBZDKBhj70uh0IPA+W4xAI8AgGBYjhCNKpJOIXCcTicXCsF5KcR6FQQCgYRDQSgSAI4IUger3eAyD9eMro281mM/x+P2xWK1iWRTgcRiqZgt1uB8MwaLVaMyAVRzMfoK7Sp2VFo1FkMmmk0yl4vR7kpByq1b+4uIgjFAop16P+/1HJ84DUVNB+f4BYLAFJKuDk2IZ4LInYeYK8rD6rZL7CWckUYN5Ho5GSIPAR6L+7sbFuxPqHn9j4/BuGTQ+OzS7lfDQaL4Fq8IJdFsrQbe3D5fDh1HIGN+PH7o4JYfF8VsliDzRUFrRb2WwWoihCykkIBHmccg7YPDZo97TwiB64eAZ3nTbpskyk0lpiNgOkYCaTCVbSSYvFAuYPA8PBLg5DJ9i266G16KBzGLDPGiETsMpV5VEzlhhGiJ7cbjecTidKpdK0w/UaOB8HlvgPvQ4C0SXrY9HtdVEsFpUBeJYhnRIqzH6/r0zMcDKG38LAu2VEcM8O3mBB+MABcdeGZCiKYunyZcDFAE0JMSzOdo7g+WWFSz/99hHPEMn8F3BegzOBEs/JaYTCHpiPDIgneOQvZeVMluVHE/IswwdRTzXoY/dxan0Lp/0DTg7fEFGzSlwi0/IqwMlkKtZKRUIm7YfbZSKSEkijKveA0mqA8yOXSefgsAZgPWaxvWmEyykStj6ywmqKElZuigro9fBkzExYe/cNH99r8WlNh69fTMhliQ4rV0tL4VlANYHKx+tlkUplyO4TwXE+snWyylk+n3+lbO6TqIDpXVHwcrmEbrc7i3c6ndUZqknValVZ79Tq9cYMhLIbDoerM1xMVO/1qd33lD3LcNXnRfsHVU52tbegVB4AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200815215109073&quot;
        title=&quot;image-20200815215109073&quot;
        src=&quot;/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/526ee/image-20200815215109073.png&quot;
        srcset=&quot;/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/12f09/image-20200815215109073.png 148w,
/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/e4a3f/image-20200815215109073.png 295w,
/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/526ee/image-20200815215109073.png 355w&quot;
        sizes=&quot;(max-width: 355px) 100vw, 355px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;여기서 Mask(opt.) 부분은 &lt;br /&gt;Decoder의 Masked MultiHead Attention과 &lt;br /&gt;다른 Masking이고 Optional 한 부분. &lt;br /&gt;단순히 아래와 같은 코드를 통해 &lt;br /&gt;입력 dimension(=512) 중 word가 &lt;br /&gt;아닌 경우를 구분하는 역할&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;Encoder&lt;/code&gt; 기능 설명&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;이름&lt;/th&gt;
&lt;th&gt;기능&lt;/th&gt;
&lt;th&gt;설명&lt;/th&gt;
&lt;th&gt;매커니즘&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Positional Encoding&lt;/code&gt;&lt;/strong&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 308px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/922895bef02e2da96f7eb3bcd6309284/2ece4/image-20200822112025592.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 160.13513513513513%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAYAAAASYli2AAAACXBIWXMAAAsSAAALEgHS3X78AAAFW0lEQVRIx41Wa09TWRTtbzMmfjHGDyYaZxKdMfH5wS/GzCSaGOMMkmEGhjcM4ntwEAS0LS30AeVSSlv6oC9KHzyUR4tAsWpbSu+ada4t1gLBk+ycc++5d9219157n6tCxZBlWbHSulAo7LHS/n5DtR9g+XzQWF9fx8bGhmJivbq6inw+vxewHOz9+/eYn5/H8vIy1tbWkEgklZetVivGx8fhdDpht9vhcrsRiUTg9/sPZvj27Vs4HA5ldrlc0Go1GB7WY3BQi5aWJoyOmhUzmYwElxCNRhEOh/cCihiJ4fF4EIvHCObBG/ULGM1N0A42wmRug26oAVpdPYYNrbDZO7nXg7ExK2ZmZg4G9Pl80Ov1WFlZxciIAX833aNV0X7HXw330Nj2B+qb76O5vRoabT/dn0IoFDoYcHp6GkaTiTEowOJwo9s1B2k1iy5rEG1aCf+O+2CYS0Ef30K/0YLJiUMYCkCD0YjtbAYmmxO6hRTMSx+gDr1DnzeGXppxcRO2NNBvlmBjHMOzs4cwJGCBUjBKE2gftuLxiAMN/UNoov3Z9RotahOeTwbxQq2Hy+lQ4q6q1F45oIhhKpViHFfgDwQxMxtRrLevD+FIFNFYHCFmdmFhQQFLp9NfGVYKWSTF6/VCoitCNgF/AH6fH25qTqPWMLQydnZ2+CKQ+ZxRNLlbKSWwXC6niFlciy9ubW1he3ubDJcRCPgwOxtiJv3QaF5TyGFEomGEZ4JkOK98dA9gJpNRUr+5uUmAgKJ+m82O3t4n1F0tevpq8EZbj4E3dXjVX4OB13UwW1oxqO/ClNP9bS3v57KIYTK5Bkkyo7nxDtpbf8OD9irU195GB+eHD6ppVayiXno0vZdhZVJElnd46bFb4B5shvG/Ooz2NMChacHIyzoYX9QiaOzAmL4bjin3dwAaDMjk8nDbTMhG+7AeeIW49SmS3m4s2J5h1d0NrBlgM7zEpGNq//b1jbAJmNvOw2E1YWa0E7PSM0j9jbBpWcOaNvhMDxVQk7oLznKGEAyLtgtIyQiXP338iBR73qPOTujZaaYcdnQ++AfjkgWjI2a0tbYiFospclIAFXepJ7nYiQtFpl72NrPFggSFneb+CuepYBAu3n/Z00s1zMBPL9ZEY6XcRF/8GsPyfpjNCkEizixbNBpYHj/GCFlIHR2QHj2C7skTPOX8jkCLbLbvPn2Cb24Onl2GwkW6U7h+HbkrV7Bz+TLyFy9i+9IlJE+eRO7oURSuXcMH7mWuXkX63DkEjhxB7MQJRM6cQYT343x/p3SmyM3NwI0b+ExGMboYZZcOkH6M7oSp/pXFRYQYoxBZJCj4HMXvGRoCzwZAsFKrgZs3gVu3lLCpZDLhgQE6iiW6kWTpiYNHtP5EIoFNrsOsmiTPlQmJbWpiAgME2Uh//KKKoskXLoDZIeDdu0BtLbKM2yK7yjwB47Qw15FkEsvM8iaT5SXDID8iXDPZbMhwFuu8QGW54qefIbPuVew5kG/fBhg3hfqdO8D9+0BNDVBVBVRXI/XrLwj8+AMWzp9H8NQpTB07hjjnxdOnkTh7FlvHjwMMl6zEsJRdxkdmC5fpkjw8DFmnQ4HGAwVzTU1IiVgP9GODWZ5ra8Pi02eQDcPKvofPF75URZkOi/KRy2RU0qSfrT1km0TE5YaHcQxy7TaPwMFKSgRCcFsnCJHfp1KEhISJD9BE+xdjgWeuRaNlzRohsQP1kuXYoA4W2ijvh7zTu71AhUPGbuMoWoHXVhsZ7fOrcgjgtz9N5f1S1G7lz1VpX/Vd7IpzlmW5tLS0e3BlM9k9zx3qcuURIQ5zwVDM4syp7Pb/A18t+UatYjBMAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200822112025592&quot;
        title=&quot;image-20200822112025592&quot;
        src=&quot;/static/922895bef02e2da96f7eb3bcd6309284/2ece4/image-20200822112025592.png&quot;
        srcset=&quot;/static/922895bef02e2da96f7eb3bcd6309284/12f09/image-20200822112025592.png 148w,
/static/922895bef02e2da96f7eb3bcd6309284/e4a3f/image-20200822112025592.png 295w,
/static/922895bef02e2da96f7eb3bcd6309284/2ece4/image-20200822112025592.png 308w&quot;
        sizes=&quot;(max-width: 308px) 100vw, 308px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;벡터로된 위치정보에다가 공식을 사용해서 &lt;strong&gt;&apos;서로 간격이 일정하도록&amp;#x26;고르게 분포 하도록 반영&lt;/strong&gt;하면, 딥러닝이 이 정보를 분석해 어떤 pattern을 찾는다.&lt;br /&gt;*&lt;code class=&quot;language-text&quot;&gt;서로 간격이 일정하도록 고르게 분포&lt;/code&gt; : 모든 벡터가 거리 값이 같다.(아래 추가 설명 참고)&lt;/td&gt;
&lt;td&gt;문장은 일반적인 임베딩과 &lt;code class=&quot;language-text&quot;&gt;Positional Encoding&lt;/code&gt;을 &lt;br /&gt;&lt;strong&gt;더하여&lt;/strong&gt; Encoder의 Layer로 들어간다&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Multi-Head Attention&lt;/code&gt;&lt;/strong&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 576px; &quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 129.7297297297297%; position: relative; bottom: 0; left: 0; background-image: url(&amp;apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAaCAYAAAC3g3x9AAAACXBIWXMAAAsSAAALEgHS3X78AAAD3ElEQVRIx51V+VMaSRTmD93aX/cf2fxiKmajkngsKIRzOAdQjmGAUYZR7sNwlCbgERCMKUtBxYDf9rQOCmKS2q561T1vXn/9vff6vVZhyri7uxutM5nMVP1LQzUJpGwqVypIppJwul1IZdKoVKu/Baqaxuz29hYhPoxEegdichv8Vgy+wMb/A7y7X4DjObDrHpgZC2wuB4Jc8JkXvwScZDAcAhwXJYyHvx3HqS5/+9aBGPehkI+AC5lRLESwLfnx/fvZL0FVT90YDgdUWSqlUCmtoLq7ir2KHuWSFonNN2jUqw/MB6M9T0Mgz6ppMTk+akCI6pCIm+DzqBEKrCAU/BedTvNFZgqGSlnc3Nyg1+sS6eHq+hoXF130+z9gMJggSTu4vOzhmugVG0W63S7VjxjKi/39fXg9QfBcHFxw80EExCIS3s4ugHX5EQmLT/5tEsYCnaO8hHr98BGwUqlhblaHlfcuLM7b8GGOobMsC29NmJ1Zg/qdBep/LFhasFOR/2kW3ffzEoNctoj+bf/e5YSYRIAtIhVvgrVtw6ANQrvkBWMUwBhiVFwWEU5LHHqNH8bVENyMBCl2gDj/BW9mlqHRrOGkfUILQiUlUuB8ZaTjLexsfoXIN5CIHmDDnSMAQQJEwMwiAdnGJrdPQOqQoofIiG3wG1X8+cdfyGazJMaXOD8/h6pU/ETYhOFxxGHWB8Hat7DB7sBuiUCv9WD5PQPtshNG3Tq8ThE+VwJOawx2cxRrKyzWff7xezgY/EDvqou4uIVXr/5GabdIgvwZzdZXmtHXr2fgJ3Vcr3+h+uvrHgLke27+HQqFHHVTFuX60Sy32x24WBYa7SoKpSJqe3s4OjpCLpfDvFqNqCBgt1xG4+CAXO4GrAwDk8VCbWVptVqPWZY3GswmaPU6GK1mmBgrXB4WdqcTGt0aVj/qYXXYYbYxCIRCWNKsQGc0EDsLjETWA340Go1HwFQ6DSmTRKqQhZROghMimFuYp+0ru1ugLSyR2oHO8BF2hwNhIYp8uUTbmje4AcZmQ/ey+wgo0xXiW2C9JAHk9KgQI7GqI5vPIbYl4MPyItyEcSabQbPZBB+NUFvNmhbhCD9ydzgcjnebYrFIqsXzrEZXtVpShhdjOlEUwYf5Z3VMGSrI8un1h1gMBoPRifl8HkO5ExGR9fLYI0lrt9tjzEZZVtCPj4+poWKk6JPJ5AhI2Vyr1egtmGQ31m1OT0/HjBR9lTxOk4CHh4fUflqzHQHKLpydjXfkq6sr2okmYyUDyiH6KaAceLkenxr1+32cnJw8c6vT6bzMcFI5ufln+mlvy39rL2AuubDZywAAAABJRU5ErkJggg==&amp;apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;image-20200822105845609&quot; title=&quot;image-20200822105845609&quot; src=&quot;/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png&quot; srcset=&quot;/static/c924934dafc5a3ca5fd9cd80c3820dd0/12f09/image-20200822105845609.png 148w,
/static/c924934dafc5a3ca5fd9cd80c3820dd0/e4a3f/image-20200822105845609.png 295w,
/static/c924934dafc5a3ca5fd9cd80c3820dd0/533c1/image-20200822105845609.png 576w&quot; sizes=&quot;(max-width: 576px) 100vw, 576px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;1문장을 여러 head로 Self-Attention 시킴&lt;br /&gt;&lt;/td&gt;
&lt;td&gt;”Je suis étudiant”라는 문장의 임베딩 벡터가 512차원(&lt;code class=&quot;language-text&quot;&gt;d_model&lt;/code&gt;)이라면 8개 &lt;code class=&quot;language-text&quot;&gt;head&lt;/code&gt;로 나눠 64개의 벡터를 한 &lt;code class=&quot;language-text&quot;&gt;Scaled Dot Attention&lt;/code&gt;이 맡아 처리하는 것. &lt;br /&gt;&lt;strong&gt;이는 동일한 문장도 8명(8 heads)이 각각의 관점에서 보고 추후에 합치는 과정&lt;/strong&gt;이라고도 볼 수 있다.&lt;br /&gt;*&lt;code class=&quot;language-text&quot;&gt;d_model&lt;/code&gt;은 임베딩을 하기 위한 차원으로 보통 512를 사용하고, &lt;code class=&quot;language-text&quot;&gt;d_k&lt;/code&gt;와 &lt;code class=&quot;language-text&quot;&gt;d_v&lt;/code&gt;는 64를 사용. &lt;br /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;https://miro.medium.com/max/2015/1*lsMX7fYhk55VCavim7GiMg.png&quot; alt=&quot;Image for post&quot;&gt;&lt;br /&gt;동일한 [batch&lt;em&gt;size x len&lt;/em&gt;q x d_model] shape의 동일한 &lt;code class=&quot;language-text&quot;&gt;Q, K, V&lt;/code&gt;를 만들어 [d&lt;em&gt;model, d&lt;/em&gt;model] &lt;code class=&quot;language-text&quot;&gt;linear&lt;/code&gt;를 곱한 후, &lt;br /&gt;&lt;strong&gt;임베딩 차원을 8등분하여 &lt;code class=&quot;language-text&quot;&gt;Scaled Dot Product Atttention&lt;/code&gt;으로 넘겨준다.&lt;br /&gt;&lt;/strong&gt;그리고 multi-head attention 그림에서의 h는 &lt;code class=&quot;language-text&quot;&gt;n_heads&lt;/code&gt;(number of head)를 나타내며 보통 8을 사용. 이는 64 * 8 = 512이기 때문.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Scaled-Dot Product Attention&lt;/code&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 355px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/526ee/image-20200815215109073.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 112.16216216216218%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAYAAADAQbwGAAAACXBIWXMAAAsSAAALEgHS3X78AAAC8ElEQVQ4y5VUW1PaUBDmv/ehfagPbWdqH5xOp4oFsVULeOEiNCAkQAIBBLkngCBFuQhyh6/nBIMIanFnFsKe5cu3Z79dDYgNBgPU63Xc3t6i2Wzi+rqKGv3dbKBcLqNJ4jc3N2i322jftVGr1ZT8RqOhxMbjMYXBZDKBhj70uh0IPA+W4xAI8AgGBYjhCNKpJOIXCcTicXCsF5KcR6FQQCgYRDQSgSAI4IUger3eAyD9eMro281mM/x+P2xWK1iWRTgcRiqZgt1uB8MwaLVaMyAVRzMfoK7Sp2VFo1FkMmmk0yl4vR7kpByq1b+4uIgjFAop16P+/1HJ84DUVNB+f4BYLAFJKuDk2IZ4LInYeYK8rD6rZL7CWckUYN5Ho5GSIPAR6L+7sbFuxPqHn9j4/BuGTQ+OzS7lfDQaL4Fq8IJdFsrQbe3D5fDh1HIGN+PH7o4JYfF8VsliDzRUFrRb2WwWoihCykkIBHmccg7YPDZo97TwiB64eAZ3nTbpskyk0lpiNgOkYCaTCVbSSYvFAuYPA8PBLg5DJ9i266G16KBzGLDPGiETsMpV5VEzlhhGiJ7cbjecTidKpdK0w/UaOB8HlvgPvQ4C0SXrY9HtdVEsFpUBeJYhnRIqzH6/r0zMcDKG38LAu2VEcM8O3mBB+MABcdeGZCiKYunyZcDFAE0JMSzOdo7g+WWFSz/99hHPEMn8F3BegzOBEs/JaYTCHpiPDIgneOQvZeVMluVHE/IswwdRTzXoY/dxan0Lp/0DTg7fEFGzSlwi0/IqwMlkKtZKRUIm7YfbZSKSEkijKveA0mqA8yOXSefgsAZgPWaxvWmEyykStj6ywmqKElZuigro9fBkzExYe/cNH99r8WlNh69fTMhliQ4rV0tL4VlANYHKx+tlkUplyO4TwXE+snWyylk+n3+lbO6TqIDpXVHwcrmEbrc7i3c6ndUZqknValVZ79Tq9cYMhLIbDoerM1xMVO/1qd33lD3LcNXnRfsHVU52tbegVB4AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200815215109073&quot;
        title=&quot;image-20200815215109073&quot;
        src=&quot;/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/526ee/image-20200815215109073.png&quot;
        srcset=&quot;/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/12f09/image-20200815215109073.png 148w,
/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/e4a3f/image-20200815215109073.png 295w,
/static/d9e4a628d8ca9bb7ff9ab4b656f83bf7/526ee/image-20200815215109073.png 355w&quot;
        sizes=&quot;(max-width: 355px) 100vw, 355px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;code class=&quot;language-text&quot;&gt;Self-Attention&lt;/code&gt;이 일어나는 부분.&lt;br /&gt;&lt;/td&gt;
&lt;td&gt;- 한 &lt;code class=&quot;language-text&quot;&gt;head&lt;/code&gt;당 &lt;br /&gt;(input 값) : Q(64), K(64), V(64)씩 가져가게 되는데, dimension(=512) 중 word가 아닌 경우를 구분하는 역할.&lt;br /&gt;- 처음의 Q를 ResNet의 &lt;code class=&quot;language-text&quot;&gt;Residual Shortcut&lt;/code&gt;와 같은 컨셉으로 더해준다. 이는 층이 깊어지는 것을 더 잘 학습 시키기 위함&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;https://miro.medium.com/max/1433/1*Mj7Bdi0xVPPIqGbfu7cbSw.png&quot; alt=&quot;Image for post&quot;&gt;&lt;br /&gt;&lt;em&gt;&lt;code class=&quot;language-text&quot;&gt;Self-Attention&lt;/code&gt;&gt;&lt;/em&gt;&lt;br /&gt;1. Q와 transposed K 사이를 내적하여 어텐션을 Softmax를 통해 구하고,&lt;br/&gt;2. 그 후에 V를 내적하여 중요한 부분(Attention)을 더 살린다.&lt;br /&gt;3. 이렇게 8개의 head(여러 관점)으로 본 것을 다시 &lt;code class=&quot;language-text&quot;&gt;concate&lt;/code&gt;하고&lt;br /&gt;4. &lt;code class=&quot;language-text&quot;&gt;PoswiseFeedForwardNet&lt;/code&gt;로 보냄&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;PoswiseFeedForwardNet&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;각 &lt;code class=&quot;language-text&quot;&gt;head&lt;/code&gt;가 만들어낸 &lt;code class=&quot;language-text&quot;&gt;Self-Attention&lt;/code&gt;을 치우치지 않게 &lt;strong&gt;균등하게 섞는 역할&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;FNN&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt; 출처: &lt;a href=&quot;https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;platfarm tech team&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;추가 설명&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;positional encoding&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;PE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; positional_encoding&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# emb_size = dmodel = 2차원으로 표현 &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;(5,6) 값을 변경해도 발산하지 않고 뱅글뱅글 돌면서 각 벡터들의 간격이 일정하게 유지된다.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;positional_encoding(50,2)&lt;/th&gt;
&lt;th&gt;positional_encoding(50,5)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 498px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/7d4dbcc8bf619669b1221fe18db80425/79e1b/image-20200813105755022.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 93.24324324324323%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAACgklEQVQ4y5VUaW/TQBDN//8RSHxAiELDIRVUVQhVqqqWpk2htIVSSGLn9n2u78fMOnYdp0Ew0mht7+zzm9k303FdF0EQIAzDNY8jgTSOkNDq+QFcL5DvWRLJvWasEAKMk2UZOrqu4zEz/BjnIwuHdxreX83QvZjg4GaBvmLBCpON+DiOkaYpOr7vI89zFEUhN5Isx8kvHd1zFYc/lrie2BjqvvS+YuLgeo5d2rsk4NLKc1EUlYCmaRBg+VEkGbGYE6MprCDGNlu4ghirEjwlMvIspS0BXc+TDNk+3s4lYGU5sV7zvFzZMnp+2x9j/9tsnWFVw6Ee4GVPlYEVmEyI1qZXYFVGT45+QzVD5GmChAG9FcN3lxPcTJ21A02Q9nsVc/RTwyeqdUGAZcq2Bc2L8OxkJP/YZtW29t7CEXjTn8BwfBQ5yUYEPgaaRx/HG2lus+Z+SqroXowx0Z0S0LFMKYm9r9P1VIF/AsyoXLsEqGorwJAY6p6QDOM0/2+GLC8GXFpeCWgRwyTN8Jo+KkawccOPgcmYVTZ8kXyhrEPZetyDFIXPAwMfrmY1YPEX2TR/tHOm4JTOUpMjSViHhiE3Qrrh56ejuqWKlZDbgFWdK8m86ilI6FscRw+9nGalXO7mLp4eD9AbmltTrez4XpNlqlq0bj3TNEs2DV1x8/MlfZ85sMO4BuPnL6pFrFSpCsOP6vEQReKBYdsyiridudin5mfRvjhTpe8QENf5fulvyKoeX8vlsh6Sggcmr+RFGtPgDTE3XSna27EORbPlkM1pT6zi5DAmMNu2S0BG1zQNzNSyLHBvG3RRjuPKoMD34Do2QlrZHceh6eyBS8VgvDIwY7D9ASBwtnbJgcnKAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200813105755022&quot;
        title=&quot;image-20200813105755022&quot;
        src=&quot;/static/7d4dbcc8bf619669b1221fe18db80425/79e1b/image-20200813105755022.png&quot;
        srcset=&quot;/static/7d4dbcc8bf619669b1221fe18db80425/12f09/image-20200813105755022.png 148w,
/static/7d4dbcc8bf619669b1221fe18db80425/e4a3f/image-20200813105755022.png 295w,
/static/7d4dbcc8bf619669b1221fe18db80425/79e1b/image-20200813105755022.png 498w&quot;
        sizes=&quot;(max-width: 498px) 100vw, 498px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 498px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/eb3b62e262ed51728b09ac5cfe9f457a/79e1b/image-20200813145235361.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 93.24324324324323%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAAC+ElEQVQ4y11UCW7cMBDL///WAkmTNmiaY7O3L8mSrMPHTknZCooKELxeazgccjR31hixbhA3DJJikBiCTCnmPaYg8/bsrRPvvcxjlBC8KONkxPmA/+zgxQBnmia569pW/DjL08lIbZOYOMm+8/KJ/dEM8nwx8lYPclRBXmuXv+2Vl8pEeamcPJ+N7Fov45hWQOucyO2GAC9/KosDvdzvlVz6ID5NeZ/wbd8N0rkkjY3yeNDygDPapxz3faekA+NlnuWuBUMgig1TBqoRwNX7UXaNQ9Ao07xIRBWf7SAvV5tBuQn2ChKs7NwZuS0AZO0EbJCdHwkUEHw1QdSQMqBPK9gB5bo45XeeYTXclGMuJTdNkxm9Qg8e5iIzAnORxeNRZ0Zl2TDmalgVpaHOnmYS0FkrBlr8PPUSp1kGaMbs3B8APmkPiW85GTdBdmDLxQqY/AqDLl8layVXZDmAyRnBFJ6LjN7hKoPKIttvH53My+0LUA8plxxj3FwGwwQ2780abFAOA1kK1whDCE6X+eQ7z5z1+r11MbfYfxreMgDLYRB7q7DgovNP0JFOF2ZvYM93JqehUwEMuCEXlMrMvyFuYaY3t8mGunKR1Wk7y8WzdL8F4Jg2QNV1ORNbgtmKuwuMoPjdpiFbpQc42ZVuYNIHNLnCM6V/NFyWRX6hn8iG2RlEYLprEUyzCkgG30plk/Mcr6G2A3Dgcl2vfZjZgDoB2EIE4A2hriyz9OECbanpW20zEMs96tXlOfch7vINDNlL538C+WSSIgFNotNs8tJKvE0VNhmzsWfe5a5rs4YvV5M/7gHCvizA1K7cWZowL2sbUXP+5vB4PPZSKbuWzHnGoPsDp4yGlgZTx+VR9QMHObqeLzazZRUcaSyVRpxQKnuwHWjK5nJdVXJpe2l7TBYMUWoRQN84n/+L29B1GKIUvtY2XzMO5RHOVspIQIzp9QqYux3N7dGPGteQrnOkWWvyIU4jpVR+8t47/D9Ad6W6PME1vpFABWJcfwENdrYAv52GawAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200813145235361&quot;
        title=&quot;image-20200813145235361&quot;
        src=&quot;/static/eb3b62e262ed51728b09ac5cfe9f457a/79e1b/image-20200813145235361.png&quot;
        srcset=&quot;/static/eb3b62e262ed51728b09ac5cfe9f457a/12f09/image-20200813145235361.png 148w,
/static/eb3b62e262ed51728b09ac5cfe9f457a/e4a3f/image-20200813145235361.png 295w,
/static/eb3b62e262ed51728b09ac5cfe9f457a/79e1b/image-20200813145235361.png 498w&quot;
        sizes=&quot;(max-width: 498px) 100vw, 498px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&quot;decoder&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#decoder&quot; aria-label=&quot;decoder permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Decoder&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;설명&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;br /&gt;&lt;img src=&quot;https://miro.medium.com/max/440/1*q8Q-3t9ej6SPrCwTN4wmrA.png&quot; alt=&quot;Image for post&quot;&gt;&lt;/td&gt;
&lt;td&gt;인코더와 동일하지만, &lt;code class=&quot;language-text&quot;&gt;Self-Attention&lt;/code&gt;시, &lt;br /&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;#39;Masked&amp;#39;-Multi-Head Attention&lt;/code&gt;&lt;/strong&gt;을 쓴다는 점이 다르다.&lt;br /&gt;* &lt;strong&gt;Masked&lt;/strong&gt;를 쓰는 이유: Self-Attention시 자신의 time step 이후의 word는 가려서 Self-Attention 되는 것을 막는 역할.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;https://miro.medium.com/max/440/1*64GKtTstp4zi3CEj5Y91iw.png&quot; alt=&quot;Image for post&quot;&gt;&lt;/td&gt;
&lt;td&gt;노란색 Box에서 왼쪽 2개 화살표가 Encoder의 K,V &lt;br /&gt;오른쪽 화살표가 Self-Attention을 거친 Decoder의 Q&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;https://miro.medium.com/max/779/1*y_oOzc5s7I6urwrXiIcQAA.png&quot; alt=&quot;Image for post&quot;&gt;&lt;br /&gt;이렇게 나온 값을 일반적인Teacher Forcing을 통해 학습&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;출처 및 참고: &lt;a href=&quot;https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;platfarm tech team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&quot;inference추론&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#inference%EC%B6%94%EB%A1%A0&quot; aria-label=&quot;inference추론 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inference(추론)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;전체 Inference&lt;/th&gt;
&lt;th&gt;Encoder Inference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Inference시 Encoder의 들어오는 문장(번역할 문장)은 정확히 알지만, Decoder에 들어오는 문장(번역되어지는 문장)은 알지 못한다. &lt;br /&gt;시작 표시를 나타내는 &amp;#x3C;S&gt;를 사용해서 Seq2Seq와 동일하게 Inference 한다.&lt;/td&gt;
&lt;td&gt;inference word가 &amp;#x3C;E&gt; (end point)이면&lt;br /&gt;inference를 멈춘다.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;https://miro.medium.com/max/946/1*JMTk-0Ky6fZh2RNFd8mghQ.png&quot; alt=&quot;Image for post&quot;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;https://miro.medium.com/max/1554/1*mmNgbfouHqgTQDY1vEAYNw.png&quot; alt=&quot;Image for post&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;출처: &lt;a href=&quot;https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;platfarm tech team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&quot;chatbot-code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#chatbot-code&quot; aria-label=&quot;chatbot code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chatbot Code&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(2020-08-14) (data set)를 transformer 모델에 학습한 챗봇 만들기 미니 프로젝트&lt;/p&gt;
&lt;p&gt;Created by jynee &amp;#x26; &lt;a href=&quot;https://github.com/molo6379&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;molo6379&lt;/a&gt; &amp;#x26; &lt;a href=&quot;https://github.com/hayjee&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;hh&lt;/a&gt; &amp;#x26; &lt;a href=&quot;https://github.com/Jude0124&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Dabi&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/CyberZHG/keras-transformer&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Zhao HG&lt;/a&gt; keras transformer 코드 응용&lt;/p&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;데이터 입력&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; keras_transformer &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; get_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; decode
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; pickle
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; warnings
warnings&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;filterwarnings&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;ignore&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;tensorflow&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# 단어 목록 dict를 읽어온다.&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;./dataset/6-1.vocabulary.pickle&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;rb&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; f&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    word2idx&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  idx2word &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pickle&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;f&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;token comment&quot;&gt;# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 읽어온다.&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;./dataset/6-1.train_data.pickle&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;rb&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; f&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    trainXE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; trainXD&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; trainYD &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pickle&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;f&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	
&lt;span class=&quot;token comment&quot;&gt;# 평가 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;./dataset/6-1.eval_data.pickle&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;rb&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; f&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    testXE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; testXD&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; testYD &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pickle&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;f&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;model 빌드&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; get_model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
    token_num&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;word2idx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;word2idx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    embed_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    encoder_num&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    decoder_num&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    head_num&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    hidden_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    dropout_rate&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    use_same_embed&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token comment&quot;&gt;# Use different embeddings for different languages&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;adam&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;sparse_categorical_crossentropy&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;model load or fit(학습)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;LOAD_MODEL &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; LOAD_MODEL&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    MODEL_PATH &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;./dataset/transformer.h5&apos;&lt;/span&gt;
    model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load_weights&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;MODEL_PATH&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;token keyword&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
    x&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;trainXE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; trainXD&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    y&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;trainYD&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    epochs&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    batch_size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;save_weights&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;MODEL_PATH&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;predict 함수 정의&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;ivec_to_word&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;q_idx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    decoded &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; decode&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
        model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        q_idx&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        start_token&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;word2idx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;START&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        end_token&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;word2idx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;END&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        pad_token&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;word2idx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;PADDING&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    decoded &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos; &apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;join&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;lambda&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; idx2word&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; decoded&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; decoded&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;chatbot &lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;MAX_SEQUENCE_LEN &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;chatting&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        question &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;Q: &apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt;  question &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;quit&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;token keyword&quot;&gt;break&lt;/span&gt;
        
        q_idx &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; x &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; question&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;split&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos; &apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; x &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; word2idx&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
                q_idx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;append&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;word2idx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;token keyword&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
                q_idx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;append&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;word2idx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;UNKNOWN&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;token comment&quot;&gt;# out-of-vocabulary (OOV)&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# &amp;lt;PADDING&gt;을 삽입한다.&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;q_idx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt; MAX_SEQUENCE_LEN&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            q_idx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;extend&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;word2idx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;PADDING&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;MAX_SEQUENCE_LEN &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;q_idx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            q_idx &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; q_idx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;MAX_SEQUENCE_LEN&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
        
        answer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; ivec_to_word&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;q_idx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;A: &apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; answer&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

chatting&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;units-of-words&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#units-of-words&quot; aria-label=&quot;units of words permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;units of words&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;등장 배경:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;FastText를 사용한다면, n-gram character 방식&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;단점 &gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;word2vec의 oov 문제를 해결하지만, &lt;strong&gt;collision 문제 발생&lt;/strong&gt;&lt;br&gt;&lt;/li&gt;
&lt;li&gt;hash 사용한다면, hash-table 크기를 작게 잡으면 collision 발생&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;참고: DB에서는,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Collision 문제를 해결하기 위해서 &lt;strong&gt;별도의 overflow page를 사용&lt;/strong&gt;한다&lt;/li&gt;
&lt;li&gt;overflow page가 늘어나면 검색 속도가 떨어진다&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;overflow page가 임계치를 초과하면 &lt;strong&gt;hash-table을 늘려서&lt;/strong&gt; DB를 재구성&lt;/p&gt;
&lt;p&gt;=&gt; &quot;Reorganization&quot; 따라서 기존의 &lt;strong&gt;단어들의 위치가 변경&lt;/strong&gt;된다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NLP 분석 시엔 Reorganization하면 단어들의 위치가 변경되므로 다시 학습시켜야 한다. 따라서 근본적인 해결책이 되진 못한다.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;Word Piece(WPM)&lt;/code&gt;, &lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Sentence Piece(SPM)&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NLP에서 Sub-word 구축할 때 사용&lt;/li&gt;
&lt;li&gt;형태소 분석과 달리 언어의 특성에 구애 받지 않아 아무 언어나(한글/영어 등) 사용이 가능하다.&lt;/li&gt;
&lt;li&gt;빈도가 높은 문자열들은 하나의 &lt;code class=&quot;language-text&quot;&gt;unit&lt;/code&gt;으로 취급하여 사전에 등록해 사용한다.&lt;/li&gt;
&lt;li&gt;&apos;unit&apos;: 의미를 가진 문자는 아니고 음절이라기엔 두 음절을 하나로 보니 unit이란 이름을 사용한다.&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;구글은 자신들의 구글 번역기에서 WPM이 어떻게 수행되는지에 대해서 기술했다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;WPM을 수행하기 이전의 문장&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
Jet makers feud over seat width &lt;span class=&quot;token keyword&quot;&gt;with&lt;/span&gt; big orders at stake
WPM을 수행한 결과&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;wordpieces&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
_J et _makers _fe ud _over _seat _width _with _big _orders _at _stake&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Jet는 J와 et로 나누어졌으며, feud는 fe와 ud로 나누어진 것을 볼 수 있다. WPM은 입력 문장에서 기존에 존재하던 띄어쓰기는 언더바로 치환하고, 단어는 내부단어(subword)로 통계에 기반하여 띄어쓰기로 분리한다.&lt;/p&gt;
&lt;p&gt;기존의 띄어쓰기를 언더바로 치환하는 이유는 &lt;strong&gt;차후 다시 문장 복원을 위한 장치&lt;/strong&gt;이다. WPM의 결과로 나온 문장을 보면, 기존에 없던 띄어쓰기가 추가되어 내부 단어(subwords)들을 구분하는 구분자 역할을 하고 있으므로 본래의 띄어쓰기를 언더바로 치환해놓지 않으면, 기존 문장으로 복원할 수가 없다.&lt;/p&gt;
&lt;p&gt;WPM이 수행된 결과로부터 다시 수행 전의 결과로 돌리는 방법은 현재 있는 띄어쓰기를 전부 삭제하여 내부 단어들을 다시 하나의 단어로 연결시키고, 언더바를 다시 띄어쓰기로 바꾸면 된다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;출처: &lt;a href=&quot;https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-5%EC%9D%BC%EC%B0%A8-%EB%8B%A8%EC%96%B4-%EB%B6%84%EB%A6%AC-60b59f681eb7&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;정민수&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;pre-trained 된 &lt;a href=&quot;https://github.com/SKTBrain/KoBERT&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;SKTBrain의 KoBert&lt;/a&gt;를  fine tuning으로 사용하면 쉽게 SentencePiece를 만들 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;WPM, SPM&lt;/li&gt;
&lt;li&gt;SPM은 GOOGLE이 C++로 만들어서 속도가 빠르다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(단점) 조사도 쪼개서 보기 때문에 챗봇 만들 때 답변으로 잘못된 조사가 붙여 나올 수 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ex: 번역 api 사용 시 부자연스러운 조사&lt;/li&gt;
&lt;li&gt;개선 방법: (1) 데이터 양을 늘린다. (2) BERT를 사용한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;[나는 학교에 간다]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;단어와 단어를 구분하는 단어의 띄어쓰기에만 언더바 붙이기 &quot;나는_학교에_간다&quot;&lt;/li&gt;
&lt;li&gt;&apos;_학교&apos; → 사전에 등록&lt;/li&gt;
&lt;li&gt;&apos;_나&apos; → 사전에 등록&lt;/li&gt;
&lt;li&gt;&apos;는&apos; → 사전에 등록&lt;/li&gt;
&lt;li&gt;&apos;간다&apos; → 사전에 등록&lt;/li&gt;
&lt;li&gt;[&apos;_학교&apos;, &apos;_나&apos;, &apos;는&apos;, &apos;간다&apos;]&lt;/li&gt;
&lt;li&gt;[5, 4, 61,12]&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;BPE&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;데이터 압축기술.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;구글의 WPM에는 BPE(Byte Pair Encoding) 알고리즘이 사용되었다.&lt;/p&gt;
&lt;p&gt;BPE 알고리즘의 기본 원리는 가장 많이 등장한 문자열에 대하여 병합하는 작업을 반복하는 데, 원하는 단어 집합의 크기. 즉, 단어의 갯수가 될 때까지 이 작업을 반복한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;출처: &lt;a href=&quot;https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-5%EC%9D%BC%EC%B0%A8-%EB%8B%A8%EC%96%B4-%EB%B6%84%EB%A6%AC-60b59f681eb7&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;정민수&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;blockquote&gt;
&lt;p&gt; 예&gt; &lt;/p&gt;
&lt;p&gt; [abcabtabsta]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&apos;ab&apos; → 사전에 등록&lt;/li&gt;
&lt;li&gt;&apos;abc&apos; → 사전에 등록&lt;/li&gt;
&lt;li&gt;&apos;r&apos; → 사전에 등록&lt;/li&gt;
&lt;li&gt;&apos;t&apos; → 사전에 등록&lt;/li&gt;
&lt;li&gt;&apos;u&apos; → 사전에 등록&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;참고:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;아마추어 퀀트, blog.naver.com/chunjein&lt;/li&gt;
&lt;li&gt;platfarm tech team. 2019.05.11. &quot;어텐션 메커니즘과 transfomer(self-attention)&quot;. &lt;a href=&quot;https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://medium.com/platfarm/어텐션-메커니즘과-transfomer-self-attention-842498fd3225&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zhao HG. &quot;keras-bert&quot;. &lt;a href=&quot;https://github.com/CyberZHG/keras-bert&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://github.com/CyberZHG/keras-bert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;정민수. 2019.06.07. &quot;자연어처리(NLP) 5일차 (단어 분리)&quot;. &lt;a href=&quot;https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-5%EC%9D%BC%EC%B0%A8-%EB%8B%A8%EC%96%B4-%EB%B6%84%EB%A6%AC-60b59f681eb7&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-5%EC%9D%BC%EC%B0%A8-%EB%8B%A8%EC%96%B4-%EB%B6%84%EB%A6%AC-60b59f681eb7&lt;/a&gt;. @omicro03&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[NLP Sequence to Sequence]]></title><description><![CDATA[NLP  NLP, Time Serise/Sin data 예측에 사용한다. 기법 종류: seq2seq (RNN) > SL(fine Tuning) → USL Attention: RNN 기반 > SL(fine Tuning) → USL Self…]]></description><link>https://jynee.github.io/tags#1st/NLP_Seq2Seq/</link><guid isPermaLink="false">https://jynee.github.io/tags#1st/NLP_Seq2Seq/</guid><pubDate>Sat, 15 Aug 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;nlp&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#nlp&quot; aria-label=&quot;nlp permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP&lt;/h1&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-textsequence-to-sequencecode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textsequence-to-sequencecode&quot; aria-label=&quot;code classlanguage textsequence to sequencecode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;Sequence to Sequence&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NLP, Time Serise/Sin data 예측에 사용한다.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;기법 종류:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;seq2seq (RNN) &gt; SL(fine Tuning) → USL&lt;/li&gt;
&lt;li&gt;Attention: RNN 기반 &gt; SL(fine Tuning) → USL&lt;/li&gt;
&lt;li&gt;Self Attention(TransFormer): RNN 제거하고 Attention만 사용 &gt; SL(fine Tuning) → USL&lt;/li&gt;
&lt;li&gt;이유: 병렬 처리 곤란(1 Step 끝나고 다음 Step 진행하기 때문에), GV 발생&lt;/li&gt;
&lt;li&gt;BERT&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RNN  Encoder - Decoder&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Encoder : 발화자의 의도&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;최종 출력 context vector : h, c&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decoder : 청취자가 발화자의 의도 해석&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decoder의 입력과 출력은 모두 Answer 문장이다.&lt;/li&gt;
&lt;li&gt;다만 여기서 입력은 태그로 &amp;#x3C;start&gt;가 붙고&lt;/li&gt;
&lt;li&gt;출력은 &amp;#x3C;end&gt;가 붙는다&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;teacher forcing&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;학습 시에는 입력값과 출력값을 모두 알기 때문에 한꺼번에 넣어 학습시킨다. 한 음절씩 입력할 필요가 없다.&lt;/li&gt;
&lt;li&gt;predict 할 때는 출력값을 모르기 때문에 &amp;#x3C;start&gt;만 넣고 1 step(음절)씩 출력되도록 for문을 돌린다. &lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/8ed9830cf42700b96e2d7dda0d04bfce/05fb0/image-20200811115522365.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 48.64864864864865%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAACF0lEQVQozzWSC2+jMBCE+f+/rDrp1KZt2rwgQEJ4Y8AY8wrfLZFuJcv2yruenRmn0x1FXtLUintoiSP72m+BkbORvCX0DVfPkMYG3+v4+W4pi4FxsARBQBzHhIFPdA9xrDFkaUKaZFKUElwzOae0TcdgZ+Zx4nffSd6y++il4cD9PuGdDFWu0FXDPE0Y+0SpAcd0C2094Z6trAHPlf1iBOXI4zGzxfk88+dNs/ubUGWKplIcPm5kUc7QWUE6kO/27P8UOKpcKIpZHg6oLKcrlaCaWaXRXBb07+/kXsDj5JJezjTZHRXfaJMrRjdooaw3PZX3yyB3hycUbcijPONGX0SJh598Y/tBOgr6KsXz3xj2Lu2pRt8sXdRTnzRN2bBpMI4j9bFjHcHJYnDDiCiL8B8+SZ5w9M4UmaUS4tveEEbfjJcbzVVR3wytiKXcUhAaGXdkmWdqV7EI5w4LZMonTD2OwQFflDoEe1o1vciu2hLv+sa4IfzJ0UGLvmvqQ41pDYPwZ+1A8VWx9E+cXq+4J8XxqITLmqoyfH7mHPetkBxRRhnhxUfHov7uQhFrUVdGLrVwZzHikkmcoJUWtJMgXFdqtZIli/hswb8OHIQPVc0855VVJngVCFd1LTm5b0sPhl7QbVFks9RNlPmCsz5X/kfbPMXkM3bcfh1fua2ZKisxbfTal0msJCVtt4kxvd7o9ik+ndh6/QPoLvPSpWU87QAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200811115522365&quot;
        title=&quot;image-20200811115522365&quot;
        src=&quot;/static/8ed9830cf42700b96e2d7dda0d04bfce/fcda8/image-20200811115522365.png&quot;
        srcset=&quot;/static/8ed9830cf42700b96e2d7dda0d04bfce/12f09/image-20200811115522365.png 148w,
/static/8ed9830cf42700b96e2d7dda0d04bfce/e4a3f/image-20200811115522365.png 295w,
/static/8ed9830cf42700b96e2d7dda0d04bfce/fcda8/image-20200811115522365.png 590w,
/static/8ed9830cf42700b96e2d7dda0d04bfce/efc66/image-20200811115522365.png 885w,
/static/8ed9830cf42700b96e2d7dda0d04bfce/05fb0/image-20200811115522365.png 1138w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;그림 출처: 아마추어 퀀트, blog.naver.com/chunjein&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;설명&lt;/th&gt;
&lt;th&gt;특징&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;h&lt;/td&gt;
&lt;td&gt;i와 f를 통해 나온 &lt;strong&gt;결과값(출력값)을 얼만큼 사용할 것인가 조절&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;f와 i의 가중평균 형태&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;이전(과거)과 현재 값들을 얼만큼 사용할 것인가 조절&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;i&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;이전&lt;/strong&gt;의 C를 얼마나 반영할 것인지 조절&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;f&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;현재 입력값(x)과 이전의 출력값(h)&lt;/strong&gt;를 얼마나 반영할 것인지 조절&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/9907a690f71dfd776cd32bb659403497/b880f/image-20200811182503171.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 35.13513513513513%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAAA+klEQVQoz11R2W6AMAzr///jXtAmHrjpfYAXh5VpC4oawHEc19z4G9d14b5vxBjRWtP6N255LpRSEEKA915xKSU9GUa60ATgnMO2bQom0FqrNYHHcSCGBBcXyQ/4sOE8reSpOPZywEMoClqt7xQqIpAkDIKHYUDOWQYklLZLs1MiCuBJAcwqPIYEfUUCSDRNE8Zx1JoNzMeOZ/EQHuy6rnoyaQ/T0DMW+75jWZY353lWQg6rtaDkipBW+PIF53ecx6k4YrguN3wJu2+UzNWYBPCd/59v4mfe4PInQrSqkiKovl8kT8MVSEjfurHvnf7Y8T9ITgF9MOt+69/qCiFJ6r4J6wAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200811182503171&quot;
        title=&quot;image-20200811182503171&quot;
        src=&quot;/static/9907a690f71dfd776cd32bb659403497/fcda8/image-20200811182503171.png&quot;
        srcset=&quot;/static/9907a690f71dfd776cd32bb659403497/12f09/image-20200811182503171.png 148w,
/static/9907a690f71dfd776cd32bb659403497/e4a3f/image-20200811182503171.png 295w,
/static/9907a690f71dfd776cd32bb659403497/fcda8/image-20200811182503171.png 590w,
/static/9907a690f71dfd776cd32bb659403497/efc66/image-20200811182503171.png 885w,
/static/9907a690f71dfd776cd32bb659403497/c83ae/image-20200811182503171.png 1180w,
/static/9907a690f71dfd776cd32bb659403497/b880f/image-20200811182503171.png 1507w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;그림 출처: &lt;a href=&quot;blog.naver.com/chunjein&quot;&gt;아마추어 퀀트&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;h2 id=&quot;code--원리&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code--%EC%9B%90%EB%A6%AC&quot; aria-label=&quot;code  원리 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;code &amp;#x26; 원리&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;입력 데이터가 2개, 출력 데이터는 1개로 총 3개의 데이터가 필요하다&lt;/li&gt;
&lt;li&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;encoder&lt;/th&gt;
&lt;th&gt;decoder&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;many-to-one&lt;/td&gt;
&lt;td&gt;many-to-many&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;return_state&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Seq2Seq의 LSTM&lt;/th&gt;
&lt;th&gt;기존 LSTM&lt;/th&gt;
&lt;th&gt;차이&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LSTM()&lt;/td&gt;
&lt;td&gt;LSTM(LSTM&lt;em&gt;HIDDEN, return&lt;/em&gt;sequences=True, &lt;strong&gt;return_state = True&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;LSTM(LSTM&lt;em&gt;HIDDEN, return&lt;/em&gt;sequences=True)&lt;/td&gt;
&lt;td&gt;&lt;em&gt;return_state = True&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;return_state = True&lt;/em&gt; : 사용하면, 중간 뉴런들 각각의 값을 뜻하는, 결과값으로서의 계산된 h와 c가 함께 출력되어 &lt;strong&gt;출력값이 3개&lt;/strong&gt;가 나오게 된다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;decoder는 입력을 한꺼번에 받았으나, chat module에선 한 단어씩 입력받아야 한다. 따라서 입력부분의 shape이 달라진다. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;n개 → 1&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;그래도 네트워크의 파라미터는 동일하다.&lt;/li&gt;
&lt;li&gt;순서:&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;encoder&lt;/code&gt; &amp;#x26; &lt;code class=&quot;language-text&quot;&gt;decoder&lt;/code&gt; 네트워크를 구성한다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;encoder&lt;/code&gt;: many-to-one으로 구성한다. 중간 출력은 필요 없고 decoder로 전달할 h와 c만 필요하다. h와 c를 얻기 위해 &lt;code class=&quot;language-text&quot;&gt;return_state = True&lt;/code&gt;를 설정한다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;decoder&lt;/code&gt;: many-to-many로 구성한다. target을 학습하기 위해서는 중간 출력이 필요하다. 그리고 초기 h와 c는 encoder에서 출력한 값을 사용한다&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;chatting 용 모델&lt;/code&gt; &lt;strong&gt;따로&lt;/strong&gt; 빌드: &lt;em&gt;&quot;&lt;/em&gt; 학습 모델(encoder &amp;#x26; decoder)에서 나온 결과(w값)를 적용한다. &lt;em&gt;&quot;&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;chatting 용 모델&lt;/em&gt; &gt; 사용자가 있는 ChatBot으로서 &lt;strong&gt;실사용할 것.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;네트워크 구성&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;encoder&lt;/code&gt; &amp;#x26; &lt;code class=&quot;language-text&quot;&gt;decoder&lt;/code&gt;  만들기.&lt;/p&gt;
&lt;p&gt;학습 때와 달리 문장 전체를 받아 recurrent하는 것이 아니라, &lt;strong&gt;단어 1개씩 입력 받아서 다음 예상&lt;/strong&gt; 단어를 확인한다&lt;/p&gt;
&lt;p&gt;따라서, input shape의 변화 말고는 가중치를 뽑아내기 위해 실행했었던 &apos;6-2. 파일&apos; 속 encoder &amp;#x26; decoder의 구성방식(층 개수, 정규화 했다면 정규화, loss 등)과 전부 같아야 한다.&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;Chatting용 model&lt;/code&gt; 만듦&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chatting용 model은 encoder&amp;#x26;decoder랑 달리 네트워크를 만드는 게 아니라, &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;실제 사용할 model을 만들므로 Encoder 및 decoder의 &lt;strong&gt;w를 써야하므로 그 둘을 합쳐준다는 의미&lt;/strong&gt;에서 model을 만든다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;따라서 Encoder의 출력을 입력으로 받기 위한 입력 input(예: ih1, ic1)을 만들어준다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;py&quot;&gt;&lt;pre class=&quot;language-py&quot;&gt;&lt;code class=&quot;language-py&quot;&gt;ih1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_shape &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; LSTM_HIDDEN&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
ic1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_shape &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; LSTM_HIDDEN&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
ih2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_shape &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; LSTM_HIDDEN&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
ic2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_shape &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; LSTM_HIDDEN&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Decoder에 넣으면 변하는 부분을 만든다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;dec_output1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dh1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dc1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; decLSTM1&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;decEMB&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; initial_state &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;ih1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; ic1&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;앞에서 encoder&amp;#x26;decoder model을 만들었으니, &lt;/p&gt;
&lt;p&gt;바로 위 code 中 Encoder의 입력 값(ih1, ic1)에서 w가 나올 텐데,  &lt;/p&gt;
&lt;p&gt;decoder의 가중치 값으로 쓴다(initial_state).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Question을 입력받아 Answer를 생성 해주는 알고리즘 작성(함수 만들기)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;궁금한 부분 &amp;#x26; 해소:&lt;/p&gt;
&lt;p&gt;&quot;챗봇의 Answer에서 &apos;그 사람도 그럴 거예요&apos; 中 &apos;거예요&apos; 다음에 문장이 또 나올 수 있지 않나? 어떻게 멈추는 거지?&quot;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;답변: 챗봇 알고리즘을 다음 예상 단어가 &amp;#x3C;END&gt;이거나 &amp;#x3C;PADDING&gt;이면 더 이상 예상할 게 없도록 만들었기 때문.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;argmax로-해당-단어를-채택한다&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#argmax%EB%A1%9C-%ED%95%B4%EB%8B%B9-%EB%8B%A8%EC%96%B4%EB%A5%BC-%EC%B1%84%ED%83%9D%ED%95%9C%EB%8B%A4&quot; aria-label=&quot;argmax로 해당 단어를 채택한다 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;argmax로 해당 단어를 채택한다.&lt;/h1&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  nextWord = np.argmax(dY[0, 0])

  # 예상 단어가 &amp;lt;END&amp;gt;이거나 &amp;lt;PADDING&amp;gt;이면 더 이상 예상할 게 없다.
  if nextWord == word2idx[&amp;#39;&amp;lt;END&amp;gt;&amp;#39;] or nextWord == word2idx[&amp;#39;&amp;lt;PADDING&amp;gt;&amp;#39;]:&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;  break&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chatting 실행하는 함수 작성&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;글 쓰는 input과 글 나오는 output이 부분을 만듦&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&quot;smt&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#smt&quot; aria-label=&quot;smt permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SMT&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Answer는 One-Hot encoding 형태로 출력&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;따라서 softmax 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이전 단어들의 Sequence로 예측할 단어를 구함&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;식에선 이전 단어가 첫 번째로 위치할 확률 등을 구함&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-textseq2seqcode--code-classlanguage-textattentioncode-메커니즘&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textseq2seqcode--code-classlanguage-textattentioncode-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98&quot; aria-label=&quot;code classlanguage textseq2seqcode  code classlanguage textattentioncode 메커니즘 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;Seq2Seq&lt;/code&gt; &amp;#x26; &lt;code class=&quot;language-text&quot;&gt;Attention&lt;/code&gt; 메커니즘&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;입력 문장이 긴 상황에서는 번역 품질이 떨어지는 현상이 나타났고, 이런 현상을 보정하기 위해 &lt;strong&gt;중요한 단어에 집중&lt;/strong&gt;하여 Decoder에 바로 전달하는 Attention 기법이 등장했습니다. 기존 방식보다 &lt;strong&gt;훨씬 더 많은 데이터를 Decoder에 전달&lt;/strong&gt;합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;출처: [glee1228][https://glee1228.tistory.com/3]&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;그림:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/d22822b3343784286ba2ec07b79e1700/0f882/image-20200812105423228.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 70.27027027027026%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACZ0lEQVQ4y4VUi26cMBC8//+j/kHVqEnURJfkmgMODo6Hz8YvMM/p2iRt1Eaq0QIyy+zs7MBuXVf4tS4r9vs9Xg4HtELgs+Vz30MpBU15xoeUaNsWzjns/MNhcOitRfT6SnEEqxtMfY8yyxAlCdI0RU/JH9c8z5jHEcs0YyEy4zSFvQB4vV4h2BWOQGZ6kKUZXp6ecDoecbkU0NqgNwbD2MNZDS05BtdjIrCe3rFKUIdzKLTzp1YqKIppGMOmpaT0lMLJFmPP0VGckwfw4x6GX2B5gaFT8GpN0wjDcmK7dbCbiW5V5ojjA4Hy0EYVx4jubzEuS2BxrY74evcF+c13FGWF4lJCkGbruvyjcWDIWI0siSE5w6w5XFUgen5A9PgIRjr2BILOYd3eCm12XReGMBIBvA0qMKzqClpp0siiE5JAaWqsQnZ4wu3NN6THiHTrwH7cg1PRgbRyNEQPaEhXf13oWLGB7vJzjpZEbUiXukxx5SVqCkl7XWfBxAVN20CcKoisIptQYd5Bt5q036ximcVGnxhaX11UeE7ukDY/UVwjRAWJrwxG1yFvXpHnt+gTAX22sBQyltCNhqNBTOQKExmY1mAhTXd+uh7wVD0jZ2QTniAj4MGRx2YamMjQUJE+biEzBZMbyERCNYpsNAQNdazJdmLzoSHtxtlRWzm1moPJEhU/w5ouaFvUKWqWQZ0FREH61gaqVOgkaWjJm8OA9tzCartp6D8hSZ+Opha947f5I1hiIdvgz9Ym/NvG+xBC0OHbDYC+AmMMnJMH5+l38n/X+vn97rNcz8zr8fH69/3vn8qHH4aPX7WYNeRKKDcNAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200812105423228&quot;
        title=&quot;image-20200812105423228&quot;
        src=&quot;/static/d22822b3343784286ba2ec07b79e1700/fcda8/image-20200812105423228.png&quot;
        srcset=&quot;/static/d22822b3343784286ba2ec07b79e1700/12f09/image-20200812105423228.png 148w,
/static/d22822b3343784286ba2ec07b79e1700/e4a3f/image-20200812105423228.png 295w,
/static/d22822b3343784286ba2ec07b79e1700/fcda8/image-20200812105423228.png 590w,
/static/d22822b3343784286ba2ec07b79e1700/efc66/image-20200812105423228.png 885w,
/static/d22822b3343784286ba2ec07b79e1700/c83ae/image-20200812105423228.png 1180w,
/static/d22822b3343784286ba2ec07b79e1700/0f882/image-20200812105423228.png 1584w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;t = 단어 하나&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;원리 및 작동 순서:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;markdown-images/img.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/8affbb6e7586e1214333a84d06cff3cb/6af66/img2.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 68.91891891891892%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACj0lEQVQ4y3WUXUsUURjH/Qhd1EUfoJs+QiBY9AmKoAsJu/EmCgIjxDJvSoOQXiwrE6O8ERFRJOxlsexiJSIt3F13R2fG3XV3XnZedmZ3XnZn5t/Zc1TUqQPPPD/OPM//PPOcc6YtiiKEYQjX84EWE4t2jTEYhwTIMAwDnudRjqKQvmPMfFvrETSb4DbWsZtzKID5PQMkWUYqnYZTd2KxVDAIAlRtE394Hgh9GC5geSygZDHvNAG1FlJWCxyKW+uUg8Ah5h2uUJMK+Lou4tSQjJqcRuekgYdfDKhyGacfmVAMGyNLKi6MawjhYOZ6Nz7290JzDGxx36GQnCOfHODzbxEnB32owg+cfy7h7gcDIr+J4wMmCpKGgbk8zr0w4Pg6JjsvY/H2LYiygNxGErKUifdwNcej6/VPMlvF6JKCuV8K7dmNyU3auW9ZC0PzAk1IvHqMxPgIZZFfgSzn4oINrwZ1e5VO+JYK19Io2zITifw6LCXPemhVsKPuUFZ0A6ZlxwUzJQeXJiotafQvqHiX1OG7Di6OleA2QsyuVXFzqkgTkqMvsTL2hvKnvj5kEwnKIdncfcEk7+DEfRdWKYuzz8ronddIf9I4dq8KPl/GnRkB7U8rqPsWprquYrGnByo5u287OpCeno4LruVttD+RaIXXpiSMLrMKzwwXSYUR3q8YuDJRYD18MIjl4WHKs93dSC0sxAVd10Nqg6MTO2WZHF6VcirDGl53XHC8yA52qYRtkXGe+Iqq/mNTGg2Yhk4nioUCKhUWpCoyTNOEKAjgclmysIsmuVW1en3/1ui6HhdsBe3dT4GsqmksSNM0SJKEdCaDHMfBtm2a2CqACpbLpBDjsODBe/g/PjiO/gyO8l/Vdgfx+n2l4QAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;img&quot;
        title=&quot;img&quot;
        src=&quot;/static/8affbb6e7586e1214333a84d06cff3cb/fcda8/img2.png&quot;
        srcset=&quot;/static/8affbb6e7586e1214333a84d06cff3cb/12f09/img2.png 148w,
/static/8affbb6e7586e1214333a84d06cff3cb/e4a3f/img2.png 295w,
/static/8affbb6e7586e1214333a84d06cff3cb/fcda8/img2.png 590w,
/static/8affbb6e7586e1214333a84d06cff3cb/6af66/img2.png 640w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;위 그림을 바탕으로 어떻게 Attention이 작용하는지 설명해보겠습니다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;우선, Attention Decoder에서 나오는 첫 단어(위의 Je)를 만들기 위해 두가지를 준비해야합니다.&lt;/p&gt;
&lt;p&gt;하나는 Attention Vector, 나머지 하나는 Step 5번째의 Decoder(빨간색)에서 나오는 hidden state입니다.&lt;/p&gt;
&lt;p&gt;우선 Attention Decoder에 전달할 Context Vector를 만들어봅니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Encoder의 hidden state(h1,h2,h3,h4h1,h2,h3,h4)들을 step별로 구합니다.&lt;/li&gt;
&lt;li&gt;각각 step의 hidden state(h1,h2,h3,h4h1,h2,h3,h4)에 이전 step 디코더의 hidden state인 si−1si−1 를 각각 dot-product하거나 다른 socre 함수들을 사용해서 점수를 부여합니다.이 점수가 바로 Attention Score입니다.&lt;/li&gt;
&lt;li&gt;점수를 softmax합니다.(점수 합이 1)&lt;/li&gt;
&lt;li&gt;Softmax된 점수에 해당하는 각각의 hidden state들을 곱해줍니다.&lt;/li&gt;
&lt;li&gt;점수에 곱해진 Vector들을 Sum up 해줍니다. =&gt; Context Vector&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Step 5번째의 첫 Decoder의 hidden state를 준비해줍니다.&lt;/p&gt;
&lt;p&gt;여기까지 다 되었다면, 위 Je라는 Attention Decoder의 첫 hidden state를 내보낼 준비가 되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;어텐션의 기본 아이디어는 &lt;strong&gt;디코더(Decoder)에서 출력 단어를 예측하는 매 시점(time-step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점&lt;/strong&gt; 입니다. 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중해서 보게됩니다. 이 내용을 알고 Decoder의 단계로 넘어갑니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;출처: &lt;a href=&quot;https://glee1228.tistory.com/3&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;glee1228&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&quot;code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code&quot; aria-label=&quot;code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;code&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;순서&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;단어 목록 dict를 읽어온다.&lt;/li&gt;
&lt;li&gt;학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 읽어온다.&lt;/li&gt;
&lt;li&gt;평가 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이때 쓸 attention layer 및 attention score 구하기&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Encoder 출력과 decoder 출력으로 &lt;code class=&quot;language-text&quot;&gt;attention value&lt;/code&gt;를 생성하고,
decoder 출력 + attention value를 &lt;code class=&quot;language-text&quot;&gt;concatenate&lt;/code&gt;한다.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# LSTM time step = 4, SMB_SIZE = 3&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;Attention&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# x : encoder 출력, y : decoder 출력&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# step-1:&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# decoder의 매 시점마다 encoder의 전체 시점과 dot-product을 수행한다.&lt;/span&gt;
    score &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dot&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;axes&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;y&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;                   &lt;span class=&quot;token comment&quot;&gt;# (1, 4, 4)&lt;/span&gt;
    
    &lt;span class=&quot;token comment&quot;&gt;# step-2:&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# dot-product 결과를 확률분포로 만든다 (softmax)&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# 이것이 attention score이다.&lt;/span&gt;
    dist &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Activation&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;softmax&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;score&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;                &lt;span class=&quot;token comment&quot;&gt;# (1, 4, 4)&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# step-3:   &lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# encoder 출력과 attention score를 각각 곱하는 단계 &lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# encoder의 전체 시점에 위의 확률 분포를 적용해서 가중 평균한다.&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# 직접 계산이 어렵기 때문에 dist를 확장하고, 열을 복제해서&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# Dot 연산이 가능하도록 trick을 쓴다.&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# 이것이 attention value이다.&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# dist_exp = K.expand_dims(dist, 2)                   # (1, 4, 1, 4)&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# dist_rep = K.repeat_elements(dist_exp, EMB_SIZE, 2) # (1, 4, 3, 4)                                       &lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# dist_dot = Dot(axes=(3, 1))([dist_rep, x])          # (1, 4, 3, 3)&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# attention = K.mean(dist_dot, axis = 2)              # (1, 4, 3)&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# step-4:&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# 교재의 step-3을 계산하지 않고 step-4를 직접 계산했다.&lt;/span&gt;
    attention &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dot&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;axes&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;dist&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;token comment&quot;&gt;# step-5:&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# decoder 출력과 attention (score)을 concatenate 한다.&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; Concatenate&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;y&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; attention&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;    &lt;span class=&quot;token comment&quot;&gt;# (1, 4, 6)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
 &lt;br&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;워드 임베딩 레이어. Encoder와 decoder에서 공동으로 사용한다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;공동으로 사용함을 가정&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;clear_session&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
wordEmbedding &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Embedding&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;input_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;VOCAB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; output_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;EMB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
 &lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;many-to-many로 구성한다. Attention value를 계산하기 위해 중간 출력이 필요하고 (return_sequences=True), &lt;/li&gt;
&lt;li&gt;decoder로 전달할 h와 c도 필요하다 (return_state = True)&lt;/li&gt;
&lt;/ul&gt;
 &lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;many-to-many로 구성한다. target을 학습하고 Attention을 위해서는 &lt;strong&gt;중간 출력이 필요&lt;/strong&gt;하다. &lt;/li&gt;
&lt;li&gt;그리고 초기 h와 c는 encoder에서 출력한 값을 사용한다(initial_state). &lt;/li&gt;
&lt;li&gt;최종 출력은 vocabulary의 인덱스인 one-hot 인코더이다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Attention 함수를 삽입한다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;att_dy2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Attention&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ey2&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dy2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# ey2: encoder 출력값 # dy2: decoder 출력값&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;activation=&apos;softmax&apos;를 사용한다&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;decOutput &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; TimeDistributed&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;VOCAB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; activation&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;softmax&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
  &lt;br&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;7. Model

   * loss=&amp;#39;sparse_categorical_crossentropy&amp;#39;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
  &lt;br&gt;
&lt;ol start=&quot;8&quot;&gt;
&lt;li&gt;
&lt;p&gt;학습 (teacher forcing)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;주의:&lt;/p&gt;
&lt;p&gt;loss = sparse&lt;em&gt;categorical&lt;/em&gt;crossentropy이기 때문에 target을 one-hot으로 변환할 필요 없이 integer인 trainYD를 그대로 넣어 준다. trainYD를 one-hot으로 변환해서 categorical_crossentropy로 처리하면 out-of-memory 문제가 발생할 수 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
  &lt;br&gt;
&lt;ol start=&quot;9&quot;&gt;
&lt;li&gt;
&lt;p&gt;학습 결과를 저장한다&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;save_weights&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;MODEL_PATH&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
 &lt;br&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;참고:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;아마추어 퀀트, blog.naver.com/chunjein&lt;/p&gt;
&lt;p&gt;ckdgus1433. 2019. 8. 7. &quot;Attention Mechanism(seq2seq)&quot;. &lt;a href=&quot;http://blog.naver.com/ckdgus1433/221608376139&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;http://blog.naver.com/ckdgus1433/221608376139&lt;/a&gt;. 튜토리얼로 익히는 머신러닝/딥러닝&lt;/p&gt;
&lt;p&gt;ratsgo. 2017.05.12. &quot;Sequence-to-Sequence 모델로 뉴스 제목 추출하기&quot;. &lt;a href=&quot;https://ratsgo.github.io/natural&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://ratsgo.github.io/natural&lt;/a&gt; language processing/2017/03/12/s2s/. ratsgo&apos;s blog for textmining&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[GAN 실전 응용]]></title><description><![CDATA[GAN 1D 정규분포에서 샘플링한 데이터를 모방하여, fake data를 생성한다. fake data는 정규분포의 특성을 갖는다. (KL divergence, 평균, 분산, 왜도, 첨도 등) Discrimi의 loss는 maxlog(Dx) + log…]]></description><link>https://jynee.github.io/tags#1st/GAN_2/</link><guid isPermaLink="false">https://jynee.github.io/tags#1st/GAN_2/</guid><pubDate>Sat, 08 Aug 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;gan&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#gan&quot; aria-label=&quot;gan permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GAN&lt;/h1&gt;
&lt;/br&gt;
&lt;ul&gt;
&lt;li&gt;1D 정규분포에서 샘플링한 데이터를 모방하여, fake data를 생성한다.&lt;/br&gt;&lt;/li&gt;
&lt;li&gt;fake data는 정규분포의 특성을 갖는다. (KL divergence, 평균, 분산, 왜도, 첨도 등)&lt;/br&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discrimi의 loss는 max[log(Dx) + log(1 - DGz)]이고, Generator의 loss는 min[log(Dx + log(1 - DGz))]이다. &lt;/br&gt;&lt;/p&gt;
&lt;/br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tensorflow에서는 이 loss 함수를 이용하여 직접 GAN을 학습할 수 있지만, &lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Keras에서는 model.fit(), model.train&lt;em&gt;on&lt;/em&gt;batch() 함수에서 target 값을 지정해야 하기 때문에 이 loss로 GAN을 학습할 수 없다 &lt;strong&gt;Keras는 기본적으로 Supervised learning 목적이다.&lt;/strong&gt;&lt;/br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keras에서는 supervised learning 방식으로 바꿔 binary_crossentropy loss 함수를 써서 GAN을 학습하는 것이 보통이다.&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이 코드는 아래 자료를 참조해서 응용했다.&lt;/br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Rowel Atienza, 2018, Advanced Deep Learning with Keras. Chap 4. p.107 ~ p.113&lt;/br&gt;&lt;/li&gt;
&lt;li&gt;아마추어 퀀트, blog.naver.com/chunjein,  2020.04.08&lt;/br&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;함수들의 기능을 파악하기 쉽도록 순서를 변경하였다.&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h2 id=&quot;basic-code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#basic-code&quot; aria-label=&quot;basic code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic CODE&lt;/h2&gt;
&lt;/br&gt;
&lt;ul&gt;
&lt;li&gt;Keras를 이용하여 기본 GAN 모델을 연습한다.&lt;/br&gt;&lt;/li&gt;
&lt;li&gt;file: 딥러닝 8-2.GAN(Kears)&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/br&gt;
&lt;h4 id=&quot;step1-정규분포로부터-데이터를-샘플링한다br&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step1-%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC%EB%A1%9C%EB%B6%80%ED%84%B0-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%EC%83%98%ED%94%8C%EB%A7%81%ED%95%9C%EB%8B%A4br&quot; aria-label=&quot;step1 정규분포로부터 데이터를 샘플링한다br permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;step1. 정규분포로부터 데이터를 샘플링한다&lt;/br&gt;&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;realData &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;normal&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
realData &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; realData&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;reshape&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;realData&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;h4 id=&quot;step2-network-빌드&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step2-network-%EB%B9%8C%EB%93%9C&quot; aria-label=&quot;step2 network 빌드 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;step2. Network 빌드&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;nDInput &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; realData&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# nDInput.shape[1] = 1&lt;/span&gt;
nDHidden &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;32&lt;/span&gt;
nDOutput &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;
nGInput &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;16&lt;/span&gt;
nGHidden &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;32&lt;/span&gt;
nGOutput &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; nDInput

&lt;span class=&quot;token comment&quot;&gt;## nDInput와 nGOutput는 값이 같아야 함&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;getNoise&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;m&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; n&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;nGInput&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    z &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;uniform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;m&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; n&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; z&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;MyOptimizer&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;a &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; RMSprop&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lr &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; a&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;h4 id=&quot;step3-모델-그림&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step3-%EB%AA%A8%EB%8D%B8-%EA%B7%B8%EB%A6%BC&quot; aria-label=&quot;step3 모델 그림 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;step3. 모델 그림&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generator --&gt; Discriminator를 연결한 모델을 생성한다.&lt;/li&gt;
&lt;li&gt;아래 네트워크로 z가 들어가면 DGz = 1이 나오도록 G를 학습한다.&lt;/li&gt;
&lt;li&gt;D 네트워크는 업데이트하지 않고, G 네트워크만 업데이트한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/3ecf4bc1296548c228482e5a0a36b43b/4b446/image-20200825092205260.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 22.972972972972975%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsSAAALEgHS3X78AAAA0klEQVQY022Qi27DIAxF8/9f2a5pkraLpoU3GJ9Bsm5TtStZGPB9wMAflKykqD/7lNJeUoWcC9aUtgoiQinCfxiCPwZyEh53YZkL3rXyjuk6Mc8LMUa2T8N4CZgtEoJnXdfWB2KoTbzgbOdVhmWOOGe53zxvZ8/ptDGNfTCiqmhVaq17pXiYt2OKZC7nlVvjG2M4d941MPRndmiFj1V5f2hLW1uq0IxcSxOw1jJNPe3MOI67gPe+mWak/H5XTsqwi+khWkV5GryizzyTvtx88w/eFzsChjFQeO2dAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200825092205260&quot;
        title=&quot;image-20200825092205260&quot;
        src=&quot;/static/3ecf4bc1296548c228482e5a0a36b43b/fcda8/image-20200825092205260.png&quot;
        srcset=&quot;/static/3ecf4bc1296548c228482e5a0a36b43b/12f09/image-20200825092205260.png 148w,
/static/3ecf4bc1296548c228482e5a0a36b43b/e4a3f/image-20200825092205260.png 295w,
/static/3ecf4bc1296548c228482e5a0a36b43b/fcda8/image-20200825092205260.png 590w,
/static/3ecf4bc1296548c228482e5a0a36b43b/4b446/image-20200825092205260.png 877w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h4 id=&quot;step4-객체지향-함수-정의&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step4-%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5-%ED%95%A8%EC%88%98-%EC%A0%95%EC%9D%98&quot; aria-label=&quot;step4 객체지향 함수 정의 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;step4. (객체지향) 함수 정의&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;0. K.clear_session()&lt;/br&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h5 id=&quot;1-discriminator--builddiscriminatorbr&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#1-discriminator--builddiscriminatorbr&quot; aria-label=&quot;1 discriminator  builddiscriminatorbr permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Discriminator = BuildDiscriminator()&lt;/br&gt;&lt;/h5&gt;
&lt;h5 id=&quot;2-generator--buildgeneratorbr&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#2-generator--buildgeneratorbr&quot; aria-label=&quot;2 generator  buildgeneratorbr permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Generator = BuildGenerator()&lt;/br&gt;&lt;/h5&gt;
&lt;h5 id=&quot;3-gan--buildgandiscriminator-generatorbr&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#3-gan--buildgandiscriminator-generatorbr&quot; aria-label=&quot;3 gan  buildgandiscriminator generatorbr permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. GAN = BuildGAN(Discriminator, Generator)&lt;/br&gt;&lt;/h5&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h4 id=&quot;step5-학습-세팅&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step5-%ED%95%99%EC%8A%B5-%EC%84%B8%ED%8C%85&quot; aria-label=&quot;step5 학습 세팅 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;step5. 학습 세팅&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;nBatchCnt &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;       &lt;span class=&quot;token comment&quot;&gt;# Mini-batch를 위해 input 데이터를 n개 블록으로 나눈다. # 333개, 333개, 334개로 쪼개짐 &lt;/span&gt;
nBatchSize &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;realData&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; nBatchCnt&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;token comment&quot;&gt;# 블록 당 Size # nBatchSize: 333개, 333개, 334개 순으로 들어감 &lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; epoch &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# Mini-batch 방식으로 학습한다&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; n &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nBatchCnt&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# input 데이터를 Mini-batch 크기에 맞게 자른다&lt;/span&gt;
        nFrom &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; n &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; nBatchSize &lt;span class=&quot;token comment&quot;&gt;#for문 다 돌면 nFrom= 666&lt;/span&gt;
        nTo &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; n &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; nBatchSize &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; nBatchSize &lt;span class=&quot;token comment&quot;&gt;#for문 다 돌면 nTo=1000&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# 마지막 루프이면 nTo는 input 데이터의 끝까지. &lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;## 왜 써주냐면 , n=2(마지막)일때 nTo = n * nBatchSize + nBatchSize는 999로 1000이 안되기 땜에.&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; n &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; nBatchCnt &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            nTo &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; realData&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;        &lt;span class=&quot;token comment&quot;&gt;# 학습 데이터를 준비한다&lt;/span&gt;
        bx &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; realData&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;nFrom &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; nTo&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#진짜 data 형성&lt;/span&gt;
        bz &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; getNoise&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;m&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;bx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; n&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;nGInput&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# bx.shape[0]=333-&gt;333-&gt;334, nGInput=16&lt;/span&gt;
        Gz &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Generator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;bz&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#진짜 data shape 맞춰서 noise 써가지고 가짜 data 형성(fake data)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 410px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/300cd66c5e4a62c747237923b76d20a9/d68e4/image-20200715110832016.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 102.7027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAAAsSAAALEgHS3X78AAADGUlEQVQ4y4VU2XKbQBDk/38p5Tw4kQ/FkqwbgUBC3OhE6LKjqvb0IGw5FScPU8Ay29PT07vGYRcjX8+wWc1w3Cc4HVK8HDMcdgn2RQz+v459Eb2/M7+Kas2Ypw7Mfh1xMELoDzW8SQ9FHgh4dgGVQrIp3/g47pL3Yvnax1bW1ksPu22k60YaW1jOXamSYrWYKtM4HCGJLA0CFVLZSyzEqymihYuDgAWzoUYSWnDsZ/izgXZmLOcTXWg+3aLVuMVY3gkaBSayZAxr+ISB00JvUIfZqyNIx4iysXRkCoFyb5GH2hnlMkjbc7vaP6mT5W4bqgTbTYDR4BdsrwvbaaMhRa1JB/O1hyyykcY22q0a7mrfMOg9qkTGZuXBlE1kwxaZxKqRADJY5CiVXWl5EPThhEOcTwtMhQT3kAj32KOWMjU44YlUpx4Tp6P6UdNANKmml0tizXzETb8GU0AJGIt2gTfA2GrpXhanAwyiWmZDqZO2Y7d0csFF5MomSym8FSkKCX5Tr/PrAr6AkkBlI4MDKAXNVDMGk1mV76X3JJk+K8rgt9pEALrtex0gXaIM2fJQfMhFsmKwAHUkYNl22fr+3eDl9+tpjrXMgIN9Oc5LH/Jj6nY0gdNlu3zOxNz8d30KDleAzGHx2bQn0deOyNygTWjuLHHgy49F5uopIWOy/xsgp06pSKTV/KG5BCYZg06nNZ6bP1F/+I6H+xuxj61TzFdfAO5LQErDvNA3laXahgzpv1iCnswSW9th0pcML4BkxUGQEC1HzS8+LDUkIMHOr0ut/hWgaihAJOLK8OhH+lc1LMUdqsDUIpR33+urYSvbXF9dBKps9HLK0BXvroTIbzG7AjKRBuZPts+hUFwO5lObtAnNzFwORf67cgwbnTtMIhNzAeXVZlSVaWZr1NDTQn991kzuQgEYC0DTbSJdTjBLbWWXi36RyOMKaCFmNw5/mJUSXN/K19GZtnFn1eHEJmI5bryBHJGGt83Ik0u5+AT472DLvGzPciLYMrWMhelUbqFxNMSCFqta/n98HL2K/cdNFOj6SazEvDfbQBgvvvdP4gAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200715110832016&quot;
        title=&quot;image-20200715110832016&quot;
        src=&quot;/static/300cd66c5e4a62c747237923b76d20a9/d68e4/image-20200715110832016.png&quot;
        srcset=&quot;/static/300cd66c5e4a62c747237923b76d20a9/12f09/image-20200715110832016.png 148w,
/static/300cd66c5e4a62c747237923b76d20a9/e4a3f/image-20200715110832016.png 295w,
/static/300cd66c5e4a62c747237923b76d20a9/d68e4/image-20200715110832016.png 410w&quot;
        sizes=&quot;(max-width: 410px) 100vw, 410px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;/br&gt;
&lt;h5 id=&quot;discriminator--builddiscriminator&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#discriminator--builddiscriminator&quot; aria-label=&quot;discriminator  builddiscriminator permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Discriminator = BuildDiscriminator()&lt;/h5&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;    	&lt;span class=&quot;token comment&quot;&gt;### &amp;lt; Discriminator를 학습한다. &gt; ###&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# Real data가 들어가면 Discriminator의 출력이 &apos;1&apos;이 나오도록 학습하고,&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# Fake data (Gz)가 들어가면 Discriminator의 출력이 &apos;0&apos;이 나오도록 학습한다.&lt;/span&gt;
        &lt;span class=&quot;token triple-quoted-string string&quot;&gt;&quot;&quot;&quot;target data 만들기&quot;&quot;&quot;&lt;/span&gt;
        target &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;zeros&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;bx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        target&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; bx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.9&lt;/span&gt;     &lt;span class=&quot;token comment&quot;&gt;# &apos;1&apos; 대신 0.9로 함&lt;/span&gt;
        target&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;bx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.1&lt;/span&gt;     &lt;span class=&quot;token comment&quot;&gt;# &apos;0&apos; 대신 0.1로 함&lt;/span&gt;
        &lt;span class=&quot;token triple-quoted-string string&quot;&gt;&quot;&quot;&quot;target data 형성 완료&quot;&quot;&quot;&lt;/span&gt;
        
        bx_Gz &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;concatenate&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;bx&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Gz&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# D 학습 &lt;/span&gt;
        Dloss &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Discriminator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;train_on_batch&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;bx_Gz&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; target&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;token comment&quot;&gt;#real data &amp;amp; fake data 모두가 D를 거치게 한다. &lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;##참고: fit 함수보다 train_on_batch 쓰는 게 더 속도가 빨라서 이거 씀&lt;/span&gt;
 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 310px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/98081371ab2415f2244c7b50905f4563/5fad2/image-20200715110841184.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 85.8108108108108%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsSAAALEgHS3X78AAAC2UlEQVQ4y2VUaXeaUBDl//+Mfm97emo/tGlimibGeFww7iiKuIAI4sbiktzOjGBM+uEdYHjvzr13Zp4SbCwEWwtxYCOiFW5tbDdTiQX05Nh2PcEumCFKYiE9eQUX36d3CwoHGMy0NcwXQwE7hA524UyWvxzhEDmw3D5Wq7GAMgA/0yRRQojJKIzKG2xvIAcaZg1PegntcQN1o4q7dh4VQ0VrVMeS/gs7OrMm1s7CkISb9RSOZ8h5JTxntHGMXVS6JXz9k0G2lEWufIdiNYcf+Z/wfJMYOyKL2YycLr5XbpCh9SmXwb1WEPlKqp99Yr9Wngm9UxY5LP0Yz6FrKhbEIAptOcT7eS+zZJvmxHTD50+STx4e6fDLzsXSG6LfUyXBwjUQk49GvyZx8TXxjYmwXJfA9hQPkwIpMW2YzHU8dgpQyTNvYaJefUSNpGavM1jSNwNqwzrMWRe59hNsKtCeCrVcjsWKU3eckijsS9/SyIssrpsP6Jh1dJpFVEp/MRm1xDPTaGDuDuCvx9AmLTJ/ImxdfyjgzDpIAVMP2QPsPfgkrcceEoOVPxLZzHhFTOOz5DdG6/VbKwlg2jYhNyhRXxCTQv4GA70KvVvBxGyh2XiSRCxNikJrR+8Dq4MZtVt8jltvbZNOhU/VHA5qeD36wvLlsKDvugAyQ+kI2suFYE+n5P8uKcpZcph0+uvOE2kMyP5Zk86HKjsid59M0pT8qw5UKVDwXrJNnT4Uww1aIypCo5bH71/fUCzcot9V4c5pkshnl6qqT9vSozOyp0f7GVzuBJbMGXlkvhSv8Ll0hZvGPZrtokjkSj+rDzD0ZwE0Zz2U9bKsF2p4l5raoDtAPPwomX+ylJjbqFel2FR8BFYCyN4eCCQdvSjx8vLC+M9DmRoC5LYx+s9UZZXGroJOqySTE14cDJPxiz7Ezm2TtkNEfeY6Ouyphg1lX9N4pWDpnvd34MX9SFj/AOky8Iew8ES7AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200715110841184&quot;
        title=&quot;image-20200715110841184&quot;
        src=&quot;/static/98081371ab2415f2244c7b50905f4563/5fad2/image-20200715110841184.png&quot;
        srcset=&quot;/static/98081371ab2415f2244c7b50905f4563/12f09/image-20200715110841184.png 148w,
/static/98081371ab2415f2244c7b50905f4563/e4a3f/image-20200715110841184.png 295w,
/static/98081371ab2415f2244c7b50905f4563/5fad2/image-20200715110841184.png 310w&quot;
        sizes=&quot;(max-width: 310px) 100vw, 310px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;/br&gt;
&lt;h6 id=&quot;def-builddiscriminator&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#def-builddiscriminator&quot; aria-label=&quot;def builddiscriminator permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;def BuildDiscriminator()&lt;/h6&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# Discriminator를 G. D 각각 생성한다&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;BuildDiscriminator&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_shape &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; nDInput&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    h &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nDHidden&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; activation &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    Dx &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nDOutput&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; activation &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;sigmoid&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;h&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#0이면 가짜, 1이면 진짜로 하려고 sigmoid를 출력값으로 &lt;/span&gt;
    model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Dx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;loss &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;binary_crossentropy&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; optimizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; MyOptimizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;token comment&quot;&gt;#sigmoid 짝꿍 binary_crossentropy를 loss에 넣어서 1과 0 값이 출력되게 함 &lt;/span&gt;
    
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; model&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h5 id=&quot;gan--buildgandiscriminator-generator&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#gan--buildgandiscriminator-generator&quot; aria-label=&quot;gan  buildgandiscriminator generator permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GAN = BuildGAN(Discriminator, Generator)&lt;/h5&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;### &amp;lt; Generator를 학습한다. &gt; ###&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# Fake data (z --&gt; Gz --&gt; DGz)가 들어가도 Discriminator의 출력이 &apos;1&apos;이 나오도록 Generator를 학습한다.&lt;/span&gt;
&lt;span class=&quot;token triple-quoted-string string&quot;&gt;&quot;&quot;&quot;target data 만들기&quot;&quot;&quot;&lt;/span&gt;
target &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;zeros&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;bx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
target&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.9&lt;/span&gt;
&lt;span class=&quot;token triple-quoted-string string&quot;&gt;&quot;&quot;&quot;target data 형성 완료&quot;&quot;&quot;&lt;/span&gt;
        
Gloss &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; GAN&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;train_on_batch&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;bz&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; target&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;token comment&quot;&gt;## GAN 함수 참고: D는 위에서 학습해서 여기선 학습 안 하고 G만 학습 &lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# 어떤 학습? 가짜 data가 들어가면 target이 전부 1로 출력되도록(Discriminator가 전부 진짜로 판별하도록)! &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 448px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/97569fb6e53b4464c4b19ffaa74c0241/33b38/image-20200715110850214.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 59.45945945945946%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsSAAALEgHS3X78AAABsElEQVQoz11T2VLCQBDM/3+Gj/oNPmhpiSAhIEeICSHk2oT7Ri1rnJ6wIfCwtdmdmZ7uno1x2KV03Gd0OmS8K8L5dFC03ya0XoZ8VhKb5R6tFhP6OU0ljjvk6oV87AZAZvlI1nzq024TC9DY69CgV6c0dmiqPDI/nsnuN8jn+1nmkUoc2qzCM5AqyRjo6DoWdaxXGnTrtJwHNHIt8pwW9T7fOdaiwO/yd43B2txgKDkAbzaeyDJfhIAmYxR0Uz4UEtEJctLIJsduilTE3aFJYdA/W5Ixu0hiOLcY9OH+TuJG1YfjvvACoFj6GzvkbdexyEPu9zEvm0/ZgjDoicfGrbFIgiRIiyZ9ypVbgqA4Zz/BPGEFeni410NlwEQkaUAE0A2+wZcJf0OeZh2HAwH03bZ4puvKKV8Y6m5KgGpvjzLlJLSlcDzqyITBGKBgj3s9Xb2uAAsflXiRqy/6+51L8XYdiQ0YGp6RAAZVQHUNWJWMhMVsTF+2yT4NhZUuxCCiyUDeIGzRkq8e9i1DvcAK3uE5lQUMCpYYCJpW5WpSxuXJVAFVOYRbjzTTy69XlazoHzpngHnz2qseAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200715110850214&quot;
        title=&quot;image-20200715110850214&quot;
        src=&quot;/static/97569fb6e53b4464c4b19ffaa74c0241/33b38/image-20200715110850214.png&quot;
        srcset=&quot;/static/97569fb6e53b4464c4b19ffaa74c0241/12f09/image-20200715110850214.png 148w,
/static/97569fb6e53b4464c4b19ffaa74c0241/e4a3f/image-20200715110850214.png 295w,
/static/97569fb6e53b4464c4b19ffaa74c0241/33b38/image-20200715110850214.png 448w&quot;
        sizes=&quot;(max-width: 448px) 100vw, 448px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;/br&gt;
&lt;h6 id=&quot;def-buildgand-g&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#def-buildgand-g&quot; aria-label=&quot;def buildgand g permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;def BuildGAN(D, G)&lt;/h6&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;BuildGAN&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;D&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; G&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 전체 NETWORK Build &lt;/span&gt;
    D&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;trainable &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;     &lt;span class=&quot;token comment&quot;&gt;# Discriminator는 업데이트하지 않는다= 학습하지 않는다. 왜냐면 자체적으로 위에서 학습했으니까. 따라서 G만 학습됨 &lt;/span&gt;
    z &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_shape&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; nGInput&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    Gz &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; G&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;z&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    DGz &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; D&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Gz&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;z&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; DGz&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#z가 들어가면 최종적으로 DGz가 나온다. &lt;/span&gt;
    model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;loss &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;binary_crossentropy&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; optimizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; MyOptimizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.0005&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# binary_crossentropy: 출력값이 0 아니면 1 나오도록&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; model&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;h6 id=&quot;def-builddiscriminator--discriminator&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#def-builddiscriminator--discriminator&quot; aria-label=&quot;def builddiscriminator  discriminator permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;def BuildDiscriminator() = Discriminator&lt;/h6&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# Discriminator를 G. D 각각 생성한다&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;BuildDiscriminator&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_shape &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; nDInput&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    h &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nDHidden&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; activation &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    Dx &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nDOutput&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; activation &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;sigmoid&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;h&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#0이면 가짜, 1이면 진짜로 하려고 sigmoid를 출력값으로 &lt;/span&gt;
    model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Dx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;loss &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;binary_crossentropy&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; optimizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; MyOptimizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;token comment&quot;&gt;#sigmoid 짝꿍 binary_crossentropy를 loss에 넣어서 1과 0 값이 출력되게 함 &lt;/span&gt;
    
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; model&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;h6 id=&quot;def-buildgenerator--generator&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#def-buildgenerator--generator&quot; aria-label=&quot;def buildgenerator  generator permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;def BuildGenerator() = Generator&lt;/h6&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# Generator를 생성한다 &lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# G는 여기서 학습 안 하므로 &apos;.complie&apos; 안 함 &lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;BuildGenerator&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; 
    z &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_shape &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; nGInput&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    h &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nGHidden&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; activation &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;z&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    Gz &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nGOutput&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; activation&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;linear&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;h&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;z&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Gz&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;h5 id=&quot;kd--klraldata-fakedata&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#kd--klraldata-fakedata&quot; aria-label=&quot;kd  klraldata fakedata permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;kd = KL(ralData, fakeData)&lt;/h5&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;    &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; epoch &lt;span class=&quot;token operator&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        z &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; getNoise&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;m&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;realData&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; n&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;nGInput&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        fakeData &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Generator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;z&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;  
        &lt;span class=&quot;token comment&quot;&gt;# Generator = BuildGenerator()에서 만든 Model(z, Gz)를 활용하여, &lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# &quot;model.predict(z=노이즈 data)&quot; 하라는 뜻 &lt;/span&gt;
        kd &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; KL&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;realData&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; fakeData&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;epoch = %d, D-Loss = %.3f, G-Loss = %.3f, KL divergence = %.3f&quot;&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;epoch&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Dloss&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Gloss&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; kd&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# Dloss: 0.6932636 , Gloss: 0.6933405 , kd: 0.2189525518812676 = 분산이 적다&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;h6 id=&quot;def-kl&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#def-kl&quot; aria-label=&quot;def kl permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;def KL&lt;/h6&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# 두 분포 (P, Q)의 KL divergence를 계산한다.&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;KL&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;P&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Q&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# 두 데이터의 분포를 계산한다&lt;/span&gt;
    histP&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; binsP &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;histogram&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;P&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; bins&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    histQ&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; binsQ &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;histogram&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Q&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; bins&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;binsP&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;token comment&quot;&gt;# 두 분포를 pdf로 만들기 위해 normalization한다.&lt;/span&gt;
    histP &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; histP &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;histP&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1e&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    histQ &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; histQ &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;histQ&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1e&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# KL divergence를 계산한다&lt;/span&gt;
    kld &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;histP &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;log&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;histP &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1e&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;log&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;histQ &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1e&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; kld&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h4 id=&quot;plt&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#plt&quot; aria-label=&quot;plt permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;plt&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# real data 분포 (p)와 fake data 분포 (q)를 그려본다&lt;/span&gt;
z &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; getNoise&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;m&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;realData&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; n&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;nGInput&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
fakeData &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Generator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;z&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;figure&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;figsize&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
sns&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;set_style&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;whitegrid&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
sns&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;kdeplot&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;realData&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;blue&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; bw&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; label&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;Real&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
sns&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;kdeplot&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;fakeData&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;red&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; bw&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; label&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;Fake&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;legend&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;title&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;Distibution of real and fake data&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;show&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 487px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/c2ef4c0767c4f98f9747be8c0cb92d40/7b439/image-20200809130155797.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 64.86486486486486%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAABtUlEQVQ4y3VTDbObIBD0///GtvNeGicaH6h8iICwPY5Ikr5Wh1G55W5v9+xijDiO422llPg5jiP6vseXEJimO+7TiGEYeN97z7hEOB8zQqh5OmstB1+TlfUslJBThPkcsNw0Mt3vxQ9o4XC7DphniW7bNsoeWqKycs4teQZwLAryaiB/CWwm0k7mQowhbN4dnDWwlIsZviYsyVpCOlSu9VMQOCMai/mieC89GDK+lKBnOd9prf/fciYWbof8mBFZrwDxU1CssiuYU8fyXta3lk+GMR7MRI0W7ktVFkjQ/QK7+KbliT+N7IwxT8de1sEMQYwkUnGQ9cxw84blulZt47Ojs8Nu3/fm6LsxCVYFrBfJTJsktGwvucA/GZ4a/s2wsFl/zwhqYzbNBHoPYqV9yw4zPj9HqXPOsYZn9eJsOeQ1tfYhqqOl+gujgyTYbxNr+q3l5nIJoI5AucSPO7VcxT8PsKuPghsZtV5qwYqp8cYwB49ArNzqiJmEnVRjd2rUpuAx8OqmYK4S3ngilXjYebBj+R81BUcJMyievT14Zn6KXszjwo/vQLFIv2TcPJww0LrgA/4A4XP6fxf+uRsAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200809130155797&quot;
        title=&quot;image-20200809130155797&quot;
        src=&quot;/static/c2ef4c0767c4f98f9747be8c0cb92d40/7b439/image-20200809130155797.png&quot;
        srcset=&quot;/static/c2ef4c0767c4f98f9747be8c0cb92d40/12f09/image-20200809130155797.png 148w,
/static/c2ef4c0767c4f98f9747be8c0cb92d40/e4a3f/image-20200809130155797.png 295w,
/static/c2ef4c0767c4f98f9747be8c0cb92d40/7b439/image-20200809130155797.png 487w&quot;
        sizes=&quot;(max-width: 487px) 100vw, 487px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h2 id=&quot;cnn-gan-dcgan-code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#cnn-gan-dcgan-code&quot; aria-label=&quot;cnn gan dcgan code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CNN GAN (DCGAN) CODE&lt;/h2&gt;
&lt;/br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;사용한 함수 총 5개:&lt;/br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;def build&lt;em&gt;generator(inputs, image&lt;/em&gt;size)&lt;/br&gt;&lt;/li&gt;
&lt;li&gt;def build_discriminator(inputs)&lt;/br&gt;&lt;/li&gt;
&lt;li&gt;def train(models, x_train, params)&lt;/br&gt;&lt;/li&gt;
&lt;li&gt;def build&lt;em&gt;and&lt;/em&gt;train&lt;em&gt;models(load&lt;/em&gt;W = False, train_steps = 100)&lt;/br&gt;&lt;/li&gt;
&lt;li&gt;def plot_images():&lt;/br&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/br&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/4867ffac049680dfe4dd33daef7986d9/573d3/image-20200809130207635.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 58.10810810810811%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsSAAALEgHS3X78AAABnklEQVQoz3VTS67CMBDrwTkBF+A8bNiwYc8CEC08+qOl/1/m2UNSFSQijRoRx/Z4gic/ljFGDDfDIH1VSVXXUvGL6vv+A7dcnl78VURMk1R5LhmKZF3XaY0QGsfxkxRfb1b6duh+w6XYD8S/3STLMilBWjeNVgviGWtJPQMHhkptJxPaYnsDvgZgFlWropAIhNH9LsH5LOnjAUyjcRi0bxZOPdO2YmgfilUYSRRFkj+f0pMQQCXmOaoDNsZ5Ese6n2iG95t2QWhb7KCUI6c7XKRpKg0EXEZcbI8YCr5eL2ks4fdwdChlWWrgSZLI5XKRMAylQJszITCcLH87nU4qWtupFyBnVLNDtkeCB3K5ISff99UFRZYOSRij1TMyJCFdspM/7HOLV0Jnmy1yio6Q6gNyc4tPxRFSmPG4uxOINE/N0PZOBy1yoTrBJHMgYy/QFc8pSAPD4i3OD/v7H7JarWS73eqeLt0Q6JC5Ubiz72+z2ch6vZb3cx3fDhk0W2XR2W63k+PxqAO6Xq9ajCEIgrl4xswOh4Ps93vd13Yw/zXqn4f5hoHcAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200809130207635&quot;
        title=&quot;image-20200809130207635&quot;
        src=&quot;/static/4867ffac049680dfe4dd33daef7986d9/fcda8/image-20200809130207635.png&quot;
        srcset=&quot;/static/4867ffac049680dfe4dd33daef7986d9/12f09/image-20200809130207635.png 148w,
/static/4867ffac049680dfe4dd33daef7986d9/e4a3f/image-20200809130207635.png 295w,
/static/4867ffac049680dfe4dd33daef7986d9/fcda8/image-20200809130207635.png 590w,
/static/4867ffac049680dfe4dd33daef7986d9/efc66/image-20200809130207635.png 885w,
/static/4867ffac049680dfe4dd33daef7986d9/c83ae/image-20200809130207635.png 1180w,
/static/4867ffac049680dfe4dd33daef7986d9/573d3/image-20200809130207635.png 1650w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;/br&gt;
&lt;h4 id=&quot;base&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#base&quot; aria-label=&quot;base permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Base&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# load MNIST dataset&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x_train&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; _&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;_&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; _&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; mnist&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load_data&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token triple-quoted-string string&quot;&gt;&quot;&quot;&quot; 비지도 방법으로 사용 &quot;&quot;&quot;&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# reshape data for CNN as (28, 28, 1) and normalize &lt;/span&gt;
&lt;span class=&quot;token triple-quoted-string string&quot;&gt;&quot;&quot;&quot; 2D CNN &quot;&quot;&quot;&lt;/span&gt;
image_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; x_train&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# x_train.shape (60000, 28, 28)&lt;/span&gt;
x_train &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;reshape&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x_train&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; image_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; image_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# x_train.shape = (60000, 28, 28, 1)&lt;/span&gt;
x_train &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; x_train&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;astype&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;float32&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;255&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 표준화&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# the latent or z vector is 100-dim&lt;/span&gt;
latent_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#latent: KNN 가기 전 층들&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h4 id=&quot;step-1-build_generator&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step-1-build_generator&quot; aria-label=&quot;step 1 build_generator permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;step 1. build_generator&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;build_generator&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; image_size&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# latent 층에 씀 &lt;/span&gt;
    image_resize &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; image_size &lt;span class=&quot;token operator&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt;
    
    &lt;span class=&quot;token comment&quot;&gt;# network parameters &lt;/span&gt;
    kernel_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;
    layer_filters &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;

    x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;image_resize &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; image_resize &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; layer_filters&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Reshape&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;image_resize&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; image_resize&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; layer_filters&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; filters &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; layer_filters&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# hidden 층을 for문으로 써줌(쫙 쌓아주는 것)&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# first two convolution layers use strides = 2&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# the last two use strides = 1&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; filters &lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt; layer_filters&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            strides &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 따라서 layer_filters의 뒤에서 2번째까진 strides = 2&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            strides &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;
        x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; BatchNormalization&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Activation&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Conv2DTranspose&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;filters&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;filters&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                            kernel_size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;kernel_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                            strides&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;strides&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                            padding&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;same&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# data 양 뿔려줌 &lt;/span&gt;

    x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Activation&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;sigmoid&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    generator &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; name&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;generator&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 모방 모델이므로 y 자리엔 x 학습 결과를 써줌 &lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; generator &lt;span class=&quot;token comment&quot;&gt;# 요렇게 fake data 만들어서 전에 모델처럼 Discriminator에 넣어줌 &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h4 id=&quot;step-2-build_discriminator&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step-2-build_discriminator&quot; aria-label=&quot;step 2 build_discriminator permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;step 2. build_discriminator&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;build_discriminator&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    kernel_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;
    layer_filters &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;

    x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; inputs
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; filters &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; layer_filters&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# first 3 convolution layers use strides = 2&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# last one uses strides = 1&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# 따라서 Discriminator를 더 많이 학습하게 됨 &lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; filters &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; layer_filters&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            strides &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            strides &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;
        x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; LeakyReLU&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;alpha&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Conv2D&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;filters&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;filters&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                   kernel_size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;kernel_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                   strides&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;strides&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                   padding&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;same&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

    x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Flatten&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Activation&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;sigmoid&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    discriminator &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; name&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;discriminator&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; discriminator&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h4 id=&quot;step-3-buildandtrain_models&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step-3-buildandtrain_models&quot; aria-label=&quot;step 3 buildandtrain_models permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;step 3. build&lt;em&gt;and&lt;/em&gt;train_models&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;build_and_train_models&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;load_W &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; train_steps &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    model_name &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;dcgan_mnist&quot;&lt;/span&gt;
    
    &lt;span class=&quot;token comment&quot;&gt;# network parameters&lt;/span&gt;
    batch_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;
    lr &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2e&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt;
    decay &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;6e&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;8&lt;/span&gt;
    input_shape &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;image_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; image_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# build discriminator model&lt;/span&gt;
    inputs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;shape&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;input_shape&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; name&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;discriminator_input&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    discriminator &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; build_discriminator&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;token comment&quot;&gt;# [1] or original paper uses Adam, &lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# but discriminator converges easily with RMSprop&lt;/span&gt;
    optimizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; RMSprop&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lr&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;lr&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; decay&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;decay&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    discriminator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;loss&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;binary_crossentropy&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                          optimizer&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;optimizer&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                          metrics&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;accuracy&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    discriminator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;token comment&quot;&gt;# 저장된 discriminator 모델을 읽어온다.&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; load_W&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        discriminator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load_weights&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;dataset/dcgan_D.h5&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# build generator model&lt;/span&gt;
    input_shape &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;latent_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    inputs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;shape&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;input_shape&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; name&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;z_input&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    generator &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; build_generator&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; image_size&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    generator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# 저장된 generator 모델을 읽어온다.&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; load_W&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        generator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load_weights&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;dataset/dcgan_G.h5&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;token comment&quot;&gt;# build adversarial model&lt;/span&gt;
    optimizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; RMSprop&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lr&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;lr &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; decay&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;decay &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;token comment&quot;&gt;# freeze the weights of discriminator during adversarial training&lt;/span&gt;
    discriminator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;trainable &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;
    
    &lt;span class=&quot;token comment&quot;&gt;# adversarial = generator + discriminator&lt;/span&gt;
    adversarial &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; 
                        discriminator&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;generator&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                        name&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;model_name&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    adversarial&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;loss&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;binary_crossentropy&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                        optimizer&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;optimizer&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                        metrics&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;accuracy&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    adversarial&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#adversarial: 최종모델&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# train discriminator and adversarial networks&lt;/span&gt;
    models &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;generator&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; discriminator&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; adversarial&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    params &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; latent_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; train_steps&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; model_name&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    train&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;models&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; x_train&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; params&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;token comment&quot;&gt;# 모델을 저장해 둔다&lt;/span&gt;
    discriminator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;save_weights&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;dataset/dcgan_D.h5&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    generator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;save_weights&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;dataset/dcgan_G.h5&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h4 id=&quot;step-4-train&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step-4-train&quot; aria-label=&quot;step 4 train permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;step 4. train&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;models&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; x_train&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; params&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# the GAN component models&lt;/span&gt;
    generator&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; discriminator&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; adversarial &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; models 
    
    
    &lt;span class=&quot;token comment&quot;&gt;# network parameters&lt;/span&gt;
    batch_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; latent_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; train_steps&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; model_name &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; params 
    
    
    &lt;span class=&quot;token comment&quot;&gt;# number of elements in train dataset&lt;/span&gt;
    train_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; x_train&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;train_steps&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# train the discriminator for 1 batch&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# 1 batch of real (label=1.0) and fake images (label=0.0)&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# randomly pick real images from dataset&lt;/span&gt;
        rand_indexes &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;randint&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; train_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;batch_size&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        real_images &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; x_train&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;rand_indexes&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# generate fake images from noise using generator &lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# generate noise using uniform distribution(모든 확률변수에 대해 균일한 확률을 가짐)&lt;/span&gt;
        noise &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;uniform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                                  size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;batch_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; latent_size&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# generate fake images&lt;/span&gt;
        fake_images &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; generator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;noise&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# real + fake images = 1 batch of train data&lt;/span&gt;
        x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;concatenate&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;real_images&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; fake_images&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# label real and fake images&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# real images label is 1.0&lt;/span&gt;
        y &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;ones&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; batch_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# fake images label is 0.0&lt;/span&gt;
        y&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;batch_size&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.0&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# train discriminator network, log the loss and accuracy&lt;/span&gt;
        loss&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; acc &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; discriminator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;train_on_batch&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;token triple-quoted-string string&quot;&gt;&quot;&quot;&quot;Q. loss: 인덱스??? &quot;&quot;&quot;&lt;/span&gt;
        log &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;%d: [D-loss: %.4f, acc: %.4f]&quot;&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;i&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; loss&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; acc&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;token comment&quot;&gt;# train the adversarial network for 1 batch&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# 1 batch of fake images with label=1.0&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# since the discriminator weights &lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# are frozen in adversarial network(adversarial network: 적대적 신경망(경쟁 속 반대편에 놓인 신경망))&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# only the generator is trained&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# generate noise using uniform distribution&lt;/span&gt;
        noise &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;uniform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; 
                                  size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;batch_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; latent_size&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# label fake images as real or 1.0&lt;/span&gt;
        y &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;ones&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;batch_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# train the adversarial network &lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# note that unlike in discriminator training, &lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# we do not save the fake images in a variable&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# the fake images go to the discriminator input of the adversarial&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# for classification&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# log the loss and accuracy&lt;/span&gt;
        loss&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; acc &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; adversarial&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;train_on_batch&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;noise&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;token triple-quoted-string string&quot;&gt;&quot;&quot;&quot;Q. adversarial 뜻???&quot;&quot;&quot;&lt;/span&gt;
        log &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;%s [G-loss: %.4f, acc: %.4f]&quot;&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;log&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; loss&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; acc&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;log&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
   
    &lt;span class=&quot;token comment&quot;&gt;# save the model after training the generator&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# the trained generator can be reloaded for &lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# future MNIST digit generation&lt;/span&gt;
    generator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;save&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;model_name &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;.h5&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h4 id=&quot;step-5-fake-data를-화면에-표시&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step-5-fake-data%EB%A5%BC-%ED%99%94%EB%A9%B4%EC%97%90-%ED%91%9C%EC%8B%9C&quot; aria-label=&quot;step 5 fake data를 화면에 표시 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;step 5. fake data를 화면에 표시&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# Generator가 생성한 이미지(fake data)를 화면에 표시한다.&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;plot_images&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    inputs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;shape&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;latent_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; name&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;z_input&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    generator &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; build_generator&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; image_size&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    generator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load_weights&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;dataset/dcgan_G.h5&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    noise_input &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;uniform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; latent_size&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    images &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; generator&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;noise_input&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# Generator 통해 나온 fake data&lt;/span&gt;
    plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;figure&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;figsize&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    num_images &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; images&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
    
    noise_input &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;uniform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    rows &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;sqrt&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;noise_input&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;num_images&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;subplot&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;rows&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; rows&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; i &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        image &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;reshape&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;images&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;i&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;image _size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; image_size&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;token triple-quoted-string string&quot;&gt;&quot;&quot;&quot;why 3차원 reshape???&quot;&quot;&quot;&lt;/span&gt;
        plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;imshow&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;image&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; cmap&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;gray&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;axis&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;off&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;show&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;h4 id=&quot;final&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#final&quot; aria-label=&quot;final permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Final&lt;/h4&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# 이미 학습된 weights를 읽어오고, 추가로 학습한다.&lt;/span&gt;
build_and_train_models&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;load_W &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; train_steps &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# train_steps 만큼 반복 학습&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# Generator가 생성한 이미지를 화면에 표시한다.&lt;/span&gt;
plot_images&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/br&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/fa51182f7404b07162d67390997b0e5b/f6b72/image-20200714193936268.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAABe0lEQVQoz5WSW28TMRCF9///Jp5QhVBFKoRACEqTkmUTmna7V9u7vn3MuApQ9amWjsYzts/xGbtCRgiRZXGC5QW8D7RtS9P84tA0WGtZ17WsnaOxDmNMQaXFvu9JKZOF/AXcSE6RlDNJ8mQHkpskl7nUY/D03SPb7Zbj8UjlRLHrelY7MXcnzNBiH/YsY0sY74j1FVFtPO5gPMB0hyzy/yiCAucclVroB1EVpegX4jKTpnvi6Rr34wJXf8Qcv2Nu3mN2l6zbd4T2lhgDmHuYRSCFQqz2K+3POIqtzPNRNknRPEBzRer2pHaH//YGW3/C/dxgr9+W3HsPrmNx0kNl7bpOCPNznHuYEzmGszno9/D7M+wvSfUGf/iCvd0UAac3VN/6KK8a3jz1cRVkEWs+wOkrq9y0SikxSA/1piGEv1GF/uVe7Lhi7SlKffVCEFn1e5kB72xpXVXaJaT6OHpgnucSp2kqUf+WEtd1XWqaK6nuszLXc1b+oUJrfwDgd7kdNL4X5wAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200714193936268&quot;
        title=&quot;image-20200714193936268&quot;
        src=&quot;/static/fa51182f7404b07162d67390997b0e5b/fcda8/image-20200714193936268.png&quot;
        srcset=&quot;/static/fa51182f7404b07162d67390997b0e5b/12f09/image-20200714193936268.png 148w,
/static/fa51182f7404b07162d67390997b0e5b/e4a3f/image-20200714193936268.png 295w,
/static/fa51182f7404b07162d67390997b0e5b/fcda8/image-20200714193936268.png 590w,
/static/fa51182f7404b07162d67390997b0e5b/f6b72/image-20200714193936268.png 615w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/924334ab46f3f70fd22ca0d3fb7204b7/0a47e/image-20200714193941200.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 43.91891891891892%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAABoElEQVQoz3WSbY+bMAzH+f6fam/25t5s0m7XHdf2Cm2a0kILJBAICb8ltJWmabNk+SG2Y/vvZBgG/sfWWq7XK1mWIYT4R8y4yK43aK0XPdFKMQPeGnwrcYPCzzPee1zgKL1zf8gne2ZnmaYJ1dbsdjvKsiTp+55IVlU4+RoUvdjMPn7D3zQ/5FRLOrG6+x4NNE1DEtuejGb78wX54wuukcym4SrWtHJ9z+6v4EY63TKHCSJVh5RL+vKo3sWqtG1LYkKH6pyzWae8f//K8XNFfjzxEeztxxvzJcUUG/xlTb5Zcc5TaASHLNjv3xirPUeRh490KKjCDrseGRznQlIWgto4fp0UBymp65o0XfFRtBz2O/I8o1AWcTwgpSA7XXjb7hE3Q61DYwGPJCJTdY5Sj7T9iB4musBm8lg3k1U9ojahqAr+cZmwaAyfF0V4pgkNxJymt2j1GNn5mX6cwmLnZcE8OKIc7cFO1N29WFx+pFINATe/wGQnt8QuoHRdt9zQON5v6na7YYxZ7BgQx7BBj3Hx7XmDcR3PvCirqmIMd/sbQ+20LI/AryoAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200714193941200&quot;
        title=&quot;image-20200714193941200&quot;
        src=&quot;/static/924334ab46f3f70fd22ca0d3fb7204b7/fcda8/image-20200714193941200.png&quot;
        srcset=&quot;/static/924334ab46f3f70fd22ca0d3fb7204b7/12f09/image-20200714193941200.png 148w,
/static/924334ab46f3f70fd22ca0d3fb7204b7/e4a3f/image-20200714193941200.png 295w,
/static/924334ab46f3f70fd22ca0d3fb7204b7/fcda8/image-20200714193941200.png 590w,
/static/924334ab46f3f70fd22ca0d3fb7204b7/0a47e/image-20200714193941200.png 600w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content:encoded></item><item><title><![CDATA[GAN 이론]]></title><description><![CDATA[GAN 비지도학습(UL) 방식의 이미지, 문서, 음성 등의 데이터를 생성(모방)하는 알고리즘 비모수적방법으로도 비교적 정확한 sampling이 가능함  위조 데이터 생성 및 판별에 사용 
EX…]]></description><link>https://jynee.github.io/tags#1st/GAN_1/</link><guid isPermaLink="false">https://jynee.github.io/tags#1st/GAN_1/</guid><pubDate>Sat, 08 Aug 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;gan&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#gan&quot; aria-label=&quot;gan permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GAN&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;비지도학습(UL) 방식의 이미지, 문서, 음성 등의 데이터를 &lt;strong&gt;생성(모방)하는 알고리즘&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;비모수적방법&lt;/strong&gt;으로도 비교적 정확한 &lt;strong&gt;sampling이 가능&lt;/strong&gt;함 &lt;/li&gt;
&lt;li&gt;위조 데이터 생성 및 판별에 사용
EX) 이미지 색깔(색칠) 해주는 프로그램, 딥페이크 영상&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/ff466a3851f29e6c207a88c4333ce60e/32056/image-20200728184326510.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAABgklEQVQoz31Su0oDURC9iBZaWfgZfomIH2BroYVEJCAWYqdfIGgKwcLaSkHsBEXUwsJCELNJ1GiMicnmscnexx5n7j7cQOKBKfbOzpk5Z0ZgBIIgsMHQxkDqvwhzSPLpf8V/hAxjAvSlTt4VEfaib005qQOYFLGI2YdFUqQ0+sqg1pa2uOlJ+z5M0dAJFXXlAi5mIl/p5J2D2mDv0sV87guHN23kv2VSayfk4jjiydLoSYWur+GRVNdTlnDrtA6x/ILxjIPZ3XcUaj1rhVAm9ISnYPMrrsT22Q8e3vp4rkrsnDfwWPbISyIkUpZLziJ7QoRLecxsvmLxqIpqyw8J7dYQSmN8ugpitYCpbAnTGyWawsF1vhtNaqwKpTUunjyS28JHUw2oESoirHckTWmok8JYpoiJNQeT6wWIFQe3BY9tp5y0ltQ7/sgzs4Qqui1GuSExt1/BwVULx/cdLOQquCt2kyK2hqXxkuOTSV+GXQrfGRvPkb65GOm8JTPBwK2mF/kL+MP0X/SgmJAAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image 20200728184326510&quot;
        title=&quot;image 20200728184326510&quot;
        src=&quot;/static/ff466a3851f29e6c207a88c4333ce60e/fcda8/image-20200728184326510.png&quot;
        srcset=&quot;/static/ff466a3851f29e6c207a88c4333ce60e/12f09/image-20200728184326510.png 148w,
/static/ff466a3851f29e6c207a88c4333ce60e/e4a3f/image-20200728184326510.png 295w,
/static/ff466a3851f29e6c207a88c4333ce60e/fcda8/image-20200728184326510.png 590w,
/static/ff466a3851f29e6c207a88c4333ce60e/32056/image-20200728184326510.png 602w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;출처: &lt;a href=&quot;https://www.naverlabs.com/storyDetail/44&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://www.naverlabs.com/storyDetail/44&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;그림-보충-설명&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EA%B7%B8%EB%A6%BC-%EB%B3%B4%EC%B6%A9-%EC%84%A4%EB%AA%85&quot; aria-label=&quot;그림 보충 설명 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;그림 보충 설명:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Real&lt;/code&gt;: 실제 데이터(이미지, 음성 등)&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Input&lt;/code&gt;: 랜덤 데이터. &lt;strong&gt;노이즈 섞인 것&lt;/strong&gt;. 	&lt;em&gt;But,&lt;/em&gt; Generator 통과하면, Real data 같은 것으로 변환돼 나온다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Generator(network)&lt;/code&gt;: 생성자. &lt;strong&gt;진짜 같은 가짜(Fake) 생성&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Discriminator(network)&lt;/code&gt;: 판별자. &lt;strong&gt;실제 데이터(Real)와 가짜 데이터(Fake)를 판별&lt;/strong&gt;함&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;즉-총-2개의-네트워크-사용&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EC%A6%89-%EC%B4%9D-2%EA%B0%9C%EC%9D%98-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EC%82%AC%EC%9A%A9&quot; aria-label=&quot;즉 총 2개의 네트워크 사용 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;즉, 총 2개의 네트워크 사용:&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Discriminator&lt;/code&gt;: real or fake 판별자&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Generator&lt;/code&gt;: fake 생성자&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;gan의-loss-function&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#gan%EC%9D%98-loss-function&quot; aria-label=&quot;gan의 loss function permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gan의 loss function:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;minGmaxDV(D,G) = logD(x) + log(1-D(G(z)))&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;학습을 반복하여 (Pg = Pdata가 되어) Discriminator가 구별 불가능인 상태(‘D(x)=0.5’)로 수렴하도록
→ 마치 Generator가 x를 만들어낸 것처럼 됨 &lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Discriminator&lt;/th&gt;
&lt;th&gt;Generator&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;엔트로피 최대화&lt;/td&gt;
&lt;td&gt;엔트로피 최소화&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;엔트로피: 정보의 가치(정보량)과 ~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt;  &lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Discriminator&lt;/code&gt;&lt;/strong&gt;(network): &lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;maxV(D, G)&lt;/code&gt;&lt;/strong&gt;로 학습. D(x) = 1 and D(G(z)) = 0일 때 최대&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;why?&lt;/em&gt; D(G(z))가 1이 되고, D(x)가 1이 되니까&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Generator&lt;/code&gt;&lt;/strong&gt; network: &lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;minV(G)&lt;/code&gt;&lt;/strong&gt;로 학습. D(G(z)) = 1 일 때 최소. 이때(D(x)는 상관 X)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;why?&lt;/em&gt; D(G(z))가 0이 되니까&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&quot;원리&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EC%9B%90%EB%A6%AC&quot; aria-label=&quot;원리 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;원리:&lt;/h3&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/6be073ef6d6fa9dd4ff79d7d459669c9/32056/image-20200728185621472.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 41.891891891891895%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsSAAALEgHS3X78AAABSUlEQVQoz31R2W7DMAzL//9c99Khd9cka27ncNw0jm1OUtECG4oZIGg7MUVK0Xr9idXqA7vdHpvNFvv9QXA8nnA4HBHHiezP5y85Z1kGXiEEwd8V1XWN7XaHJElR5AX4bG433O93zPMsPE0TbnI3wzn3EnuHiFW5MuN6zVCVFS7kSDUK42jIUS7gb22rRPi/FbGq1hpVVSMnh6ppoIhb4kYpctxgtuwswJiFXDsS9QJrA7n3AmO8/BMtyyKC4zii7TqJ+4zHcbUeoUdNDyYMw0OMmQWee2atHZaFBPkRNzxJv5FTtPwSIzmdpJdFUYrzYRhE2Dn/6pX34X1k7/2vaU3kriQhYwwJaWEehLUefR/ozqPrHBVcoNTDFUfl+FwkqskBT5h7pahn7KQnRx3F7/seHaPrpS3MPKgQWNQiTSc0zYyynIWtdfgBzhpml1rCyu4AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image 20200728185621472&quot;
        title=&quot;image 20200728185621472&quot;
        src=&quot;/static/6be073ef6d6fa9dd4ff79d7d459669c9/fcda8/image-20200728185621472.png&quot;
        srcset=&quot;/static/6be073ef6d6fa9dd4ff79d7d459669c9/12f09/image-20200728185621472.png 148w,
/static/6be073ef6d6fa9dd4ff79d7d459669c9/e4a3f/image-20200728185621472.png 295w,
/static/6be073ef6d6fa9dd4ff79d7d459669c9/fcda8/image-20200728185621472.png 590w,
/static/6be073ef6d6fa9dd4ff79d7d459669c9/32056/image-20200728185621472.png 602w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;출처: Goodfellow 논문 공식&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;Discriminator&lt;/code&gt;를 k번 학습시키고, &lt;code class=&quot;language-text&quot;&gt;Generator&lt;/code&gt;를 1번 학습시킨다.&lt;/p&gt;
&lt;p&gt;→ D가 G보다 더 많이 학습된다.    * D: Discriminator / * G: Generator&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;em&gt;이에 따라&lt;/em&gt; &gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Real Data와 Fake Data를 구별해내는 &lt;code class=&quot;language-text&quot;&gt;D loss&lt;/code&gt;는 점점 더 작아져 영향력이 줄어들고,&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;D&lt;/code&gt;와 &lt;code class=&quot;language-text&quot;&gt;G&lt;/code&gt;는 점점 더 비슷해지며,&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;KL&lt;/code&gt;이 적어지고,   &lt;strong&gt;*&lt;/strong&gt;KL: G와 D의 정보량의 차이/분산의 차이&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;p&gt;&lt;em&gt;이럴수록&lt;/em&gt; &gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;​	3-1) G가 더 많은 영향력을 행사하고(역할을 하고),&lt;/p&gt;
&lt;p&gt;​	3-2) &lt;code class=&quot;language-text&quot;&gt;D loss&lt;/code&gt;를 계산하는 공식 中 [-log4 + 2JSD(Pdata+||Pg)]에서 -log4의 값이 더욱더 1.38에 가까워진다.
​    *[2JSD(Pdata+||Pg) ] : KL이라 보면 됨. (KL이 작아진다) = (D와 G의 분산이 적다) = (D loss가 1.38에 가깝다)&lt;/p&gt;
   &lt;br&gt;
&lt;p&gt;   &lt;em&gt;따라서&lt;/em&gt;  &gt;&lt;/p&gt;
&lt;p&gt;   분별할 수 없이 실제와 가까운 Fake Data가 생성된다.&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;위 원리에 따라,&lt;/p&gt;
&lt;p&gt;&lt;em&gt;학습이 안 된 상태에선&lt;/em&gt; &gt;&lt;/p&gt;
&lt;p&gt;G(z) ( = Fake Data. 이때 ‘z’는 input에서 들어온 random data임)의 분포가 오른쪽으로 치우친 상태로서 D(x)는 1에 가까운 값이 출력되고 D(G(z))는 0에 가까운 값이 출력된다.&lt;/p&gt;
&lt;p&gt;이때 D와 G는 아래 함수와 같은 상태임&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 377px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/7329e5014c8de29c438849a8359f8ee5/6146e/image-20200728190633284.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 16.216216216216214%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsSAAALEgHS3X78AAAAnUlEQVQI1z1PSwqFMBDz/sfxAuJKwR/oQt2KC0EQaj/W/iLTPt4iZCaESSYzxkNrB2LOLYDwA6LGWNKex0WEECClw31bKJU02oVInC2LQp5faFuBpuE4jhf7riOmSaKuOaqKoygY1lXFI+MoUZYs+vteYBgEuk5gnhUyakHG901NKZmObZvGeZrY8rpM1K31cM7De//3UwC1o+9o/gBXXeO5Ghev+wAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200728190633284&quot;
        title=&quot;image-20200728190633284&quot;
        src=&quot;/static/7329e5014c8de29c438849a8359f8ee5/6146e/image-20200728190633284.png&quot;
        srcset=&quot;/static/7329e5014c8de29c438849a8359f8ee5/12f09/image-20200728190633284.png 148w,
/static/7329e5014c8de29c438849a8359f8ee5/e4a3f/image-20200728190633284.png 295w,
/static/7329e5014c8de29c438849a8359f8ee5/6146e/image-20200728190633284.png 377w&quot;
        sizes=&quot;(max-width: 377px) 100vw, 377px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/112ffbe40ca0e7e655685f848da8bdb7/32056/image-20200728190622855.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 9.45945945945946%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsSAAALEgHS3X78AAAAbklEQVQI1yWMSwqAMAxEvf/1ClYENy3oQvBTbVqbyEji4jHhZZLOe0IIFfvOxrI86HuCcxnjSIixYp4f4zgY01Rsp1772lU3DH92KmsVbFvDujakxLhvMYgEpQhae3FdYg9yFpwnm2d+7VZn9Zofk++YH5r8gOMAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200728190622855&quot;
        title=&quot;image-20200728190622855&quot;
        src=&quot;/static/112ffbe40ca0e7e655685f848da8bdb7/fcda8/image-20200728190622855.png&quot;
        srcset=&quot;/static/112ffbe40ca0e7e655685f848da8bdb7/12f09/image-20200728190622855.png 148w,
/static/112ffbe40ca0e7e655685f848da8bdb7/e4a3f/image-20200728190622855.png 295w,
/static/112ffbe40ca0e7e655685f848da8bdb7/fcda8/image-20200728190622855.png 590w,
/static/112ffbe40ca0e7e655685f848da8bdb7/32056/image-20200728190622855.png 602w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;즉, D는 진짜 (X)와 가짜 G(z)를 잘 구별하고, &lt;code class=&quot;language-text&quot;&gt;G&lt;/code&gt;는 진짜 같은 가짜를 잘 못 만든다&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;em&gt;학습이 진행되면&lt;/em&gt; &gt; &lt;/p&gt;
&lt;p&gt;가짜 데이터 G(z)의 분포가 점점 Real Data( = X)의 분포와 유사해지고 D(G(z)) 값도 점차 커져서 D(x)값은 점차 작아진다 &lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;em&gt;학습이 완료되면&lt;/em&gt; &gt; &lt;/p&gt;
&lt;p&gt;Real data와 G(z)의 분포가 잘 일치하고 “D(x) = D(G(z)) = 0.5 “로 수렴한다.&lt;/p&gt;
&lt;p&gt;즉, 임의의 random data를 G에 입력해 나온 Fake data는 Real data와 유사한 분포 특성을 갖는 데이터가 출력된다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/4fb82e1ce3bea1e96e8be416befbd8a0/32056/image-20200728190502482.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 31.756756756756754%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsSAAALEgHS3X78AAABCElEQVQY00VR2XKDMAzM//9d+tRMkpeUI4AhYGMDPthq3ZBqZse2jtVKPu37DoK2LAuMMZjnGdZaOOfQdR3Ksvz4CN4ZI0IIOIw8p+NCSykJqYP3HjFERnIBwRjzQgyffPqtnTMO4tMRJAkVar2hKGtoMwlpkrcULDaTJ+lBxSH4TOrcgn54oVM9nHXiS38K2Z2E3ktHt6IbRihJ5GhaVtAMCmY2mKYJqioxjeN7RRvUS6MXBB//R6YyFvN8tjW+LxfUdYvNb3gUD5zPX7LbOZNeb1dU9fNN6HC/31AUP0iiLo9MkuMDqFJrjaqqpIFBjBFKqTwmp1jXVRrVGXwH2XPTNGjbNsdovxtm0E5Ax17BAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image 20200728190502482&quot;
        title=&quot;image 20200728190502482&quot;
        src=&quot;/static/4fb82e1ce3bea1e96e8be416befbd8a0/fcda8/image-20200728190502482.png&quot;
        srcset=&quot;/static/4fb82e1ce3bea1e96e8be416befbd8a0/12f09/image-20200728190502482.png 148w,
/static/4fb82e1ce3bea1e96e8be416befbd8a0/e4a3f/image-20200728190502482.png 295w,
/static/4fb82e1ce3bea1e96e8be416befbd8a0/fcda8/image-20200728190502482.png 590w,
/static/4fb82e1ce3bea1e96e8be416befbd8a0/32056/image-20200728190502482.png 602w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;참고:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; &lt;a href=&quot;http://blog.skby.net/gan-generative-adversarial-networks/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;http://blog.skby.net/gan-generative-adversarial-networks/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[NLP maLSTM]]></title><description><![CDATA[NLP Quora  : 질문 간 텍스트 유사도 분석 maLSTM : 맨하탄 거리 사용한 LSTM GloVe : 빈도 + 맥락(Embedding) 고려한 워드 패키지 FastText : hash…]]></description><link>https://jynee.github.io/tags#1st/NLP한글_4/</link><guid isPermaLink="false">https://jynee.github.io/tags#1st/NLP한글_4/</guid><pubDate>Fri, 07 Aug 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;nlp&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#nlp&quot; aria-label=&quot;nlp permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Quora  : 질문 간 텍스트 유사도 분석&lt;/li&gt;
&lt;li&gt;maLSTM : 맨하탄 거리 사용한 LSTM&lt;/li&gt;
&lt;li&gt;GloVe : 빈도 + 맥락(Embedding) 고려한 워드 패키지&lt;/li&gt;
&lt;li&gt;FastText : hash 값을 사용해 어딘가에 저장해두므로, 사전에 없는 단어를 입력해도 비슷한 걸 찾아 vector 값으로 나온다.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;glove--malstm&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#glove--malstm&quot; aria-label=&quot;glove  malstm permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GloVe &amp;#x26; maLSTM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;maLSTM = 맨하탄 거리 사용한 LSTM&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# Question-1, 2 입력용&lt;/span&gt;
K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;clear_session&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
inputQ1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_shape&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; trainQ1&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
inputQ2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;batch_shape&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; trainQ2&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# Question-1 처리용 LSTM&lt;/span&gt;
embQ1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Embedding&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;input_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;VOCAB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; output_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;EMB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputQ1&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
embQ1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dropout&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;rate&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;embQ1&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
lstmQ1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; LSTM&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;HIDDEN_DIM&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;embQ1&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
lstmQ1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;FEATURE_DIM&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; activation&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; kernel_regularizer&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;regularizers&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;l2&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;REGULARIZER&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lstmQ1&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
lstmQ1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dropout&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;rate&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lstmQ1&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# Question-2 처리용 LSTM&lt;/span&gt;
embQ2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Embedding&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;input_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;VOCAB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; output_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;EMB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputQ2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
embQ2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dropout&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;rate&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;embQ2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
lstmQ2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; LSTM&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;HIDDEN_DIM&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;embQ2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
lstmQ2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;FEATURE_DIM&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; activation&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; kernel_regularizer&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;regularizers&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;l2&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;REGULARIZER&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lstmQ2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
lstmQ2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dropout&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;rate&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lstmQ2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;위 코드처럼 Embedding을 각각 해줄 땐, &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;한글 사전 / 영어 사전 속 단어의 Vector 값을 각각 확인하는 것 처럼 사전의 근본이 다를 때&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;챗봇 Q&amp;#x26;A : Q는 형태소 분석을 하고, A는 형태소 분석을 하지 않을 때 처럼 결과값이 달라야 할 때.&lt;/p&gt;
&lt;p&gt;&quot;달라야 할 필요가 있을 때만&quot; 이렇게 사용한다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;각각 embedding 값에서 맨하탄 거리 측정&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;py&quot;&gt;&lt;pre class=&quot;language-py&quot;&gt;&lt;code class=&quot;language-py&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# Question-1, 2의 출력으로 맨하탄 거리를 측정한다.&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# lstmQ1 = lstmQ2 --&gt; mDist = 1&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# lstmQ1 - lstmQ2 = inf --&gt; mDist = 0&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# mDist = 0 ~ 1 사잇값이므로, trainY = [0, 1]과 mse를 측정할 수 있다.&lt;/span&gt;
mDist &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;exp&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lstmQ1 &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; lstmQ2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; axis&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; keepdims&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;ex:
a=np.array(np.arange(20).reshape(4,5)
np.sum(a,axis=0, keepdims=true)
*keepdims=true쓰면 원래 1차원으로 나와야 할 것이 원래 a의 차원대로 2차원으로 나옴&lt;/li&gt;
&lt;li&gt;Backend as k:네트워크 출력 결과에 어떤 연산을 취할 때
keepdims=true:원래 a값 구조대로 줌&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;compile&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;py&quot;&gt;&lt;pre class=&quot;language-py&quot;&gt;&lt;code class=&quot;language-py&quot;&gt;model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;inputQ1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; inputQ2&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; mDist&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;loss&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;mean_squared_error&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; optimizer&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;Adam&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lr&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.0005&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;glove--malstm--quora&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#glove--malstm--quora&quot; aria-label=&quot;glove  malstm  quora permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GloVe &amp;#x26; maLSTM &amp;#x26; Quora&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;동시발생 확률 고려&lt;br&gt;&lt;/li&gt;
&lt;li&gt;빈도 기반 문장 , 맥락(context) 고려&lt;br&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;순서&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;전처리가 완료된 학습 데이터를 읽어온다.&lt;/li&gt;
&lt;li&gt;Quora 데이터의 Vocabulary를 읽어온다.&lt;/li&gt;
&lt;li&gt;maLSTM 모델을 빌드한다.&lt;/li&gt;
&lt;li&gt;저장된 WE를 읽어온다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pre-trained GloVe 파일을 읽어와서 GloVe dictionary를 생성한다&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;    GloVe &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#Vocabulary&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 1라인씩 읽음. # line=[&apos;love&apos;,&apos;~&apos;,&apos;~&apos;..]. vector가 300개 들어있다.&lt;/span&gt;
        wv &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; line&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;split&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        word &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;join&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;wv&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 앞부분 # word=wv[0] 워드만.&lt;/span&gt;
        GloVe&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;word&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;asarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;wv&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dtype&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;float32&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 뒷부분 vector만.&lt;/span&gt;
    &lt;span class=&quot;token builtin&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;close&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 뒷부분 300개 &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;GloVe[&apos;love&apos;]을  실행하면 아래 vector가 출력된다. shape = (300,)
array([ 1.3949e-01,  5.3453e-01, -2.5247e-01, -1.2565e-01,  4.8748e-02,
1.5244e-01,  1.9906e-01, -6.5970e-02,  1.2883e-01,  2.0559e+00, ...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WE = np.zeros((VOCAB&lt;em&gt;SIZE, EMB&lt;/em&gt;SIZE))&lt;/p&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; word&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; i &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; word2idx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;items&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#여기서 word2idx는 quora 용으로 우리가 만든 사전 &lt;/span&gt;
      vec &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; GloVe&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;get&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;word&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#300개짜리 vector # GloVe.get(word) : 있으면 나오고 없으면 null 값이 나와서 프로그램이 멈추지 않음(get)을 쓰면. # glove[word]라고만 쓸 땐, 있으면 나오고 없으면 error 뜸 &lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; vec &lt;span class=&quot;token keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
          WE&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;i&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; vec&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;결과를 저장한다.&lt;/p&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;학습 데이터와 시험 데이터로 나눈다.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Question-1, 2 입력용(input)&lt;br&gt;&lt;/li&gt;
&lt;li&gt;공통으로 사용(범용)할 Embedding layer 빌드&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Question-1 처리용 LSTM model 빌드&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Question-2 처리용 LSTM model 빌드&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Question-1, 2의 출력으로 맨하탄 거리를 측정한다.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;  mDist &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;exp&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lstmQ1 &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; lstmQ2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; axis&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; keepdims&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Question-1, 2의 출력으로 맨하탄 거리를 측정한다.
lstmQ1 = lstmQ2 --&gt; mDist = 1
lstmQ1 - lstmQ2 = inf --&gt; mDist = 0
mDist = 0 ~ 1 사잇값이므로, trainY = [0, 1]과 mse를 측정할 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ol start=&quot;11&quot;&gt;
&lt;li&gt;학습&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;inputQ1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; inputQ2&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; mDist&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;loss&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;mean_squared_error&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; optimizer&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;Adam&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lr&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.0005&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
  &lt;br&gt;
&lt;ol start=&quot;12&quot;&gt;
&lt;li&gt;예측&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;trainY &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; trainY&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;reshape&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
testY &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; testY&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;reshape&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
hist &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;trainQ1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; trainQ2&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; trainY&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
               validation_data &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;testQ1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; testQ2&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; testY&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                 batch_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; epochs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
  &lt;br&gt;
&lt;ol start=&quot;13&quot;&gt;
&lt;li&gt;시험 데이터로 학습 성능을 평가&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;predicted &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;testQ1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; testQ2&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
predY &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;where&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;predicted &lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
accuracy &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;testY &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; predY&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;mean&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;\nAccuracy = %.2f %s&quot;&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;accuracy &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;%&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
  &lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;fasttext--malstm--quora&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#fasttext--malstm--quora&quot; aria-label=&quot;fasttext  malstm  quora permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FastText &amp;#x26; maLSTM &amp;#x26; Quora&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;subword 덕분에 어떤 단어라도 vector 형태로 만들어준다&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;model.wv.vocab.keys()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;한글 hash도 있다&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; gensim&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;models &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; fasttext&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;순서&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;전처리가 완료된 학습 데이터를 읽어온다.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Quora 데이터의 Vocabulary를 읽어온다.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;maLSTM 모델을 빌드한다.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# 이때, &lt;/span&gt;
EMB_SIZE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;300&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# pre-trained FastText의 vector size = 300이므로, 이와 동일하게 맞춘다.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;저장된 WE를 읽어온다&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;savedWeightEmbedding &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Pre-trained FastText 파일을 읽어와서 dictionary를 생성한다.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; fasttext&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load_facebook_vectors&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;./dataset/wiki.en.bin&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;빈(zero) 가중치 파일 생성&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;WE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;zeros&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;VOCAB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; EMB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Embedding의 w 값을 뽑아낼 수 있게 되었다. &lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;py&quot;&gt;&lt;pre class=&quot;language-py&quot;&gt;&lt;code class=&quot;language-py&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; word&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; i &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; word2idx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;items&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
WE&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;i&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;wv&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;word&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;결과 저장 &lt;br&gt;&lt;/li&gt;
&lt;li&gt;학습 데이터와 시험 데이터로 나눈다.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Question-1, 2 입력용(input)&lt;br&gt;&lt;/li&gt;
&lt;li&gt;공통으로 사용할 Embedding layer 빌드&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Question-1 처리용 LSTM model 빌드&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Question-2 처리용 LSTM model 빌드&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Question-1, 2의 출력으로 맨하탄 거리를 측정&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;mDist &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;exp&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;K&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lstmQ1 &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; lstmQ2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; axis&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; keepdims&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;lstmQ1 = lstmQ2 --&gt; mDist = 1
lstmQ1 - lstmQ2 = inf --&gt; mDist = 0
mDist = 0 ~ 1 사잇값이므로, trainY = [0, 1]과 mse를 측정할 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;
 &lt;br&gt;
&lt;ol start=&quot;10&quot;&gt;
&lt;li&gt;학습   &lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;inputQ1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; inputQ2&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; mDist&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;loss&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;mean_squared_error&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; optimizer&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;Adam&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lr&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.0005&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
  &lt;br&gt;
&lt;ol start=&quot;11&quot;&gt;
&lt;li&gt;
&lt;p&gt;예측&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;trainY &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; trainY&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;reshape&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
testY &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; testY&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;reshape&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
hist &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;trainQ1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; trainQ2&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; trainY&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; validation_data &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;testQ1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; testQ2&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; testY&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; batch_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; epochs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;시험 데이터로 학습 성능을 평가&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;predicted &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;testQ1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; testQ2&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
predY &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;where&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;predicted &lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
accuracy &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;testY &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; predY&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;mean&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;\nAccuracy = %.2f %s&quot;&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;accuracy &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;%&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
 &lt;br&gt;
&lt;h3 id=&quot;fasttext-연습&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#fasttext-%EC%97%B0%EC%8A%B5&quot; aria-label=&quot;fasttext 연습 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FastText 연습&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;패키지&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; gensim&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;models &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; FastText
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; gensim&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;test&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;utils &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; common_texts &lt;span class=&quot;token comment&quot;&gt;# 9개 문장&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[&apos;human&apos;, &apos;interface&apos;, &apos;computer&apos;],
[&apos;survey&apos;, &apos;user&apos;, &apos;computer&apos;, &apos;system&apos;, &apos;response&apos;, &apos;time&apos;],
[&apos;eps&apos;, &apos;user&apos;, &apos;interface&apos;, &apos;system&apos;],
[&apos;system&apos;, &apos;human&apos;, &apos;system&apos;, &apos;eps&apos;],
[&apos;user&apos;, &apos;response&apos;, &apos;time&apos;],
[&apos;trees&apos;],
[&apos;graph&apos;, &apos;trees&apos;],
[&apos;graph&apos;, &apos;minors&apos;, &apos;trees&apos;],
[&apos;graph&apos;, &apos;minors&apos;, &apos;survey&apos;]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;모델 빌드&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; FastText&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; window&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; min_count&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;build_vocab&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sentences&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;common_texts&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;train&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sentences&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;common_texts&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; total_examples&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;common_texts&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; epochs&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;bucket=hash table. 따라서 100이라 주면, model.wv.vectors&lt;em&gt;ngrams.shape가 100, n이 됨. hashing trick을 통과시키고 나오는 값들이 들어가는 곳으로, bucket 값이 크면 collision을 방지한다
size = EMB&lt;/em&gt;SIZE
window = 좌우 context 개수
min_count = 빈도 1개 이상(그니까 전부 다)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;word2vec 확인&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;어떤 단어라도 Hashing되어 vector값을 준다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;wv&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;human&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;wv&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;klakasdfsdjf&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;model.wv[&apos;human&apos;]
Out[16]:
array([-0.00893843, -0.03338013, -0.0210454 ,  0.03005639, -0.03349178], dtype=float32)&lt;/p&gt;
&lt;p&gt;model.wv[&apos;klakasdfsdjf&apos;]
Out[17]:
array([ 0.00078089, -0.01366953, -0.00929362,  0.00640602, -0.00282957], dtype=float32)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;wv&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocab
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;wv&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocab&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;keys&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;wv&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vectors_ngrams&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;(2000000, 5)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;참고: &lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;아마추어 퀀트, blog.naver.com/chunjein&lt;/li&gt;
&lt;li&gt;전창욱, 최태균, 조중현. 2019.02.15. 텐서플로와 머신러닝으로 시작하는 자연어 처리 - 로지스틱 회귀부터 트랜스포머 챗봇까지. 위키북스&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[NLP Kaggle competition 우승자가 제안한 새로운 접근방법을 배워보자]]></title><description><![CDATA[AUC Area under the roc curve (AUC) Confusion matix(Binaray classification)  predictionP N actual P TP FP N FN TN TPR  FPR  Thes…]]></description><link>https://jynee.github.io/tags#1st/NLP한글_3/</link><guid isPermaLink="false">https://jynee.github.io/tags#1st/NLP한글_3/</guid><pubDate>Thu, 06 Aug 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;auc&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#auc&quot; aria-label=&quot;auc permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AUC&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Area under the roc curve (AUC)&lt;/li&gt;
&lt;li&gt;Confusion matix(Binaray classification)&lt;/li&gt;
&lt;li&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;prediction&lt;br /&gt;P&lt;/th&gt;
&lt;th&gt;&lt;br /&gt;N&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;actual P&lt;/td&gt;
&lt;td&gt;TP&lt;/td&gt;
&lt;td&gt;FP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;N&lt;/td&gt;
&lt;td&gt;FN&lt;/td&gt;
&lt;td&gt;TN&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;TPR &lt;/li&gt;
&lt;li&gt;FPR &lt;/li&gt;
&lt;li&gt;Thes(임계치)가 작을수록 TPR이 높아진다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ROC 면적이 클수록 좋다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Area Under the ROC(AUC) : AUC 값을 0~1 사이의 값으로 표현.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;kaggle-competition&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#kaggle-competition&quot; aria-label=&quot;kaggle competition permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle Competition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;논문 분석: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alejandro Peláez외. Sring 2015. Sentiment analysis of IMDb movie reviews. Rutgers University &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&quot;code-classlanguage-textnegation-handlingcode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textnegation-handlingcode&quot; aria-label=&quot;code classlanguage textnegation handlingcode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;Negation Handling&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;새로운 접근&lt;/em&gt; &gt; &apos;&apos;&lt;strong&gt;Negation Handling&apos;&lt;/strong&gt;&apos;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hardly good → &lt;strong&gt;neg&lt;/strong&gt;.good&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&quot;code-classlanguage-textmutual-information상호-정보량code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textmutual-information%EC%83%81%ED%98%B8-%EC%A0%95%EB%B3%B4%EB%9F%89code&quot; aria-label=&quot;code classlanguage textmutual information상호 정보량code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;Mutual information(상호 정보량)&lt;/code&gt;&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(쿡북에서) 사전 생성할 때, 전체 단어 中 6,000개만 썼다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;6,000개: most common() 빈도가 가장 높은 6,000개의 단어를 선택하여 vocab을 만들고, 이걸 가지고 문서 전처리를 했었음.&lt;/li&gt;
&lt;li&gt;이때 핵심: &lt;strong&gt;빈도가 가장 높은&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;단어 빈도수를 수동 카운트&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;collection.counter()&lt;/li&gt;
&lt;li&gt;counter.most_common()&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;하지만 이 분은 Mutual information(상호정보량)이라는 개념을 사용했다.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;새로운 접근&lt;/em&gt; &gt; &lt;strong&gt;Mutual information&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;class인 Y와 일정한 관계가 높은 순서를 vocab 생성&lt;/li&gt;
&lt;li&gt;순서:&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;review 전처리&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;review&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;I like you&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I dislike you&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;모든 단어의 Mutual information 계산&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;단어(Unigram)&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;MI(공식에 의해 연산)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;like&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dislike&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&quot;I&quot;와 y는 무관 → x, y는 독립&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(x, y) = P(x|y) * P(y)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&apos;like, dislike&apos;와 y는 연관 → x, y는 종속 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(x, y) = P(x) * P(y)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ML에서 Entropy 공부 할 때, KL값으로 Cross Entropy 값을 구했었다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;KL : 두 분포의 정보량의 차이(두 분포의 유사성)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mutual Information의 KL&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;KL(P(x, y) || P(x) * P(y))&lt;/li&gt;
&lt;li&gt;예: MI(&apos;like&apos;) = ?&lt;/li&gt;
&lt;li&gt;P(x, y) = P(x|y) * P(y) = &gt; &lt;/li&gt;
&lt;li&gt;P(x|y = 0) * P(y = 0)  + P(x|y = 1) * P(y = 1)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(like|y = 0) * P(y = 0)&lt;/p&gt;
&lt;p&gt;P(like|y = 0) : label(y)가 0(neg)인 리뷰 中 &apos;like&apos; 단어가 등장한 비율&lt;/p&gt;
&lt;p&gt;P(y = 0) : 리뷰 25,000개 中 y=0(neg)인 비율&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;따라서 MI(&apos;like&apos;) = 0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&apos;I&apos;의 경우 긍/부정이 섞여 있어 MI 수치가 적다. &lt;/li&gt;
&lt;li&gt;&apos;MI 수치가 적다&apos;는 말은, 긍/부정이 명확하지 않다는 뜻이고, 이는 리뷰의 긍/부정을 분류하기 애매하다는 뜻이므로 실질적으로 목적을 이루는 데엔 도움이 되진 않는단 뜻이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;MI 높은 순서로 Sort&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;단어&lt;/th&gt;
&lt;th&gt;MI&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;like&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dislike&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
  &lt;br&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;
&lt;p&gt;상위 N% 선택하여 vocab 생성&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;상위만 선택한 이유: 분석에 별로 도움되지 않는 단어도 vocab에 포함해 생성해봤자 연산량만 늘어나고 분석에 실질적인 도움은 되지 않으니까 후순위로 밀어낸다.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TF - IDF&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;예: movie → 여러 리뷰에 등장 DF 가 높다 IDF가 낮다&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;감정분석&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;학습 기반(신경망 속 Embedding)이 아닌 사전 기반의 감정분석(VADER 알고리즘) 사용&lt;/li&gt;
&lt;li&gt;VADER 알고리즘&lt;/li&gt;
&lt;li&gt;리뷰 속 단어들을 사전에 등재된 단어에 따라 VADER Score를 매기고, 그 하나의 리뷰의 VADER Score 값을 평균낸 것&lt;br&gt;&lt;/li&gt;
&lt;li&gt;각 단어마다 score를 매겨 양수/음수에 따라 positive and negative를 구분한다&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;score 식 &gt; label = 1인 리뷰 中 &apos;like&apos;의 개수 - label = 0인 리뷰 中 &apos;like&apos;의 개수&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;평균, top k, bottom 값을 matrix로 해서 구해봄&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;word2vec&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;단어 vector들을 평균 내서 이걸 문장 vector로 쓰는 것&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;문제: 정보가 손실되어 별로 좋은 방법은 아니다.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;doc2vec&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;단어를 50~60% 선택할 때 score가 가장 좋았다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;하지만 대체적으로 많은 단어를 선택할 때 score가 좋아지긴 한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;doc2vec + TF-IDF : 의미적관계+구조적 관계를 보기 위해 CONCAT or average 등을 사용하여 두 값을 합침&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;감정분석&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;학습 기반&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;결과 ROC : 0.99259&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;참고: &lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;아마추어 퀀트, blog.naver.com/chunjein&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[NLP Ask Me Anything]]></title><description><![CDATA[NLP 분야에서 딥러닝의 고급 응용 DMN Ask Me Anything attention score layer story layer episodic memory layer answer layer 텍스트 자동 생성 예제 문장: I love you…]]></description><link>https://jynee.github.io/tags#1st/NLP응용_4/</link><guid isPermaLink="false">https://jynee.github.io/tags#1st/NLP응용_4/</guid><pubDate>Wed, 05 Aug 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;nlp-분야에서-딥러닝의-고급-응용&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#nlp-%EB%B6%84%EC%95%BC%EC%97%90%EC%84%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98-%EA%B3%A0%EA%B8%89-%EC%9D%91%EC%9A%A9&quot; aria-label=&quot;nlp 분야에서 딥러닝의 고급 응용 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP 분야에서 딥러닝의 고급 응용&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;DMN&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ask Me Anything&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;attention score layer&lt;/li&gt;
&lt;li&gt;story layer&lt;/li&gt;
&lt;li&gt;episodic memory layer&lt;/li&gt;
&lt;li&gt;answer layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;텍스트-자동-생성&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%9E%90%EB%8F%99-%EC%83%9D%EC%84%B1&quot; aria-label=&quot;텍스트 자동 생성 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;텍스트 자동 생성&lt;/h2&gt;
&lt;p&gt;예제 문장: I love you very much&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;문자 단위의 시계열 데이터 생성&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;아래처럼 되도록 생성: 보통의 시계열 Batch data처럼 생성&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/78dfc4b9095769d7132abeeec32f0606/fe9e8/image-20200822112750536.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 22.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsSAAALEgHS3X78AAAAdElEQVQY032Oyw7DIAwE+f8vRSBRhHkYG2mbuEoTKWkPc9rdsV3rHcyMOeeNEAK890aMESLy2DvYc0dEyDlDVW+klEBUMcYwVNdj72TBlVLw2oQiaheMLfxcFax1Sr75D/aOa62j1moSG13GzPMi0r/fHbs3O1U4qDSKIQAAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200822112750536&quot;
        title=&quot;image-20200822112750536&quot;
        src=&quot;/static/78dfc4b9095769d7132abeeec32f0606/fcda8/image-20200822112750536.png&quot;
        srcset=&quot;/static/78dfc4b9095769d7132abeeec32f0606/12f09/image-20200822112750536.png 148w,
/static/78dfc4b9095769d7132abeeec32f0606/e4a3f/image-20200822112750536.png 295w,
/static/78dfc4b9095769d7132abeeec32f0606/fcda8/image-20200822112750536.png 590w,
/static/78dfc4b9095769d7132abeeec32f0606/efc66/image-20200822112750536.png 885w,
/static/78dfc4b9095769d7132abeeec32f0606/fe9e8/image-20200822112750536.png 1146w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt; y값을 word 단위가 아니고 characters(ex: a, b, c, ... z) 로 설정&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LSTM으로 학습&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I love you를 넣어도 v가 나오게끔 신경망 속 activation은 softmax 함수를 쓴다.&lt;/li&gt;
&lt;li&gt;compile 시, loss 함수는 categorical_crossentropy 사용&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;순서:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;문장으로 이루어진 raw data 불러오기&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;전처리&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;word 아니고 character 단위로 분류&lt;/li&gt;
&lt;li&gt;시계열 x data 생성. (위 예시의 x 처럼)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Converting indices into vectorized format&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X, Y를 np.zeros &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Building&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;softmax: 출력층의 값이 [0.3, 0.4, 0.8] 등으로 나오면 이 총합이 1이 나오게끔 확률분포 다시 계산.
이때 나온 값들의 차이를 더 크게 조작하고 싶을 때, 베타가 들어간 식을 사용하여 계산.
이렇게 하면 model.predict(x) 시, 원하는 문자의 수치가 나올 확률이 높아진다
(역으로 원하지 않는 단어가 나올 확률은 적어진다.)&lt;/li&gt;
&lt;li&gt;softmax 함수를 쓰는 skip-gram은 계산량이 많단 단점이 있는데, 이를 SGNS가 보완한다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;예측치를 softmax 확률로 뽑아 다시 역연산(exp) 하는 함수 생성&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;np.random.multinomial로 sampling&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;pred_indices&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;preds&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; metric&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
   preds &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;asarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;preds&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;astype&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;float64&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
   preds &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;log&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;preds&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; metric
   exp_preds &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;exp&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;preds&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
   preds &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; exp_preds&lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;exp_preds&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
   probs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;multinomial&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; preds&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;argmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;probs&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;gt; * 다항 분포 (Multinomial distribution):
&amp;gt;
&amp;gt;   다항 분포는 여러 개의 값을 가질 수 있는 독립 확률변수들에 대한 확률분포로, 여러 번의 독립적 시행에서 각각의 값이 특정 횟수가 나타날 확률을 정의한다. 다항 분포에서 차원이 2인 경우 이항 분포가 된다.
&amp;gt;
&amp;gt;   &amp;gt; 출처: [위키백과. 다항분포]([https://ko.wikipedia.org/wiki/%EB%8B%A4%ED%95%AD_%EB%B6%84%ED%8F%AC](https://ko.wikipedia.org/wiki/다항_분포))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Train &amp;#x26; Evaluate the Model&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;batch&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;randint로 random하게 시작하도록 설정&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;확률 임의 설정 후 단어 생성&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[0.2, 0.7,1.2] 처럼 &lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; diversity &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;a &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;array&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
b &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;
e &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;exp&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;a&lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;b&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;e&lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;e&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&apos;0.9&apos;처럼 유독 하나의 값이 클 경우 다른 값들과의 차이가 더 커짐 &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;print(e/np.sum(e))&lt;br/&gt;[0.40175958 0.2693075  0.32893292]&lt;/th&gt;
&lt;th&gt;print(e/np.sum(e))&lt;br/&gt;[0.47548496 0.23611884 0.2883962 ]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;a = np.array([0.6, 0.2, 0.4])&lt;br/&gt;b = 1.0&lt;br/&gt;e = np.exp(a/b)&lt;/td&gt;
&lt;td&gt;a = np.array([0.9, 0.2, 0.4])&lt;br/&gt;b = 1.0&lt;br/&gt;e = np.exp(a/b)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;model.predict(x):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;예측(predict): model.predict(x) = &gt; [0.01, 0.005, 0.3, 0.8 ...]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;문자 추출:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;sys&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;stdout&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;write&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;pred_char&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
sys&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;stdout&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;flush&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-textdmncode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textdmncode&quot; aria-label=&quot;code classlanguage textdmncode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;DMN&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dynamic Memory Networks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;아래 5가지의 N/W가 결합되어 있는 모습&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Input Module&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Question Module&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Episodic Memory Module&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Answer Module&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;attention score N/W&lt;/code&gt;&lt;/strong&gt; (FNN)&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;code-classlanguage-textask-me-anythingcode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textask-me-anythingcode&quot; aria-label=&quot;code classlanguage textask me anythingcode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;Ask Me Anything&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Q → A: Question &amp;#x26; Answering&lt;/li&gt;
&lt;li&gt;DL 사용&lt;/li&gt;
&lt;li&gt;논문 저자는 GRU 사용&lt;/li&gt;
&lt;li&gt;특징: Q&amp;#x26;A를 기억하는 하나의 경험 단위인 Episode를 기억하는 장치가 있다.&lt;/li&gt;
&lt;li class=&quot;task-list-item&quot;&gt;
&lt;p&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 순서:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Input 문장(text sequence)과 &lt;/li&gt;
&lt;li&gt;attention 연산이 들어간 question 받아 &lt;/li&gt;
&lt;li&gt;attention 연산: attention score&lt;/li&gt;
&lt;li&gt;episodic memory 구성한 후, &lt;/li&gt;
&lt;li&gt;일반적인 답변을 줄 수 있게끔 구성한 네트워크&lt;/li&gt;
&lt;li&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/a3b05891cf69b95d604a666ba46c6279/74d4e/image-20200731122306179.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 60.810810810810814%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsSAAALEgHS3X78AAAB5klEQVQoz21Ti27bMAz0/3/bhg1Fk3RZ0qax3Tp+yA9ZsR62byRtZx0wAowU6UgeqXOkh4C6d+juXrw1bvEv++2uWfds/SPuL4bvIw70YcI8z5jI/2d8zPdhnCSIrTUeLoz/4Pgs0oMXIJu1Fvv9HrvdDr9PJxxeXmR1bknCBTPVo2h6xIUmIiOOxyPFHGCHAcaOiJjqOE0PKs5ZSTyGAGeX/bwy54Tt2jIzZSLBO8FRMHh8wpCpplUvIOMm9FTpUxlZ+T8XTUots3rLGmIWMLhRZqjIP+s7YcMyw616pjQ6Q4w8gz3eKfBuPe2DrCkl3KzWVtpjY2zZmkcH0QYqGiMJPSV05NdbA0tgdp7V/pzi9HpBkqZQSuH8dkWSFURglLjNIj64E4ucEpqVEXte98QsiNsw49drgm8/nvD95zOOpzOeng+4JJmM4KPsJLEhbMRzYE05OiD1gJ+H4mXd3NMP33GTZnkTaDtRoWXPGBZKRaOI+GXGVTZKVUiSGB/UVhzHuF6prSRBU9dyPz50OMuDtZ3G++Ui3veaHpBko7/IRusOZVmgbRqoqkKe5yiKHL3Wq6pmUYRgichgHUq6z2830SEXkS+FnfsfqFcqIjPjlRQjPvhZZLF9XmzL5xhkHAtmEhn9AbSIoN9wa3B/AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200731122306179&quot;
        title=&quot;image-20200731122306179&quot;
        src=&quot;/static/a3b05891cf69b95d604a666ba46c6279/fcda8/image-20200731122306179.png&quot;
        srcset=&quot;/static/a3b05891cf69b95d604a666ba46c6279/12f09/image-20200731122306179.png 148w,
/static/a3b05891cf69b95d604a666ba46c6279/e4a3f/image-20200731122306179.png 295w,
/static/a3b05891cf69b95d604a666ba46c6279/fcda8/image-20200731122306179.png 590w,
/static/a3b05891cf69b95d604a666ba46c6279/efc66/image-20200731122306179.png 885w,
/static/a3b05891cf69b95d604a666ba46c6279/74d4e/image-20200731122306179.png 1157w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;출처: Ankit Kumar외, 2016.05, Ask Me Anything: Dynamic Memory Networks for Natural Language Processing. 에서 &apos;attention score&apos; 추가&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;strong&gt;attention process&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;attention score 계산&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;attention score&lt;/li&gt;
&lt;li&gt;여러 문장에 Epsodic stroy가 있고 이것에 대한 답을 찾을 때 질문과 가장 관련이 높은 (저장된) 문장에 점수를 매기기 위한 계산을 수행함 &lt;/li&gt;
&lt;li&gt;즉, 답을 내기 위해 어떤 문장에 attention을 해야 하는지 attention score로 계산하는 알고리즘&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;기계번역, test 분류, part-of-speech tagging, image captioning, Dialog system(chatbot) 가능&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;mission: 주어진 Question의 의미를 파악할 수 있도록 네트워크를 구성해야 한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&apos;의미를 파악할 수 있도록&apos; : 조응어(Anaphora resolution) 해석.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li class=&quot;task-list-item&quot;&gt;
&lt;p&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;code class=&quot;language-text&quot;&gt;input module&lt;/code&gt; = story module&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Input에 넣을 문장들을 1행으로 붙이고, [EOS] 로 구분&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;문장 1&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;문장 2&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;문장3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;When I was young, I passed test&lt;/td&gt;
&lt;td&gt;&amp;#x3C;EOS&gt;&lt;/td&gt;
&lt;td&gt;But, Now Test is so crazy&lt;/td&gt;
&lt;td&gt;&amp;#x3C;EOS&gt;&lt;/td&gt;
&lt;td&gt;Because The test level pretty hard more and more.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Embedding layer 투입&lt;/li&gt;
&lt;li&gt;RNN 거쳐서&lt;/li&gt;
&lt;li&gt;Hidden layer 출력은 다시 n개의 문장(c1, c2, c3 등)으로 출력&lt;/li&gt;
&lt;li&gt;episodic memory module 투입&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li class=&quot;task-list-item&quot;&gt;
&lt;p&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;code class=&quot;language-text&quot;&gt;Question module&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Question 문장 투입&lt;/li&gt;
&lt;li&gt;Embedding layer 투입&lt;/li&gt;
&lt;li&gt;RNN 거쳐서&lt;/li&gt;
&lt;li&gt;episodic memory module 투입&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li class=&quot;task-list-item&quot;&gt;
&lt;p&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;code class=&quot;language-text&quot;&gt;episodic memory module&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input module(문장마다)+Question module+ atttention mechanism출력된 걸 반복해서 내부의 episodic memory를 반복 update&lt;/li&gt;
&lt;li&gt;&quot;어떻게 update?&quot;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;input module의  embedding value과 atttention score 계산하여 RNN layer에 통과 시키기&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이때, &lt;code class=&quot;language-text&quot;&gt;atttention score&lt;/code&gt;:
atttention score layer의 출력층에서 나온 w인&lt;code class=&quot;language-text&quot;&gt;g&lt;/code&gt;를 input module의 출력값인 c1,c2,c3 등과 계산한 값&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Question module의 embedding value 값을 RNN layer에 통과 시키기&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이때, w = &lt;code class=&quot;language-text&quot;&gt;Q&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2를 answer을 output할 Answer Module의 RNN layer에 통과 시킴&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이때, w = &lt;code class=&quot;language-text&quot;&gt;m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;episodic memory module의 RNN layer를 반복할 때마다 &lt;code class=&quot;language-text&quot;&gt;attetion score&lt;/code&gt;계산&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;그렇게 해서 나온 attetion score 값 중 가장 높은 것(g) 찾음&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;atttention mechanism&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;그렇게 해서 나온 attetion score 값 중 가장 높은 것(g) 찾고&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;g로 다시 네트워크 형성&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2층 구조가 됨 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;memory update mechanism&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;attention score로 가중 평균&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;용어: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;c&lt;/code&gt;: Input의 출력&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;m&lt;/code&gt;: episodic memory module 출력값이자 attention score의 입력값&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;q&lt;/code&gt;: question layer의 출력값&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;g&lt;/code&gt;: attention score의 출력값&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&quot;code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code&quot; aria-label=&quot;code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;code:&lt;/h3&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/9d93b7c1a2b416814ac294610a2f0c57/c5bb3/image-20200812161218399.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 141.2162162162162%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAYAAABh2p9gAAAACXBIWXMAAAsSAAALEgHS3X78AAAFR0lEQVRIx3VWaVvaaBTl/3+azjNLq+1oizp1aREEK1oBQRZlDzthSSAJewhLEmhtz9w3QDva9sN9IiSc3HvOuefVZsw7eFyKdZ3PJCijDOaGCHMpYaLxmI+aGEsVlJMhFAv34MtJ5HMx1Pg0FkYP+kyBbQXS/Q6od60yjS5qgwB8iQNcRexweJ+hXIjCHAgo3vnh93vgOTvCnn0bfp8bGxzbo+5mHeiaBINqMhQgCRXEwj4c7P2Dvb2X4JJBzNJRfCok0CgmcPHhPexvthCnF+jT1WSPAFnLRq8OrVVCjYvROFGrAtRBNHwFoZnDpMNjQWPP6Llep45yI4y84kGp44VKtNg2nFmAVJ8WA+g0VkepAg9jCI08enIdjRqHHBfFkt0nShZmH5raQvzeR0A1TKbNNYesq9lqXAaqUHeJ6DXcvh0keCfqnTCkMQkgxsClIjAJbEkCtJp5eNzHNPI2gv4LEqVvYaxGpgdYzXoNJImPw8PXcJySEE47vFcnaClpDNUq5HbZUpNVu1VEMHAB++tthOiqrxuyGdUsjEYOJnU2UXhUKinrzaGAF+eu9ySKH1NVxkxTLA6XNCr7oaaKSGcDCEc+IJWmZ6ZkL0tlpQZDKEInQL1LRBPZTeLLdW5HJO1AuX4LdcpjMKyiJRQsQNaNTtznZA+ykgvl7hVGWpUAewQ4aMKo52A0CzCHIgb9BvwfPThybsHp24InsItaN4TuoEiAxTWgYhXjk5VuCbvyso0pxvibk/fEYhwB/zlS5LeTQzuyyQgSsSDdUzAd030a+dMacOUK5ZuYm7KxL5b6ANqojXY1gxGNXaGVuidxqpUk8cZhNpEw1doEmF91xEaebbz7fx+zDucypnoTg1EFgpCllSPCx23LBmwDRCFveZN9ZoAT615vPeL3Yi+yVk/p5xArkpoFB+45D9rDBMYj0QJhIzI1Le+ZK8DN5x9DZT3yldeJa68b8VgAHy9dcLzbx5nrkATIQ5ZKUIdN60HW1Qqw9W3snwJm0rcIBS9wsP8KL54/w+vd55QqSfQ7DcitMnErWRwvzSeAv+qwRLmWuA+gVIgjmwmjkLuD0s1jZjbQEOPoUCYONPIf45AWQBuJ6w6VnwO2xIL1x9cHFQ+fR8CXCWQtgVj5GOc3u3AHt8FLt/hK30vVNCa0fibbll8BMuKZCJbslGkDShm1LyKbiuHkeB+X5y4ELpzoyhW0qEOVJydQVho/CLM2ttDgoFK0P1CH004NQjYMlcTgKdovKZEziRsM6xyGtI6jbg1j2qgvxtrcT4SxfCgKORTES4Ryh7iJ7yFeO0WFQDp3N1gSSJu7QyV8De72CnUKjjTxLZADFmsen3Jpk9sl5PMRnLkPcO134vR0H3Y6J1KpkLXXtyEvXNTpPrnATdejt2/gcZ2QC+q0hkMs5v3HgBLlGl9Ko0p15jzCzqu/sPXidxxTJvYo7nPZKDhSf59e8vzv3/Bm70+U+DDaShbqhEdX5b7v9mZkFj1yJ4d0moJgKoO9pE984YuGGimbvAvScTAlPw4h9u5Rln1oj+4hDKJoDsJg62tuROH5BD7GaYzIFjL8B2vFmOdYIAxpZEZJouSGOIySJzniro/PxogA+pbhF1SPRmbJcuE+ReD6HPVaZnVQLYdghn/77w5evfwD75w76E4LUDQOc52EoK1h0fVUFGtkZht2TDocB8ilbmGy1aKxWWcBnwdO51tUMjHozTKWLR4GxRsLY2Mi/9zY/S5vdZMiMIEOb5OOTTOfwIjOF/ZvRijohUQvNYlTnUT6lvCUj4be+cGL/wHNlt10zii32AAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200812161218399&quot;
        title=&quot;image-20200812161218399&quot;
        src=&quot;/static/9d93b7c1a2b416814ac294610a2f0c57/fcda8/image-20200812161218399.png&quot;
        srcset=&quot;/static/9d93b7c1a2b416814ac294610a2f0c57/12f09/image-20200812161218399.png 148w,
/static/9d93b7c1a2b416814ac294610a2f0c57/e4a3f/image-20200812161218399.png 295w,
/static/9d93b7c1a2b416814ac294610a2f0c57/fcda8/image-20200812161218399.png 590w,
/static/9d93b7c1a2b416814ac294610a2f0c57/c5bb3/image-20200812161218399.png 680w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;순서&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;패키지 불러오기 &lt;br&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;전처리&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;2-1. Document Data processing(raw data)&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;데이터 불러오기&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;3-1. Raw Document Data 불러오기 &lt;/li&gt;
&lt;li&gt;3-2. train/test data split 해서 가져오기&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;vocab 만들기&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4-1. Train &amp;#x26; Test data를 한꺼번에 묶어서 vocab을 만듦&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;collections.Counter()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4-2. word2indx / indx2word 만듦&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;padding&lt;/code&gt;&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;벡터화&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;5-1. vocab_size 변수 설정&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;len(word2indx)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;5-2. story와 question 각각의 max len 변수 설정&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;뒤에서 padding 맞춰 주려고 max len 설정해줌&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;5-3. 벡터화 시킴&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;raw data&lt;/code&gt;와 &lt;code class=&quot;language-text&quot;&gt;word2indx&lt;/code&gt;, 각 모듈(story, question)의 &lt;code class=&quot;language-text&quot;&gt;maxlen&lt;/code&gt;을 함수에 넣어 &lt;code class=&quot;language-text&quot;&gt;padding&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;categorical&lt;/code&gt; 등을 진행함&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;모델 빌드&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;6-1. train/test data split&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이때, Xstrain, Xqtrain, Ytrain = &lt;code class=&quot;language-text&quot;&gt;data_vectorization&lt;/code&gt;(data&lt;em&gt;train, word2indx, story&lt;/em&gt;maxlen, question&lt;em&gt;maxlen) 이고,
data&lt;/em&gt;vectorization의 return 값은 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;pad_sequences(Xs, maxlen=story_maxlen)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;pad_sequences(Xq, maxlen=question_maxlen)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;to_categorical(Y, num_classes=len(word2indx))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;6-2. Model Parameters 설정&lt;/li&gt;
&lt;li&gt;6-3. Inputs&lt;/li&gt;
&lt;li&gt;6-4. Story encoder embedding&lt;/li&gt;
&lt;li&gt;6-5. Question encoder embedding&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;6-6. 모듈 만들어줌&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Question module는 위에서 만들어준 걸로 사용함&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;attention score layer&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;dot&lt;/strong&gt;으로 만듦&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;story module&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이 layer는 story layer의 input에서 시작하여 question layer는 건너뛰고 또 다른 embedding layer를 거쳐, 추후에 dot layer와 add를 &lt;strong&gt;해주려고 만듦&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;episodic memory module&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dot한 layer와 바로 위의 story&lt;em&gt;encoder&lt;/em&gt;c를 &lt;strong&gt;add&lt;/strong&gt;해서 만들어지게 됨&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;answer module&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;episodic memory layer(response) + quetion layer&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;compile&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;model = Model(inputs=&lt;strong&gt;[story&lt;em&gt;input, question&lt;/em&gt;input]&lt;/strong&gt;, outputs=output)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input에 story와 question 두 개 써줬단 것!&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ol start=&quot;8&quot;&gt;
&lt;li&gt;**fit **&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;loss plot&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;정확도 측정(predict)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;적용&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/91702bc3f10f90de85396cbf5fb4ba3e/df56e/image-20200803190130197.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 99.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAAClklEQVQ4y51Uia6bMBDk/7+vUl+TkAQIl8Hc9zXdcUKVRnmRWksWxqxnZ2fHWNu2IcsmxGpCrgokKobWGkqep9MRtn3C5eJgmjaYsW4Yux5xGCJNUkRRhOPhgMPxiCJVsPphkeAV0yyBfY++qZDqSkArRLGWAxp53mCeFyzrgq2fMNcdWl2gLSU2TVHXNZqmlUQNrHHP/BhL30r2xDBVwiAMYziOg9vtJkwvaOVwlmWSlIlyw/B6vcIPAnjXM6y8aNCTmcy8KCRYI2Mpno9CDpRlicAPDBMClRJTF6XZ5wyldO5rnaHQCSzPi4xmu262bSOQoJvrQQURMgHlt8v5DN/34QibNFbCUBLk2R/mjusi9FxYw7ii61q0bWsOsgxqUtfCINWo6gZN2yGMYhSimRKmStaDNIYNHYcBmzRqW1Zs1LDvF0M3zzMD6LqO0YwZA8dDJp1z7V8iwRXxTbS8nJBKJfPQQzqFsakh6DJbacAEa1k2DMOKcdz4XZ6ree+6GVVZQ8c+sutPKPsHtPOFwv1CqRXWSZjNgzhDgIQp2bK9Fju7PTYIvqz3Ncc4jkbDIIxMyYlIEKsEZVXdHSFlzmTxwCCwtR9+HXsSDjqgFhAt+oViD6UUVBwb3dd1/SvW2l9e5/MgoLGM2CQWIFqFoAR8JWG9Y2ZumGSuhNXuN67vN6IxgK40jabmmnaapunO8BMggQoxMp+0093A2oBwcj9JEhOzLMt7wGfQ19F1nWFKZjQ0mRHsWfNvGb7TlIAsm9eQTMmQe8/n3jL8TgKCsERO/hB4AciS375tyifG9OUgV42NITjL5fpjyZ9AWR7LDcSLZ/lZUEdK8F8l7ww5CUI/UkMyfI75Zw1paGpIH+4/3t0yjP0NP0kJW9zPtykAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200803190130197&quot;
        title=&quot;image-20200803190130197&quot;
        src=&quot;/static/91702bc3f10f90de85396cbf5fb4ba3e/fcda8/image-20200803190130197.png&quot;
        srcset=&quot;/static/91702bc3f10f90de85396cbf5fb4ba3e/12f09/image-20200803190130197.png 148w,
/static/91702bc3f10f90de85396cbf5fb4ba3e/e4a3f/image-20200803190130197.png 295w,
/static/91702bc3f10f90de85396cbf5fb4ba3e/fcda8/image-20200803190130197.png 590w,
/static/91702bc3f10f90de85396cbf5fb4ba3e/efc66/image-20200803190130197.png 885w,
/static/91702bc3f10f90de85396cbf5fb4ba3e/c83ae/image-20200803190130197.png 1180w,
/static/91702bc3f10f90de85396cbf5fb4ba3e/df56e/image-20200803190130197.png 1188w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;패키지 불러오기&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; collections
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; itertools
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; nltk
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; np
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; matplotlib&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;pyplot &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; plt
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; random
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tensorflow&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;keras&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;layers &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Activation&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Dropout
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tensorflow&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;keras&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;layers &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; LSTM&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Permute
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tensorflow&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;keras&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;layers &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; Embedding
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tensorflow&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;keras&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;layers &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; Add&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Concatenate&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Dot
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tensorflow&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;keras&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;models &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; Model
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tensorflow&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;keras&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;preprocessing&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;sequence &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; pad_sequences
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tensorflow&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;keras&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;utils &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; to_categorical&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;전처리&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EC%A0%84%EC%B2%98%EB%A6%AC&quot; aria-label=&quot;전처리 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;전처리&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Raw Document Data processing &lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# 문서 내용 예시 : 3문장의 story(episodic story)&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# 현재까지 NLP는 한 문장 안에서 단어들의 의미를 파악하고 이를 통해 한 개의 문장을 분석하는 수준에 그쳐있다(step1 수준).&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# 이때, episodic story는 &apos;한 문장 안&apos;이 아니라 문장 &apos;간&apos;의 단어들의 관계(=문장 간의 관계)를 파악하는 데에 의의가 있으며 따라서 매우 분석이 어렵다(이는 시의 영역이다. step2). 나아가 문단(Paragrape) 간의 관계까지 파악할 필요가 있다(이는 소설의 영역이다. step3).  &lt;/span&gt;
&lt;span class=&quot;token triple-quoted-string string&quot;&gt;&quot;&quot;&quot;
data 생김새
# 1 Mary moved to the bathroom.\n
# 2 Daniel went to the garden.\n
# 3 Where is Mary?\tbathroom\t1 
&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;## Question과 answer은 #\t : tab 으로 구분되어 있다.&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# Return: # 3개(Stories, question, answer)를 return 해줌 &lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# Stories = [&apos;Mary moved to the bathroom.\n&apos;, &apos;John went to the hallway.\n&apos;]&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# questions = &apos;Where is Mary? &apos;&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# answers = &apos;bathroom&apos;&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;#----------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;get_data&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;infile&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
  stories&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; questions&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; answers &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
  story_text &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
  fin &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;infile&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
  &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; fin&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
      lno&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; text &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; line&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;split&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;\t&quot;&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; text&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# &gt;data 생김새&amp;lt;에서 \t가 있는 3번을 말하는 것임 &lt;/span&gt;
          question&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; answer&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; _ &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; text&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;split&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;\t&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;#\t으로 구분해서 quetion과 answer 구분  # 숫자(ex:1)&lt;/span&gt;
          stories&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;append&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;story_text&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
          questions&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;append&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;question&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
          answers&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;append&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;answer&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# &gt;data 생김새&amp;lt; 에서 3번의 \t 앞의 answer문 &lt;/span&gt;
          story_text &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;token keyword&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
          story_text&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;append&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;text&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 사실상 해당 함수는 else 부터 시작하는 것. &lt;/span&gt;
  fin&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;close&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; stories&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; questions&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; answers&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;데이터-불러오기&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0&quot; aria-label=&quot;데이터 불러오기 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;데이터 불러오기&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Raw Document Data 불러오기 &lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;Train_File &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;./dataset/qa1_single-supporting-fact_train.txt&quot;&lt;/span&gt;
Test_File &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;./dataset/qa1_single-supporting-fact_test.txt&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;get the data&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;data_train &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; get_data&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Train_File&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 출력: stories, questions, answers&lt;/span&gt;
data_test &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; get_data&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Test_File&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;\n\nTrain observations:&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;data_train&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;Test observations:&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;data_test&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;\n\n&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Train observations: 10000 Test observations: 1000 &lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;vocab-만들기&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#vocab-%EB%A7%8C%EB%93%A4%EA%B8%B0&quot; aria-label=&quot;vocab 만들기 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;vocab 만들기&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Building Vocab dictionary from Train &amp;#x26; Test data &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train &amp;#x26; Test data를 한꺼번에 묶어서 vocab을 만듦 &lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;dictnry &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; collections&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Counter&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# collections.Counter() 이용하여 단어들이 사용된 count 조회할 예정 &lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; stories&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; questions&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; answers &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;data_train&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; data_test&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; story &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; stories&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; sent &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; story&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; word &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; nltk&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;word_tokenize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sent&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
              dictnry&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;word&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;lower&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; question &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; questions&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; word &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; nltk&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;word_tokenize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;question&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
          dictnry&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;word&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;lower&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; answer &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; answers&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; word &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; nltk&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;word_tokenize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;answer&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
          dictnry&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;word&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;lower&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;word2indx / indx2word 만듦&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# collections.Counter()과 구조는 같은데, 단어 index는 1부터 시작하게 바꿔줌.  &lt;/span&gt;
word2indx &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;w&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;i&lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; i&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;w&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;_&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;dictnry&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;most_common&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt; 
word2indx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;PAD&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# padding&lt;/span&gt;
indx2word &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;v&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;k &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; k&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;v &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; word2indx&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;items&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt; 
&lt;span class=&quot;token comment&quot;&gt;# 위에서 word2indx[&quot;PAD&quot;] 해줘서 print(indx2word) 하면, 맨 마지막에 &apos;,0: &apos;PAD&apos;&apos; 가 들어가 있다. &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;벡터화&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EB%B2%A1%ED%84%B0%ED%99%94&quot; aria-label=&quot;벡터화 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;벡터화&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;vocab_size&lt;/code&gt; 변수 설정&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;vocab_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;word2indx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# vocab_size = 22 -&gt; 즉 21개 단어만이 쓰인 것(하나는 패딩)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;vocabulary size:&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;word2indx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;word2indx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;vocabulary size: 22&lt;/li&gt;
&lt;li&gt;{&apos;to&apos;: 1, &apos;the&apos;: 2, &apos;.&apos;: 3, &apos;where&apos;: 4, &apos;is&apos;: 5, &apos;?&apos;: 6, &apos;went&apos;: 7, &apos;john&apos;: 8, &apos;sandra&apos;: 9, &apos;mary&apos;: 10, &apos;daniel&apos;: 11, &apos;bathroom&apos;: 12, &apos;office&apos;: 13, &apos;garden&apos;: 14, &apos;hallway&apos;: 15, &apos;kitchen&apos;: 16, &apos;bedroom&apos;: 17, &apos;journeyed&apos;: 18, &apos;travelled&apos;: 19, &apos;back&apos;: 20, &apos;moved&apos;: 21, &apos;PAD&apos;: 0}&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;story와 question 각각의 &lt;code class=&quot;language-text&quot;&gt;max len&lt;/code&gt; 변수 설정&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;story_maxlen &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;
question_maxlen &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; stories&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; questions&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; answers &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;data_train&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; data_test&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; story &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; stories&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
      story_len &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;
      &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; sent &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; story&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
          swords &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; nltk&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;word_tokenize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sent&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
          story_len &lt;span class=&quot;token operator&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;swords&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; story_len &lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt; story_maxlen&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
          story_maxlen &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; story_len &lt;span class=&quot;token comment&quot;&gt;# story 중 가장 긴 문장 찾기(=단어가 가장 많은 거)&lt;/span&gt;
          
  &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; question &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; questions&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
      question_len &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nltk&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;word_tokenize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;question&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; question_len &lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt; question_maxlen&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; 
          question_maxlen &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; question_len &lt;span class=&quot;token comment&quot;&gt;# question 중 가장 긴 문장 찾기 &lt;/span&gt;
          
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;Story maximum length:&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; story_maxlen&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Question maximum length:&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; question_maxlen&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Story maximum length: 14 Question maximum length: 4&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Converting data into &lt;code class=&quot;language-text&quot;&gt;Vectorized&lt;/code&gt; form &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위의 문장을 수치화함&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;data_vectorization&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; word2indx&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; story_maxlen&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; question_maxlen&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;  
  Xs&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Xq&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Y &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
  stories&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; questions&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; answers &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; data
  &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; story&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; question&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; answer &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;stories&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; questions&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; answers&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
      xs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;word2indx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;w&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;lower&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; w &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; nltk&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;word_tokenize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;s&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; s &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; story&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# vocab의 index로 단어를 표시한다(수치화한다)&lt;/span&gt;
      xs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;itertools&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;chain&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;from_iterable&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;xs&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# chain.from_iterable([&apos;ABC&apos;, &apos;DEF&apos;]) --&gt; [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;D&apos;, &apos;E&apos;, &apos;F&apos;]&lt;/span&gt;
      xq &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;word2indx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;w&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;lower&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; w &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; nltk&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;word_tokenize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;question&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
      Xs&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;append&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;xs&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
      Xq&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;append&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;xq&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
      Y&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;append&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;word2indx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;answer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;lower&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# Y = answer&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; pad_sequences&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Xs&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; maxlen&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;story_maxlen&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; pad_sequences&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Xq&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; maxlen&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;question_maxlen&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;\
         to_categorical&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Y&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; num_classes&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;word2indx&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
         &lt;span class=&quot;token comment&quot;&gt;# 가장 긴 문장(maxlen=story_maxlen))을 기준으로 문장의 길이를 통일시킨다. 이것보다 짧은 부분은 padding(0)으로 채움&lt;/span&gt;
         &lt;span class=&quot;token comment&quot;&gt;# y: anwser이고, 여기선 한 단어로 나온다. 즉, 한 단어 = 숫자 1개 &lt;/span&gt;
         &lt;span class=&quot;token comment&quot;&gt;# to_categorical: 안 쓰고 sparse categorical을 써도 ok &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;함수 data_vectorization() 中&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;xs = [[word2indx[w.lower()] for w in nltk.word_tokenize(s)] for s in story]&lt;/p&gt;
&lt;p&gt;&lt;em&gt;xs&lt;/em&gt; &gt;
Out[19]: [[8, 7, 20, 1, 2, 13, 3], [10, 19, 1, 2, 17, 3]]
&lt;em&gt;story&lt;/em&gt; &gt;
Out[20]: [&apos;John went back to the office.\n&apos;, &apos;Mary travelled to the bedroom.\n&apos;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;xs = list(itertools.chain.from_iterable(xs))
&lt;em&gt;xs&lt;/em&gt; &gt;
Out[22]: [8, 7, 20, 1, 2, 13, 3, 10, 19, 1, 2, 17, 3]&lt;/li&gt;
&lt;li&gt;Xs.append(xs) 해줌으로써
for문 통해서 output 된 것들을 list 형태로 축적해줌 &lt;/li&gt;
&lt;li&gt;padding 해줌
pad&lt;em&gt;sequences(Xs, maxlen=story&lt;/em&gt;maxlen) # story_maxlen = 14
Out[31]: array([[ 0,  8,  7, 20,  1,  2, 13,  3, 10, 19,  1,  2, 17,  3]])&lt;/li&gt;
&lt;li&gt;pad&lt;em&gt;sequences(Xs, maxlen=story&lt;/em&gt;maxlen)
Out[31]: array([[ 0,  8,  7, 20,  1,  2, 13,  3, 10, 19,  1,  2, 17,  3]])&lt;/li&gt;
&lt;li&gt;pad&lt;em&gt;sequences(Xq, maxlen=question&lt;/em&gt;maxlen)
Out[32]: array([], shape=(0, 4), dtype=int32)&lt;/li&gt;
&lt;li&gt;to&lt;em&gt;categorical(Y, num&lt;/em&gt;classes=len(word2indx))
Out[33]: array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;모델-빌드&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EB%AA%A8%EB%8D%B8-%EB%B9%8C%EB%93%9C&quot; aria-label=&quot;모델 빌드 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;모델 빌드&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;train/test data split &lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;Xstrain&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Xqtrain&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Ytrain &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; data_vectorization&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;data_train&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; word2indx&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; story_maxlen&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; question_maxlen&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
Xstest&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Xqtest&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Ytest &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; data_vectorization&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;data_test&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; word2indx&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; story_maxlen&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; question_maxlen&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;Train story&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;Xstrain&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;Train question&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Xqtrain&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;Train answer&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Ytrain&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Test story&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;Xstest&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Test question&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;Xqtest&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Test answer&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;Ytest&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;print&lt;/em&gt; &gt;
Train story (10000, 14) Train question (10000, 4) Train answer (10000, 22)
Test story (1000, 14) Test question (1000, 4) Test answer (1000, 22)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model Parameters 설정&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;EMBEDDING_SIZE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;128&lt;/span&gt;
LATENT_SIZE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;
BATCH_SIZE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;
NUM_EPOCHS &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;40&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Inputs&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;story_input &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;shape&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;story_maxlen&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# story_maxlen = 14&lt;/span&gt;
question_input &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Input&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;shape&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;question_maxlen&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;Story encoder embedding&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;story_encoder &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Embedding&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;input_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;vocab_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# vocab_size: 22&lt;/span&gt;
                        output_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;EMBEDDING_SIZE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# EMBEDDING_SIZE* = 128(한 단어를 128개의 vector로 표시, embedding layer의 colum 담당)&lt;/span&gt;
                        input_length&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;story_maxlen&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;story_input&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# story_maxlen = 14&lt;/span&gt;
story_encoder &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dropout&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;story_encoder&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;Question encoder embedding&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;question_encoder &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Embedding&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;input_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;vocab_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                           output_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;EMBEDDING_SIZE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                           input_length&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;question_maxlen&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;question_input&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
question_encoder &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dropout&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;question_encoder&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h5 id=&quot;code-classlanguage-textattention-score-layercode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textattention-score-layercode&quot; aria-label=&quot;code classlanguage textattention score layercode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;attention score layer&lt;/code&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;attention score layer&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;match &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dot&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;axes&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;story_encoder&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; question_encoder&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Match between story and question: story and question를 dot 연산 수행.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;여기서 dot 연산은 attention score로 사용함 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;story&lt;em&gt;encoder = [None, 14, 128], question&lt;/em&gt;encoder = [None, 4, 128]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;match = [None, 14, 4]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;axes=[2, 2]?  story D2(=128 = embedding vector)와 question D2(=128 = embedding vector)를 dot 해라&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;즉, (x, 128)과 (128,y)로 한쪽을 transpose 시켜서 연산 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;우선 story input의 embedding layer의 출력은 story_encoder = (None, 한 story에 사용된 최대 단어 개수(=14), embedding vector(128)) 이다. &lt;/li&gt;
&lt;li&gt;question input의 embedding layer의 출력은 question_encoder = (None, 한 question에 사용된 최대 단어 개수(=14), embedding vecotr(128)) 이다. &lt;/li&gt;
&lt;li&gt;dot -&gt; (None)을 빼고 (row, colum)끼리(=14, 128)과 (128, 14)가 연산 수행 &lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h5 id=&quot;code-classlanguage-textstory-layercode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textstory-layercode&quot; aria-label=&quot;code classlanguage textstory layercode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;story layer&lt;/code&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;story layer&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;story_encoder_c &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Embedding&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;input_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;vocab_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# vocab_size = 22&lt;/span&gt;
                          output_dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;question_maxlen&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# question_maxlen = 4 &lt;/span&gt;
                          input_length&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;story_maxlen&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;story_input&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# story_maxlen = 14 &lt;/span&gt;

story_encoder_c &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dropout&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;story_encoder_c&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# story_encoder_c.shap=(14, 4)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;이 layer는 story layer의 input에서 시작하여 question layer는 건너뛰고 또 다른 embedding layer를 거쳐, 추후에 dot layer와 add를 하게 됨  &lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h5 id=&quot;code-classlanguage-textepisodic-memory-layercode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textepisodic-memory-layercode&quot; aria-label=&quot;code classlanguage textepisodic memory layercode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;episodic memory layer&lt;/code&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;episodic memory layer&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;response &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Add&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;match&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; story_encoder_c&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# dot한 layer와 바로 위의 story_encoder_c를 add함 =&gt; (14, 4)&lt;/span&gt;
response &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Permute&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;response&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 결론 shape = (4, 14) # Permute((2, 1)): (D2, D1)으로 transpose. permute가 transpose보다 더 축이동이 자유로움 &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h5 id=&quot;code-classlanguage-textanswer-layercode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textanswer-layercode&quot; aria-label=&quot;code classlanguage textanswer layercode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;answer layer&lt;/code&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;episodic memory layer(response) + quetion layer&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;answer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Concatenate&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;response&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; question_encoder&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
answer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; LSTM&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;LATENT_SIZE&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;answer&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# LATENT_SIZE = 64&lt;/span&gt;
answer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dropout&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;answer&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
answer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Dense&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;vocab_size&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;answer&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# shape=(None, 22) # 마지막 dense는 vocab_size=22(단어들의 총 개수)로!&lt;/span&gt;
output &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Activation&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;softmax&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;answer&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# shape=(None, 22)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h5 id=&quot;compile&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#compile&quot; aria-label=&quot;compile permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;compile&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;모델 빌드 마지막&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; Model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;inputs&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;story_input&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; question_input&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; outputs&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;output&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 합쳤으니 input을 []로 써주는 것 &lt;/span&gt;
model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;optimizer&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;adam&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; loss&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;categorical_crossentropy&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 처음에 to_categorial 안 해줬으면 loss=&quot;sparse_categorical_crossentropy&quot; 해야함 &lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h5 id=&quot;fit&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#fit&quot; aria-label=&quot;fit permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;fit&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;모델 학습&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# Model Training&lt;/span&gt;
history &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;Xstrain&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Xqtrain&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;Ytrain&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# Ytrain: answer&lt;/span&gt;
                  batch_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; BATCH_SIZE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; 
                  epochs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; NUM_EPOCHS&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                  validation_data&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;Xstest&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Xqtest&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;Ytest&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# ytest??? fit 하면 0,0,0 이던 게 13,14 등지로 바뀜&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Ytest.shape
Out[78]: (1000, 22)&lt;/li&gt;
&lt;li&gt;Ytest
Out[79]:
array([[0., 0., 0., ..., 0., 0., 0.],
[0., 0., 0., ..., 0., 0., 0.],
[0., 0., 0., ..., 0., 0., 0.],
...,
[0., 0., 0., ..., 0., 0., 0.],
[0., 0., 0., ..., 0., 0., 0.],
[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&lt;/li&gt;
&lt;li&gt;ytest.shape
Out[87]: (1000,)&lt;/li&gt;
&lt;li&gt;ytest
Out[92]:
array([15, 12, 16, 15, 16, 15, 14 ... ])&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;loss-plot&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#loss-plot&quot; aria-label=&quot;loss plot permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;loss plot&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;loss plot&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;title&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;Episodic Memory Q &amp;amp; A Loss&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;plot&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;history&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;history&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;loss&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;g&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; label&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;train&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;plot&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;history&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;history&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;val_loss&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; label&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;validation&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;legend&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;loc&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;best&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
plt&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;show&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;정확도-측정&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EC%A0%95%ED%99%95%EB%8F%84-%EC%B8%A1%EC%A0%95&quot; aria-label=&quot;정확도 측정 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;정확도 측정&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;get predictions of labels&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;ytest &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;argmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Ytest&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; axis&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
Ytest_ &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;Xstest&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; Xqtest&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
ytest_ &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;argmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Ytest_&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; axis&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&quot;적용&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EC%A0%81%EC%9A%A9&quot; aria-label=&quot;적용 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;적용&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;적용&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Select Random questions and predict answers&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;NUM_DISPLAY &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;
 
&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;sample&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Xstest&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;shape&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;NUM_DISPLAY&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
  story &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;join&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;indx2word&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; x &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; Xstest&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;i&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;tolist&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; x &lt;span class=&quot;token operator&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  question &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;join&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;indx2word&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; x &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; Xqtest&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;i&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;tolist&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  label &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; indx2word&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;ytest&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;i&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
  prediction &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; indx2word&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;ytest_&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;i&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;story&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; question&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; label&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; prediction&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;출력층&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EC%B6%9C%EB%A0%A5%EC%B8%B5&quot; aria-label=&quot;출력층 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;출력층&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;출력층이 0 or 1 처럼 하나일 때&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Binary classification. 따라서 sigmoid - binary-crossentropy 사용&lt;/li&gt;
&lt;li&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;yHat&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;정확도: 2/3&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;출력층이 두 개 이상 나올 때&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multi-classification. 따라서 softmax - categorical-crossentropy 사용&lt;/li&gt;
&lt;li&gt;one-hot 구조&lt;/li&gt;
&lt;li&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;yHat&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0 1 0&lt;/td&gt;
&lt;td&gt;0 1 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0 0 1&lt;/td&gt;
&lt;td&gt;0 1 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1 0 0&lt;/td&gt;
&lt;td&gt;1 0 0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;정확도: 2/3&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;출력층에 &apos;1&apos;이 여러 개인 구조. one-hot 구조가 아닐 때&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multi-labeled classification. 따라서 sigmoid - binary-crossentropy 사용&lt;/li&gt;
&lt;li&gt;입력 뉴런 각각에 대해 binary-classification 해야 함&lt;/li&gt;
&lt;li&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;yHat&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0 1 0&lt;/td&gt;
&lt;td&gt;0 1 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0 0 1&lt;/td&gt;
&lt;td&gt;0 1 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1 0 0&lt;/td&gt;
&lt;td&gt;1 0 0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;정확도: 9개 중 7개 맞춤.  따라서 7/9&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위에처럼 row 전체가 다 맞았을 때 맞았다고 보는 게 아니고, row+colum으로 각각 개별로 봄 &lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;참고: &lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;아마추어 퀀트, blog.naver.com/chunjein&lt;/li&gt;
&lt;li&gt;크리슈나 바브사 외. 2019.01.31. 자연어 처리 쿡북 with 파이썬 [파이썬으로 NLP를 구현하는 60여 가지 레시피]. 에이콘&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://frhyme.github.io/python-libs/ML_multilabel_classfication/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://frhyme.github.io/python-libs/ML_multilabel_classfication/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[NLP 카운트 기반 방법의 텍스트 유사도 측정]]></title><description><![CDATA[one-hot 인코딩 categorical 변환 방법  Keras를 이용한 one-hot encoding '남자' '여자' '아빠' '엄마' '삼촌' '이모'
'남자', '삼촌', '아빠', '엄마', '여자', '이모' [1. 0. 0. 0.…]]></description><link>https://jynee.github.io/tags#1st/NLP한글_1/</link><guid isPermaLink="false">https://jynee.github.io/tags#1st/NLP한글_1/</guid><pubDate>Wed, 05 Aug 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;one-hot-인코딩&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#one-hot-%EC%9D%B8%EC%BD%94%EB%94%A9&quot; aria-label=&quot;one hot 인코딩 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;one-hot 인코딩&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;categorical 변환 방법&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-textkerascode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textkerascode&quot; aria-label=&quot;code classlanguage textkerascode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;keras&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Keras를 이용한 one-hot encoding&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;data &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;남자&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;여자&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;아빠&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;엄마&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;삼촌&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;이모&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
values &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;array&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;values&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;values&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[&apos;남자&apos; &apos;여자&apos; &apos;아빠&apos; &apos;엄마&apos; &apos;삼촌&apos; &apos;이모&apos;]
[&apos;남자&apos;, &apos;삼촌&apos;, &apos;아빠&apos;, &apos;엄마&apos;, &apos;여자&apos;, &apos;이모&apos;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tensorflow&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;keras&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;utils &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; to_categorical
encoded &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; to_categorical&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;integer_encoded&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;encoded&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[1. 0. 0. 0. 0. 0.]
[0. 0. 0. 0. 1. 0.]
[0. 0. 1. 0. 0. 0.]
[0. 0. 0. 1. 0. 0.]
[0. 1. 0. 0. 0. 0.]
[0. 0. 0. 0. 0. 1.]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-textsklearncode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textsklearncode&quot; aria-label=&quot;code classlanguage textsklearncode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;sklearn&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;sklearn의 preprocessing을 이용한 one-hot encoding 방법&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;data &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;남자&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;여자&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;아빠&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;엄마&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;삼촌&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;이모&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
values &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;array&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;values&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;values&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[&apos;남자&apos; &apos;여자&apos; &apos;아빠&apos; &apos;엄마&apos; &apos;삼촌&apos; &apos;이모&apos;]
[&apos;남자&apos;, &apos;삼촌&apos;, &apos;아빠&apos;, &apos;엄마&apos;, &apos;여자&apos;, &apos;이모&apos;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;label 인코딩 필요&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; sklearn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;preprocessing &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; sk

label_encoder &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; sk&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;LabelEncoder&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
integer_encoded &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; label_encoder&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit_transform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;values&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;integer_encoded&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[0]
[4]
[2]
[3]
[1]
[5]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;OneHotEncoding&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# integer encoding&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;integer_encoded&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# binary encoding&lt;/span&gt;
integer_encoded &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; integer_encoded&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;reshape&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;integer_encoded&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
onehot_encoder &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; sk&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;OneHotEncoder&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sparse&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; categories&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;auto&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
onehot_encoded &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; onehot_encoder&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit_transform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;integer_encoded&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;onehot_encoded&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[1. 0. 0. 0. 0. 0.]
[0. 0. 0. 0. 1. 0.]
[0. 0. 1. 0. 0. 0.]
[0. 0. 0. 1. 0. 0.]
[0. 1. 0. 0. 0. 0.]
[0. 0. 0. 0. 0. 1.]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;단점: OOV 문제&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;해결을 위한 노력: 페이스북의 FastText&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h1 id=&quot;code-classlanguage-texthashcode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-texthashcode&quot; aria-label=&quot;code classlanguage texthashcode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;Hash&lt;/code&gt;&lt;/h1&gt;
&lt;br&gt;
&lt;h2 id=&quot;indexing&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#indexing&quot; aria-label=&quot;indexing permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Indexing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;예) 견출지&lt;/li&gt;
&lt;li&gt;SQL의 내부 알고리즘&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;단점: 견출지 붙일 때, 몇 장마다 붙일지 미리 정해야 됨&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Key의 범위가 넓다면 Memory 비효율성&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;장점: 나중에 찾을 때, 빠르다&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;검색 속도는 빠르다&lt;/p&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;hashing&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#hashing&quot; aria-label=&quot;hashing permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hashing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;예) apple이란 단어를 수치변환하여 84 page의 36번째 line에 기록한다&lt;/li&gt;
&lt;li&gt;다른 단어를 입력할 때, apple에 적용했던 방법을 답습하여 손쉽게 기록할 수 있다&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;단점: collision 발생&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;apple과 tiger 단어가 우연히 같은 line에 기록될 수 있다. &lt;/li&gt;
&lt;li&gt;단점 해결: overflow 처리&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;장점: 메모리 효율성&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;특히 넓은 key 영역에서 효율적이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&quot;hashing을-단어-표현에-적용&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#hashing%EC%9D%84-%EB%8B%A8%EC%96%B4-%ED%91%9C%ED%98%84%EC%97%90-%EC%A0%81%EC%9A%A9&quot; aria-label=&quot;hashing을 단어 표현에 적용 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hashing을 단어 표현에 적용&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Hashing trick을 위한 word emgedding 방법&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;vocabulary 크기를 미리 지정(hash table)하고, 단어들을 hash table에 대응 시키는 방식&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hash(&apos;apple&apos;) or md5(&apos;apple&apos;) % 8 = 1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&apos;% 8&apos; : 모듈러 연산을 취해준다&lt;/li&gt;
&lt;li&gt;&apos;1&apos; : vetor화 시킨 것 =&gt; 1번째 line에 있다&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;현업에선 &quot;collision 발생 확률을 몇 프로로 줄이기 위해 hash table의 크기를 몇으로 둘 것인가?&quot; 에 관해 논의 후 설계하기도 함&lt;/li&gt;
&lt;li&gt;code: 3-2. hashing_trick.py&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h1 id=&quot;카운트-기반-방법code-classlanguage-textco-occurrence-matrixcode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EC%B9%B4%EC%9A%B4%ED%8A%B8-%EA%B8%B0%EB%B0%98-%EB%B0%A9%EB%B2%95code-classlanguage-textco-occurrence-matrixcode&quot; aria-label=&quot;카운트 기반 방법code classlanguage textco occurrence matrixcode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;카운트 기반 방법(&lt;code class=&quot;language-text&quot;&gt;Co-occurrence matrix&lt;/code&gt;)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Co-occurrence matrix: 동시 발생(출현) 행렬&lt;/li&gt;
&lt;li&gt;같이 쓰인 횟수(인접 사용한 횟수)를 matrix에 기록하는 것 &lt;/li&gt;
&lt;li&gt;학습 기반이 아닌 &lt;strong&gt;빈도 기반&lt;/strong&gt;의 단어들 간의 관계 정보(단어 간 유사도)를 내포하고 있다.&lt;/li&gt;
&lt;li&gt;모든 단어를 vocabulary라고 할 때, one-hot의 size(개수) = vocabulary의 size(개수) = 단어 벡터&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;one-hot 인코딩이 아니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;예) 0 0 0  2 1 0 0 0 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;대칭 행렬, 희소 행렬(&apos;0&apos;이 많음)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;긴 단어일수록 차원이 크다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SVD(특이값 분해)를 사용해 단어 벡터의 차원을 줄일 수 있다.&lt;/li&gt;
&lt;li&gt;약 25% 정도 줄일 수 있다.&lt;/li&gt;
&lt;li&gt;vocab 사이즈를 줄이고 embedding layer에 쓴 것처럼.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Documnet Term Freq 2개) Xt와 X의 곱행렬을 하면, 일일이 빈도를 구하지 않아도 문장 간 공통된 단어가 쓰인 횟수를 행렬로 만들 수 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;그렇게 만들어진 행렬이 Co-occurrence matrix&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unigram(1단어), Bigram(2단어)까지 vocab을 만들 수 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;조합이 더 많아진다.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Glove: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;아래 2개의 단점을 고려해 장점을 합친 것 &lt;/li&gt;
&lt;li&gt;Co-occurrence matrix: (빈도 기반) 전체 단어를 고려했다.&lt;/li&gt;
&lt;li&gt;Skip-gram: (학습 기반)주변 단어에 한정했다.&lt;/li&gt;
&lt;li&gt;두 단어를 Embedding layer에 통과시키고(학습기반) 각 vector의 내적의 합(빈도기반)을 구해 거리를 구한다.&lt;/li&gt;
&lt;li&gt;log(P(도서관, 갔다)) = 0.33 &lt;/li&gt;
&lt;li&gt;기계 : 도서관은... 가는 곳이다.... 라고 기계가 인식함 &lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;단점: 계산량이 많다&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-text특이값-분해svd-차원축소code-code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-text%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4svd-%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8Ccode-code&quot; aria-label=&quot;code classlanguage text특이값 분해svd 차원축소code code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;특이값 분해(SVD: 차원축소)&lt;/code&gt; code&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LSA : 잠재 의미 분석 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;U, S, VT 행렬의 의미 --&gt; Latent Semantic Analysis (LSA)&lt;/li&gt;
&lt;li&gt;U 행렬 ~ 차원 = (문서 개수 X topic 개수) : 문서당 topic 분포&lt;/li&gt;
&lt;li&gt;S 행렬 ~ 차원 = (topic 개수 X topic 개수) : 대각성분. 나중에 행렬에 넣을 땐 대각성분만 빼면 0&lt;/li&gt;
&lt;li&gt;VT 행렬. 차원 = (topic 개수 X 단어 개수) : topic 당 단어 빈도의 분포&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;=&gt; 이를 여기선 &lt;strong&gt;문장과 단어 사이의 관계로 해석&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/b9872c633b3e4a7f22f8c0e9e281b945/78958/image-20200722094819751.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 52.70270270270271%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAABl0lEQVQoz2WT6XLCMAyE/f5v2B8tpSFNgNzORRJQ93Njhk6Z0TiWVseujLtcLna9Xq0oCquqyuq6DmdZluGbs21bezweFn+v39M0Wdd11ve9zfNsDscwDP+A/O73+/PEiEfMtm22LMsfLDF3u90Mo/o4jgFEk+jHon9d1zA9rLDz+RzYcXIH47gAgjIOkqEIXSb33gdKNKQ4sSzLLEkSS9PUTqeTHY9Hy/M8TPmckGI4oMIk0cc9+jCmb5rGovZhQhVDa3COKVoBvO9CsFKAhFmJJDMp0zMZ/kHiv2pKk3mXLBRkklXB27KFouMoass+jUA0pDDJJLyubdU96K8mozCLctxZGlTHL+ta/7vpdbMqkU86+V1ouseNbmgqfyPt889Pq4WbxADfJDauVKBRwIvOvv8AmHQnaVYhlsKy7lAU9fzjw7Ld0vd3yw4HS97erJdkbpJmo0Bss+Axy3ptvd/PQV294hQMDGS9sFeejhqyEE42jzyu5onIWEj6/W0XWSkZCkxPI5gSaFbuxpI44z+q3J8di/sBVyJODi8AubUAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200722094819751&quot;
        title=&quot;image-20200722094819751&quot;
        src=&quot;/static/b9872c633b3e4a7f22f8c0e9e281b945/fcda8/image-20200722094819751.png&quot;
        srcset=&quot;/static/b9872c633b3e4a7f22f8c0e9e281b945/12f09/image-20200722094819751.png 148w,
/static/b9872c633b3e4a7f22f8c0e9e281b945/e4a3f/image-20200722094819751.png 295w,
/static/b9872c633b3e4a7f22f8c0e9e281b945/fcda8/image-20200722094819751.png 590w,
/static/b9872c633b3e4a7f22f8c0e9e281b945/efc66/image-20200722094819751.png 885w,
/static/b9872c633b3e4a7f22f8c0e9e281b945/c83ae/image-20200722094819751.png 1180w,
/static/b9872c633b3e4a7f22f8c0e9e281b945/78958/image-20200722094819751.png 1320w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;py&quot;&gt;&lt;pre class=&quot;language-py&quot;&gt;&lt;code class=&quot;language-py&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; sklearn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;feature_extraction&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;text &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; CountVectorizer&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;docs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;성진과 창욱은 야구장에 갔다&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;token string&quot;&gt;&apos;성진과 태균은 도서관에 갔다&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;token string&quot;&gt;&apos;성진과 창욱은 공부를 좋아한다&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;Vocab 만들기 &lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;count_model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; CountVectorizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ngram_range&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# gram_range=(1,1): Unigram 단어 1개씩 동시 발생. # CountVectorizer: 문장의 단어를 단어 하나씩 자른단 뜻 &lt;/span&gt;
x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; count_model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit_transform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;docs&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;문서에 사용된 사전을 조회한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;count_model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocabulary_&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;{&apos;성진과&apos;: 3, &apos;창욱은&apos;: 6, &apos;야구장에&apos;: 4, &apos;갔다&apos;: 0, &apos;태균은&apos;: 7, &apos;도서관에&apos;: 2, &apos;공부를&apos;: 1, &apos;좋아한다&apos;: 5}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;Compact Sparse Row(CSR) format: 단어별 빈도를 표현한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# (row, col) value&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;  (0, 3)	1
(0, 6)	1
(0, 4)	1
(0, 0)	1
(1, 3)	1
(1, 0)	1
(1, 7)	1
(1, 2)	1
(2, 3)	1
(2, 6)	1
(2, 1)	1
(2, 5)	1&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;행렬 형태로 표시한다. (Document-Term Freq)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;toarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;T&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;toarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;print(x.toarray())&lt;/em&gt; &gt;&lt;/p&gt;
&lt;p&gt;[[1 0 0 1 1 0 1 0]
[1 0 1 1 0 0 0 1]
[0 1 0 1 0 1 1 0]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;print(x.T.toarray())&lt;/em&gt; &gt;&lt;/p&gt;
&lt;p&gt;[[1 1 0]
[0 0 1]
[0 1 0]
[1 1 1]
[1 0 0]
[0 0 1]
[1 0 1]
[0 1 0]]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;x.T의 의미 &gt;
1 2 3  - 문장
갔다    [[1 1 0] - &apos;갔다&apos;라는 단어는 문장-1과 문장-2에 쓰였음.
공부를   [0 0 1] - &apos;공부를&apos;은 문장-3에만 쓰였음.
도서관에 [0 1 0]
성진과   [1 1 1]
야구장에 [1 0 0]
좋아한다 [0 0 1]
창욱은   [1 0 1]
태균은   [0 1 0]]&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;xc &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;T &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; x &lt;span class=&quot;token comment&quot;&gt;# this is co-occurrence matrix in sparse csr format&lt;/span&gt;
xc&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;setdiag&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# sometimes you want to fill same word cooccurence to 0&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;xc&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;toarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;0&lt;br /&gt;갔다&lt;/th&gt;
&lt;th&gt;1&lt;br /&gt;공부를&lt;/th&gt;
&lt;th&gt;2&lt;br /&gt;도서관에&lt;/th&gt;
&lt;th&gt;3&lt;br /&gt;성진과&lt;/th&gt;
&lt;th&gt;4&lt;br /&gt;야구장에&lt;/th&gt;
&lt;th&gt;5&lt;br /&gt;좋아한다&lt;/th&gt;
&lt;th&gt;6&lt;br /&gt;창욱은&lt;/th&gt;
&lt;th&gt;7&lt;br /&gt;태균은&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0 갔다&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1 공부를&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2 도서관에&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3 성진과&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4 야구장에&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5 좋아한다&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6 창욱은&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7 태균은&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;참고&lt;/em&gt; &gt; ngram&lt;em&gt;range(min&lt;/em&gt;n = 1, max_n = 2)인 경우&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;즉,  ngram_range=(1,2):unigram과 bigram 둘다 동시에 조회하는 경우&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;count_model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; CountVectorizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ngram_range&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; count_model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit_transform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;docs&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# 문서에 사용된 사전을 조회&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;count_model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocabulary_&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  
xc &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;T &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; x &lt;span class=&quot;token comment&quot;&gt;# this is co-occurrence matrix in sparse csr format&lt;/span&gt;
xc&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;setdiag&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# sometimes you want to fill same word cooccurence to 0&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;xc&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;toarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;{&apos;성진과&apos;: 5, &apos;창욱은&apos;: 11, &apos;야구장에&apos;: 8, &apos;갔다&apos;: 0, &apos;성진과 창욱은&apos;: 6, &apos;창욱은 야구장에&apos;: 13, &apos;야구장에 갔다&apos;: 9, &apos;태균은&apos;: 14, &apos;도서관에&apos;: 3, &apos;성진과 태균은&apos;: 7, &apos;태균은 도서관에&apos;: 15, &apos;도서관에 갔다&apos;: 4, &apos;공부를&apos;: 1, &apos;좋아한다&apos;: 10, &apos;창욱은 공부를&apos;: 12, &apos;공부를 좋아한다&apos;: 2}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[[0 0 1 2 1 0 1 1]
[0 0 0 1 0 1 1 0]
[1 0 0 1 0 0 0 1]
[2 1 1 0 1 1 2 1]
[1 0 0 1 0 0 1 0]
[0 1 0 1 0 0 1 0]
[1 1 0 2 1 1 0 0]
[1 0 1 1 0 0 0 0]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&quot;변수-정리--x--xt--xc&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EB%B3%80%EC%88%98-%EC%A0%95%EB%A6%AC--x--xt--xc&quot; aria-label=&quot;변수 정리  x  xt  xc permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;변수 정리&lt;/em&gt; &gt; x / x.T / xc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;x: 단어별 빈도&lt;/li&gt;
&lt;li&gt;x.T: Sparse 해주려고 x를 transpose함&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;xc : x와 x.T의 곱행렬이자, 처음에 쓸 땐 co-occurrence matrix 만들기 위한 csr 형태. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;여기에 &lt;/li&gt;
&lt;li&gt;xc.setdiag(0) 해주고, &lt;/li&gt;
&lt;li&gt;xc.toarray() 해주면&lt;/li&gt;
&lt;li&gt;co-occurrence matrix 완성&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;numpy를-이용한-svd-예시&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#numpy%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-svd-%EC%98%88%EC%8B%9C&quot; aria-label=&quot;numpy를 이용한 svd 예시 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;numpy를 이용한 SVD 예시&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Co-occurrence matrix를 SVD로 분해한다.&lt;/li&gt;
&lt;li&gt;C = &lt;strong&gt;U&lt;/strong&gt; , &lt;strong&gt;S&lt;/strong&gt;, &lt;strong&gt;VT&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; np
C &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; xc&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;toarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
U&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; S&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; VT &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;linalg&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;svd&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;C&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; full_matrices &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# np.linalg.svd 써주면 U, S, VT로 자동 분배됨&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;U&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;\n&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;\n&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;VT&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;\n&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[-0.44 -0.39 -0.58  0.41  0.35  0.   -0.   -0.19]
[-0.24 -0.12  0.29  0.41 -0.24  0.65 -0.29  0.35]
[-0.24 -0.12 -0.29 -0.41 -0.24 -0.29 -0.65  0.35]
[-0.56  0.8   0.   -0.    0.19  0.    0.    0.02]
[-0.27 -0.01 -0.   -0.   -0.7   0.   -0.   -0.66]
[-0.24 -0.12  0.29  0.41 -0.24 -0.65  0.29  0.35]
[-0.44 -0.39  0.58 -0.41  0.35 -0.    0.   -0.19]
[-0.24 -0.12 -0.29 -0.41 -0.24  0.29  0.65  0.35]] &lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[5.27 2.52 1.73 1.73 1.27 1.   1.   0.53] &lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[[-0.44 -0.24 -0.24 -0.56 -0.27 -0.24 -0.44 -0.24]
[ 0.39  0.12  0.12 -0.8   0.01  0.12  0.39  0.12]
[-0.    0.5  -0.5  -0.    0.    0.5  -0.   -0.5 ]
[-0.71  0.   -0.    0.    0.    0.    0.71 -0.  ]
[-0.35  0.24  0.24 -0.19  0.7   0.24 -0.35  0.24]
[-0.   -0.65  0.29 -0.    0.    0.65  0.   -0.29]
[-0.    0.29  0.65 -0.    0.   -0.29 -0.   -0.65]
[-0.19  0.35  0.35  0.02 -0.66  0.35 -0.19  0.35]] &lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;S를 &lt;strong&gt;정방행렬&lt;/strong&gt;로 바꾼다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S&lt;/li&gt;
&lt;li&gt;정방행렬이므로, 같은 수의 행과 열을 가지는 행렬&lt;/li&gt;
&lt;li&gt;대각행렬이므로, 대각 성분을 제외한 원소는 모두 0 인 행렬&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;s &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;diag&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# s는 대각행렬이자 정방행렬&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;s&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[5.27 0.   0.   0.   0.   0.   0.   0.  ]
[0.   2.52 0.   0.   0.   0.   0.   0.  ]
[0.   0.   1.73 0.   0.   0.   0.   0.  ]
[0.   0.   0.   1.73 0.   0.   0.   0.  ]
[0.   0.   0.   0.   1.27 0.   0.   0.  ]
[0.   0.   0.   0.   0.   1.   0.   0.  ]
[0.   0.   0.   0.   0.   0.   1.   0.  ]
[0.   0.   0.   0.   0.   0.   0.   0.53]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;A = &lt;strong&gt;U.s.VT&lt;/strong&gt;를 계산하고, &lt;strong&gt;A와 C가 일치&lt;/strong&gt;하는지 확인한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;A &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;dot&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;U&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;dot&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;s&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; VT&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;A&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;C&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[ 0.  0.  1.  2.  1.  0.  1.  1.]
[-0.  0.  0.  1.  0.  1.  1.  0.]
[ 1. -0.  0.  1.  0. -0.  0.  1.]
[ 2.  1.  1.  0.  1.  1.  2.  1.]
[ 1.  0. -0.  1.  0.  0.  1. -0.]
[ 0.  1.  0.  1.  0. -0.  1.  0.]
[ 1.  1.  0.  2.  1.  1.  0. -0.]
[ 1. -0.  1.  1. -0. -0. -0.  0.]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[[0 0 1 2 1 0 1 1]
[0 0 0 1 0 1 1 0]
[1 0 0 1 0 0 0 1]
[2 1 1 0 1 1 2 1]
[1 0 0 1 0 0 1 0]
[0 1 0 1 0 0 1 0]
[1 1 0 2 1 1 0 0]
[1 0 1 1 0 0 0 0]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&quot;sklearn을-이용한-svd-예시&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#sklearn%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-svd-%EC%98%88%EC%8B%9C&quot; aria-label=&quot;sklearn을 이용한 svd 예시 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;sklearn을 이용한 SVD 예시&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Co-occurrence matrix를 SVD로 분해한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; sklearn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;decomposition &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; TruncatedSVD&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;특이값 (S)이 큰 4개를 주 성분으로 C의 차원을 축소한다. &lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;svd &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; TruncatedSVD&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;n_components&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; n_iter&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
D &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; svd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit_transform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;xc&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;toarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# fit_transform : 학습 시키고 transpose도 한꺼번에 시킴 &lt;/span&gt;

U &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; D &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; svd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;singular_values_ &lt;span class=&quot;token comment&quot;&gt;# svd.singular_values_ : 대각 성분의 값 &lt;/span&gt;
S &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;diag&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;svd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;singular_values_&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# np.diag: &apos;대각행렬&apos;로, 대각 성분의 값만을 행렬 형태로 추출한 것. 정방행렬의 형태를 띄고 있음 &lt;/span&gt;
VT &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; svd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;components_&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;print(np.round(U, 2), &apos;\n&apos;)&lt;/em&gt; &gt;&lt;/p&gt;
&lt;p&gt;[[ 0.44 -0.39  0.41 -0.58]
[ 0.24 -0.12  0.41  0.29]
[ 0.24 -0.12 -0.41 -0.29]
[ 0.56  0.8  -0.    0.  ]
[ 0.27 -0.01 -0.   -0.  ]
[ 0.24 -0.12  0.41  0.29]
[ 0.44 -0.39 -0.41  0.58]
[ 0.24 -0.12 -0.41 -0.29]] &lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;print(np.round(S, 2), &apos;\n&apos;)&lt;/em&gt; &gt;&lt;/p&gt;
&lt;p&gt;[[5.27 0.   0.   0.  ]
[0.   2.52 0.   0.  ]
[0.   0.   1.73 0.  ]
[0.   0.   0.   1.73]] &lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;print(np.round(VT, 2), &apos;\n&apos;)&lt;/em&gt; &gt;&lt;/p&gt;
&lt;p&gt;[[ 0.44  0.24  0.24  0.56  0.27  0.24  0.44  0.24]
[ 0.39  0.12  0.12 -0.8   0.01  0.12  0.39  0.12]
[-0.71  0.   -0.    0.    0.    0.    0.71 -0.  ]
[-0.    0.5  -0.5  -0.    0.    0.5  -0.   -0.5 ]] &lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;C를 4개 차원으로 축소: truncated (U * S)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;U * S * VT 하면 원래 C의 차원과 동일해 진다. &lt;/li&gt;
&lt;li&gt;U * S가 축소된 차원을 의미하고, &lt;/li&gt;
&lt;li&gt;V는 &lt;strong&gt;축소된 차원을 원래 차원으로 되돌리는&lt;/strong&gt; 역할을 한다 (mapping back)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;D&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[ 2.31 -0.97  0.71 -1.  ]
[ 1.24 -0.3   0.71  0.5 ]
[ 1.24 -0.3  -0.71 -0.5 ]
[ 2.97  2.03 -0.    0.  ]
[ 1.44 -0.03 -0.   -0.  ]
[ 1.24 -0.3   0.71  0.5 ]
[ 2.31 -0.97 -0.71  1.  ]
[ 1.24 -0.3  -0.71 -0.5 ]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&quot;변수-정리--c--d--vt&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%EB%B3%80%EC%88%98-%EC%A0%95%EB%A6%AC--c--d--vt&quot; aria-label=&quot;변수 정리  c  d  vt permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;변수 정리&lt;/em&gt; &gt; C / D / Vt&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;원래 행렬: C&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;차원축소: D = U*S&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이때, U, S는 중요한 부분만 추림 U(truncated), S(truncated))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Vt = S를 기준으로 Vt를 truncated함 &lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&quot;code-classlanguage-textsvdcode--numpy--sklearn-비교&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textsvdcode--numpy--sklearn-%EB%B9%84%EA%B5%90&quot; aria-label=&quot;code classlanguage textsvdcode  numpy  sklearn 비교 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;SVD&lt;/code&gt; : Numpy &amp;#x26; sklearn 비교&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Co-occurrence matrix를 SVD로 분해&lt;/li&gt;
&lt;li&gt;여기서 말하는 &apos;차원 축소&apos;: 한 문장 내 다른 문장과 쓰이는 중요 단어만 추출함&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Numpy&lt;/th&gt;
&lt;th&gt;sklearn&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Code&lt;/td&gt;
&lt;td&gt;import numpy as np&lt;/td&gt;
&lt;td&gt;from sklearn.decomposition import TruncatedSVD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&gt; &lt;strong&gt;C = U.S.VT&lt;/strong&gt;&lt;br /&gt;C = xc.toarray()&lt;br/&gt;U, S, VT = np.linalg.svd(C, full_matrices = True)&lt;/td&gt;
&lt;td&gt;&gt; &lt;strong&gt;특이값 (S)이 큰 4개를 주 성분으로 C의 차원을 축소&lt;/strong&gt;&lt;br /&gt;svd = TruncatedSVD(n&lt;em&gt;components=4, n&lt;/em&gt;iter=7)&lt;br/&gt;D = svd.fit&lt;em&gt;transform(xc.toarray())&lt;br /&gt;U = D / svd.singular&lt;/em&gt;values&lt;em&gt;&lt;br/&gt;S = np.diag(svd.singular&lt;/em&gt;values&lt;em&gt;)&lt;br/&gt;VT = svd.components&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&gt; &lt;strong&gt;S를 정방행렬로 바꾼다.&lt;/strong&gt;&lt;br/&gt;s = np.diag(S)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&gt; &lt;strong&gt;A = U.s.VT를 계산하고, A와 C가 일치하는지 확인&lt;/strong&gt;&lt;br/&gt;A = np.dot(U, np.dot(s, VT))&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;특징&lt;/td&gt;
&lt;td&gt;알아서 주성분을 추출해 차원 축소할 수 있다.&lt;/td&gt;
&lt;td&gt;원하는 수의 주성분으로 차원을 축소할 수 있다.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;U * S * VT 하면 원래 C의 차원과 동일해 진다. &lt;/li&gt;
&lt;li&gt;U * S가 축소된 차원을 의미하고, &lt;/li&gt;
&lt;li&gt;V는 &lt;strong&gt;축소된 차원을 원래 차원으로 되돌리는&lt;/strong&gt; 역할(mapping back)을 한다&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h1 id=&quot;텍스트-유사도거리-측정&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%9C%A0%EC%82%AC%EB%8F%84%EA%B1%B0%EB%A6%AC-%EC%B8%A1%EC%A0%95&quot; aria-label=&quot;텍스트 유사도거리 측정 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;텍스트 유사도(거리 측정)&lt;/h1&gt;
&lt;br&gt;
&lt;h2 id=&quot;step-1-word의-vector화&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#step-1-word%EC%9D%98-vector%ED%99%94&quot; aria-label=&quot;step 1 word의 vector화 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 1. word의 vector화&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;TfidfVectorizer&lt;/code&gt; 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; sklearn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;feature_extraction&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;text &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; TfidfVectorizer

sent &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; 
        &lt;span class=&quot;token string&quot;&gt;&quot;폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니다.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 

tfidf_vectorizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; TfidfVectorizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
tfidf_matrix &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; tfidf_vectorizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit_transform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sent&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;toarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;tfidf_matrix&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[0.    0.324 0.    0.    0.324 0.324 0.324 0.324 0.324 0.324 0.    0.231
0.324 0.231 0.    0.    0.231]
[0.365 0.    0.365 0.365 0.    0.    0.    0.    0.    0.    0.365 0.259&lt;/p&gt;
&lt;ol start=&quot;0&quot;&gt;
&lt;li&gt;0.259 0.365 0.365 0.259]]&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;HashingVectorizer&lt;/code&gt; 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; sklearn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;feature_extraction&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;text &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; HashingVectorizer

sent &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; 
        &lt;span class=&quot;token string&quot;&gt;&quot;폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니다.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 

VOCAB_SIZE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;20&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# 사용자가 직접 지정해야 하는 값&lt;/span&gt;
hvectorizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; HashingVectorizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;n_features&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;VOCAB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;norm&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;alternate_sign&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
hash_matrix &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; hvectorizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit_transform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sent&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;toarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;hash_matrix&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[0. 2. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 2. 2. 0. 1. 0. 0. 0. 0.]
[0. 2. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0.]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-text자카드-유사도code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-text%EC%9E%90%EC%B9%B4%EB%93%9C-%EC%9C%A0%EC%82%AC%EB%8F%84code&quot; aria-label=&quot;code classlanguage text자카드 유사도code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;자카드 유사도&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;문장 中 중복되는 단어의 개수를 센다.&lt;/li&gt;
&lt;li&gt;두 문장에 겹친 단어 개수 / 사전의 전체 단어 개수&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(수치화(vector)화 안 하고) 문장 간의 겹치는 단어만 세는 것도 가능하다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;sent_1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sent&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;split&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
sent_2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sent&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;split&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sent_1&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sent_2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# 합집합과 교집합을 구한다.&lt;/span&gt;
hap_set &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; sent_1 &lt;span class=&quot;token operator&quot;&gt;|&lt;/span&gt; sent_2 &lt;span class=&quot;token comment&quot;&gt;# | : or&lt;/span&gt;
gyo_set &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; sent_1 &lt;span class=&quot;token operator&quot;&gt;&amp;amp;&lt;/span&gt; sent_2 &lt;span class=&quot;token comment&quot;&gt;# &amp;amp; : and&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;hap_set&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;\n&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gyo_set&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;\n&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

jaccard &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gyo_set&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;hap_set&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;jaccard&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;{&apos;폭염&apos;, &apos;있습니다.&apos;, &apos;비&apos;, &apos;휴일&apos;, &apos;을&apos;} &lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;(수치화(vector)화 하고) jaccard_score 패키지&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;count_model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; CountVectorizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ngram_range&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# bigram은 ngram_range=(1,2) : (from, to) = 1에서 2까지&lt;/span&gt;
x &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; count_model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit_transform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sent&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;toarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; sklearn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;metrics &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; jaccard_score
jaccard_score&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;0.17647058823529413&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-text코사인-유사도code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-text%EC%BD%94%EC%82%AC%EC%9D%B8-%EC%9C%A0%EC%82%AC%EB%8F%84code&quot; aria-label=&quot;code classlanguage text코사인 유사도code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;코사인 유사도&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;코사인 유사도는 클수록 유사도가 높다&lt;/li&gt;
&lt;li&gt;코사인 거리는 작을수록 거리가 좁다&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; sklearn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;metrics&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;pairwise &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; cosine_similarity
d &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; cosine_similarity&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;tfidf_matrix&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; tfidf_matrix&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;d&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[0.17952266]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-text유클리디안-거리code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-text%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%94%94%EC%95%88-%EA%B1%B0%EB%A6%ACcode&quot; aria-label=&quot;code classlanguage text유클리디안 거리code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;유클리디안 거리&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;L2 - Distance&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; sklearn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;metrics&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;pairwise &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; euclidean_distances
euclidean_distances&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;tfidf_matrix&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; tfidf_matrix&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;array([[1.28099753]])&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-text맨하탄-거리code&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-text%EB%A7%A8%ED%95%98%ED%83%84-%EA%B1%B0%EB%A6%ACcode&quot; aria-label=&quot;code classlanguage text맨하탄 거리code permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;맨하탄 거리&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;L1 - Distance&lt;/li&gt;
&lt;li&gt;유클리디안 거리와의 비교: 거리 값은 유클리디안 거리가 작게 나오지만, 대각선의 길을 선택할 수 있다는 건 현실성이 떨어지기 때문에 맨하탄 거리를 선택하는 경우도 있다. &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; sklearn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;metrics&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;pairwise &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; manhattan_distances
d &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; manhattan_distances&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;tfidf_norm_l1&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; tfidf_norm_l1&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;[[0.77865927]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-text정규화codel1--l2&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-text%EC%A0%95%EA%B7%9C%ED%99%94codel1--l2&quot; aria-label=&quot;code classlanguage text정규화codel1  l2 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;정규화&lt;/code&gt;(l1 &amp;#x26; l2)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;l1&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;유클리디안/맨하탄 거리는 &apos;거리&apos;라 값이 1이 넘어갈 수 있기 때문에 가시적인 효과를 위해 0~1 사이의 값을 갖도록 L1 정규화를 수행한 후, 각각의 유클리디안/맨하탄 거리를 수행할 수도 있다.&lt;/li&gt;
&lt;li&gt;함수&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;l1_normalize&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;v&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
     		&lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; v &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;v&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  
		tfidf_norm_l1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; l1_normalize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;tfidf_matrix&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
		d &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; euclidean_distances&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;tfidf_norm_l1&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; tfidf_norm_l1&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;d&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;numpy 패키지&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;L1_norm &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;linalg&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;norm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; axis&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;L1_norm&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;l2&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;l2 + HashingVectorizer 패키지&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;VOCAB_SIZE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;20&lt;/span&gt;
hvectorizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; HashingVectorizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;n_features&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;VOCAB_SIZE&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;norm&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;l2&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;alternate_sign&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
hash_matrix &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; hvectorizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fit_transform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sent&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;toarray&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;hash_matrix&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numpy 패키지&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; np
L2_norm &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;linalg&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;norm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; axis&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt; L2_norm&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;참고: &lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;아마추어 퀀트, blog.naver.com/chunjein&lt;/li&gt;
&lt;li&gt;코드 출처 및 내용 공부: 전창욱, 최태균, 조중현. 2019.02.15. 텐서플로와 머신러닝으로 시작하는 자연어 처리 - 로지스틱 회귀부터 트랜스포머 챗봇까지. 위키북스&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[NLP Doc2Vec]]></title><description><![CDATA[텍스트 분류 Skip-Gram SGNS Hirarchical softmax  연산이 많아진단 단점의 softmax를 개선하여 Binary Tree 사용 Binary Tree iForest 알고리즘, DB indexing…]]></description><link>https://jynee.github.io/tags#1st/NLP한글_2/</link><guid isPermaLink="false">https://jynee.github.io/tags#1st/NLP한글_2/</guid><pubDate>Wed, 05 Aug 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;텍스트-분류&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%B6%84%EB%A5%98&quot; aria-label=&quot;텍스트 분류 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;텍스트 분류&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Skip-Gram&lt;/li&gt;
&lt;li&gt;SGNS&lt;/li&gt;
&lt;li&gt;Hirarchical softmax&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&quot;code-classlanguage-texthirarchical-softmaxcode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-texthirarchical-softmaxcode&quot; aria-label=&quot;code classlanguage texthirarchical softmaxcode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;Hirarchical softmax&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;연산이 많아진단 단점의 softmax를 개선하여 Binary Tree 사용&lt;br&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Binary Tree&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;iForest 알고리즘, DB indexing&lt;/li&gt;
&lt;li&gt;각 트리마다 나눠서 연산을 함 &lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;순서&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;vocab 생성 단계에서 단어들을 sort&lt;/li&gt;
&lt;li&gt;출력층을 binary (huffman) tree&lt;/li&gt;
&lt;li&gt;y와 yHat 사이의 차이를 줄이는 방향으로 역전파 &lt;/li&gt;
&lt;li&gt;기존 softmax는 yHat(확률분포 형태)를 구하는 부분에서 계산량이 많다. 이걸 binary tree를 사용하는 것.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;그렇게 되면 기존 yHat은 0.01, 0.03 ... 등으로 출력됐는데, 0.6, 0.42 등으로 출력되어 기존의 방법보다 계산량이 작아져 속도도 빨라진다.&lt;/p&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;word2vec&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;의미 공간이라는 고차원의 공간에 각 단어의 좌표값(벡터)을 부여한다&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;기준점이 달라지면 숫자도 달라지는 상대적인 거리로 단어 간의 유사도/차이를 분석할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h1 id=&quot;code-classlanguage-textpv-dmcode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textpv-dmcode&quot; aria-label=&quot;code classlanguage textpv dmcode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;PV-DM&lt;/code&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;paragraph vector&lt;br&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;doc2vec&lt;/code&gt;에 사용됨&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;distributed word representation(분포확률/가설 이론 사용)&lt;/li&gt;
&lt;li&gt;한 문장에 같이 쓰인 단어들끼리의 유사성이 높을 것이란 가설 이론&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;word vector 가 아닌 paragraph 단위의 vector 표현&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paragraph : 문장, 문서 단위&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;word2vec → (발전 시킴) Doc2vec&lt;/li&gt;
&lt;li&gt;USL&lt;/li&gt;
&lt;li&gt;유사도 계산에 순서 고려까지 하는 알고리즘 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/fc07fc23018e5c3f755208039dfa5ea2/58213/image-20200805181033974.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 40.54054054054054%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsSAAALEgHS3X78AAABU0lEQVQoz4WSW0/CQBCF+/9/ic8+GIgx0QcTiZeIXFQQsEXkUi6FlnbbvXxuFzD4gpNMZjN7ZnbOnPWMMZxySgd3FmqfY5c7WJpq5+Wdxz8W29rRVtPbSM4GGaPMMMkNzUiy3hYO0+/l3NcyhNB4ShqyTDOZSJJEMw+Vi1JY8Pgb+ekzSyUzixnEivUqZR2ltHxB7WGNLAzRSjEMCpRl4IUzRbeTU8bJWHJ9ldB81YR37+D3MdMQuUrodBRBM4LLCqvzGx5uE1qNwtZJR/drKMlzO2E5zWKhXEOtDW9twTKyPOMIMx6TBjOmfsJHT9LrCgJLu93IqFa2lmZK4BeMviSbjXZMvePFmr0ApRCHtRv7SBzvwEIY51JBFCmH39qBPro5y6VyeO9XzUODI1WP839tly8ZvdQF1YuEp8fMNfZOfpdT38laYdWeh9I2Tak/C6fBD1kSaS7IbENYAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200805181033974&quot;
        title=&quot;image-20200805181033974&quot;
        src=&quot;/static/fc07fc23018e5c3f755208039dfa5ea2/fcda8/image-20200805181033974.png&quot;
        srcset=&quot;/static/fc07fc23018e5c3f755208039dfa5ea2/12f09/image-20200805181033974.png 148w,
/static/fc07fc23018e5c3f755208039dfa5ea2/e4a3f/image-20200805181033974.png 295w,
/static/fc07fc23018e5c3f755208039dfa5ea2/fcda8/image-20200805181033974.png 590w,
/static/fc07fc23018e5c3f755208039dfa5ea2/efc66/image-20200805181033974.png 885w,
/static/fc07fc23018e5c3f755208039dfa5ea2/58213/image-20200805181033974.png 902w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;그림 출처: 아마추어 퀀트, blog.naver.com/chunjein&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;concat 후 lstm 넣으면 정보가 조금은 유실된 채로 넣어지는 것이라 우리는 지금까지 embeding layer 거친 것을 바로 lstm으로 넣었음.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;원리: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;문장 -1 &quot;doc#1&quot; : the cat sat on the table 가 있을 때,&lt;/li&gt;
&lt;li&gt;문장마다 unique한 ID(Paragraph id)를 붙여놓고 이것도 하나의 문장으로 취급함.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;paragraph vector&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이것들이 추후엔 word가 됨&lt;/li&gt;
&lt;li&gt;즉, paragraph token은 또 다른 word라 생각하면 됨 &lt;/li&gt;
&lt;li&gt;모든 문장의 paragraph vector를 공유함 &lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;따라서&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;concat으로 합쳐져서 y 값이 나오도록 역전파&lt;/li&gt;
&lt;li&gt;x의 fix value는 3, y는 1라면, window size = 4로 설정&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;context&lt;/th&gt;
&lt;th&gt;x(vocab의 index로 입력)&lt;/th&gt;
&lt;th&gt;y(vocab의 index로 출력)&lt;/th&gt;
&lt;th&gt;missing value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;doc - # 1(7)&lt;/td&gt;
&lt;td&gt;the(1), cat(3), sat(4)&lt;/td&gt;
&lt;td&gt;on(13)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;doc - # 1(7)&lt;/td&gt;
&lt;td&gt;cat(3), sat(4), on(13)&lt;/td&gt;
&lt;td&gt;table(8)&lt;/td&gt;
&lt;td&gt;the(1)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;이렇게 학습 시킨다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;*&apos;이렇게 학습&apos; : Distributed memory model of paragraph vector(PV-DM): missing된 값을 기억한다&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;학습 후에는 paragraph vector가 1개의 colum vector(word vector)가 됨&lt;br&gt;&lt;/li&gt;
&lt;li&gt;단점: 학습 단계는 괜찮은데, prediction 단계에서는 새 문장에 paragraph id를 알 수 없어 값을 넣지 못한다는 점&lt;br&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;해결: inference step(추론 단계) 필요&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;학습에 사용되지 않은 문장에 대해서는 inference step이 필요하다.&lt;/li&gt;
&lt;li&gt;GD 방법에 의해 얻을 수 있다.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;따라서 알고리즘 순서: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2개의 주 Stage가 있다.&lt;/li&gt;
&lt;li&gt;W, D, U 결정(학습단계) 및 고정&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;inferece stage&lt;/strong&gt;가 필요&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GD(학습) 통해 알아냄. &lt;/li&gt;
&lt;li&gt;위에서 학습 완료된 W(embedding layer 거친 word vector), U(softmax 가중치), b(softmax의 bias) 고정 시킴&lt;/li&gt;
&lt;li&gt;일반적으론 model.fit(x, y)로 학습 한 후, model.predict(x)로 예측했는데 &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;inference 때에는 D(paragraph Matrix) 값을 모르니 D를 random하게 둔다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;따라서 model.fit(x, y)로 학습 한 후, model.predict(new x) 수행&lt;/li&gt;
&lt;li&gt;advantage paragraph vector &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;PV-DM에서는 다음과 같은 크게 두 가지 단계를 거쳐서 문장의 발화 의 도를 예측한다. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;주어진 데이터로 W(word vectors), U(softmax weights), b(softmax weights), D(paragraph vectors)를 학습한다. &lt;/li&gt;
&lt;li&gt;새로운 문장에 대해서 이미 학습했던 W, U, b는 유지하면서 D에 column을 추가하고 기울기 하강기법(gradient descent)을 적용하여 새로운 문장에 대한 벡터(D’)을 만들게 된다.
두 번째 단계에서 얻은 D’은 인식기를 통해 미리 정의한 발화 의도 집합 중 하나로 분류된다&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;출처: 최성호, 김은솔, 장병탁. 2016. Paragraph Vector를 이용한 문장 의도 예측 기법. 2016년 한국컴퓨터종합학술대회 논문집&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h1 id=&quot;code-classlanguage-textpv-dbowcode&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#code-classlanguage-textpv-dbowcode&quot; aria-label=&quot;code classlanguage textpv dbowcode permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code class=&quot;language-text&quot;&gt;PV-DBOW&lt;/code&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;PV-DM과 반대 &lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&quot;cnn--lstm&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#cnn--lstm&quot; aria-label=&quot;cnn  lstm permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CNN &amp;#x26; LSTM&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/dbea49c77f57e710c7c2132d49463002/73fd0/image-20200805175130043.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 141.2162162162162%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAYAAABh2p9gAAAACXBIWXMAAAsSAAALEgHS3X78AAAD30lEQVRIx41ViW7bVhDk//9G/qAfUARBERQoYjuOncSyJMqSJVEU7/vezCz5ZKp1gBJYUTzevNnZ2aUl09H3A6KXYRDEoGGO+bX5P4/5YTUAaRHT0qszNynL7gr4vWMObt0FqdwEiXj7RGw7kMMhliSuxPdz8c6ZfL11ZLMJxPNyqaoWzxPZvkTyuovl9TWWuuqkbTvNrut6sfK6lbRqJM9qiaNSiqJRQHsdSJbWctjH8vOHK0VeK/O/P2/l459r+evTC2Ir61UgSVJJ23XSNJ1YPVAZnca4E48wGNktngIwCmVjh5JiIVnznbOb6f0ahLgR75GpRRCC8YI7dDifTrkEQa4sH7458vTTk+VzKLc3R9ltI/G9Qs5nStBIHJcqB8+actO0CtRgJ+5GYHsdaqqUwPcypEQpal2YQZrNJpS7m738+O5qPD6c9MxnlqY6CWrEZbUIvnz2xUVqZDK3TRSWkOIsD/cncU+ZAqVpNaWMnwFBgBpVNEzJ/HhMNLUITMkyDAplSplaZDUMvcZFNhZln1fywXZU4AOsQ1sQOASLqmzk5KSyWvpazS//7JQZdSzL0QG7bah2urKNnWBnLC4RZGd2ZCWp52oZgGUB8Fhy2McAeNDXPSUXuTTlHj/Sz6rcdZeH1IbpsoJM+f6ro9dMk5IQ1NjlAtgCvWlHTUbA/hLGk2Tyuos0g6aeCifDu31tDVgo02CYg3E3sqVm3x9Pyo4ScFGGir6gHZfPnmq7ePLg22K0jZ0W8sWLxUc1z+gCtl6eNwrK7iAgDU1QVplZsNLPC0/ubo+y18LE0DdD/xdi3WMw/LF1FdBBRbkLBwOLsMMA2GMAsFdpoQVAvt07F6NT27H1ZLQPsrSGXgfgGDhqaESANC3Ru+gOMkeqNDOra1qMTEe/shhvUlnNVAxjZjJkhGGh/cz2Y1psOwJT6xjTaLdNwD66dIjBsUw1zQ68GU+dMXqyB7tCwch2LNabC+aFVIZqULz4sokgdIBhAC2PKeZfI8dDqkOUXTOKnmthzMZjut3Vf4s/JSpLANMF9BvTXjz5Okm42Xo1zsQtWo0b0ia2HSl7Xtt4RodcUiYAJzUnM20zWqQdJziKRE+yS1bw3WGfalYe5uIWluFmnAOqIRnRIvxOkJHjJGrWxwcXxg10hDHY0xxVfIfvMk64PrscxJFuTM0t/iSwQuAXqlMYlEghUXuQJUGYVjR9b8iCJKpyZP/2hZzZhgtoE/2iYTTRzATkfV6zgwIUhOOs/1dV5wWZvinX03r+wTHDgf37nkXeC8sgG3PPR5G53zbdf+7/Lq4AWdX/u/B38Qv7PW53wn67wAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;image-20200805175130043&quot;
        title=&quot;image-20200805175130043&quot;
        src=&quot;/static/dbea49c77f57e710c7c2132d49463002/fcda8/image-20200805175130043.png&quot;
        srcset=&quot;/static/dbea49c77f57e710c7c2132d49463002/12f09/image-20200805175130043.png 148w,
/static/dbea49c77f57e710c7c2132d49463002/e4a3f/image-20200805175130043.png 295w,
/static/dbea49c77f57e710c7c2132d49463002/fcda8/image-20200805175130043.png 590w,
/static/dbea49c77f57e710c7c2132d49463002/73fd0/image-20200805175130043.png 793w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;그림 출처: 아마추어 퀀트, blog.naver.com/chunjein&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;Word2vec&lt;/code&gt; : LSTM/CNN&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위 그림 中 1번 그림이다.&lt;/li&gt;
&lt;li&gt;1개 문장 안에서 단어들의 sequence 분석한다.&lt;/li&gt;
&lt;li&gt;즉, 순서/흐름에 주목한 모델이다.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;부족한 부분은 padding 처리하며 time step을 통일 시켜주었다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;근데 &quot;time step의 크기를 굳이 통일시킬 필요가 있나?&quot; &quot;어떤 batch는 부족하면 안 되나? &quot;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;부족해도 된다. 경우에 따라 padding을 써주는데 쓰는 게 일반적이다. &lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;Doc2vec&lt;/code&gt;: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위 그림 中 2번 그림일 때, batch가 문장 1개라 sequence 분석이 불가능&lt;/li&gt;
&lt;li&gt;기계가 입력값을 기억하지 못한다.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;인공지능 로봇으로 따지면, &lt;/p&gt;
&lt;p&gt;나: &quot;지니야 내가 방금 뭐라고 그랬지?&quot;&lt;/p&gt;
&lt;p&gt;기계: &quot;무슨 말씀인지 모르겠어요.&quot;&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;따라서 3번 그림. &lt;code class=&quot;language-text&quot;&gt;Episodic stroy&lt;/code&gt;개념 사용하여 episode가 batch 역할을 한다. &lt;/li&gt;
&lt;li&gt;기계가 입력값을 기억할 수 있다. &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;인공지능 로봇으로 따지면, &lt;/p&gt;
&lt;p&gt;나: &quot;지니야 내가 방금 뭐라고 그랬지?&quot;&lt;/p&gt;
&lt;p&gt;기계: &quot;오늘 날씨가 어떠냐고 물어봤죠?&quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;현재 개발이 되고는 있지만 쉽지 않은 영역이라 발전은 더딘 편이다.&lt;/li&gt;
&lt;li&gt;만약 개발이 된다면, A 작가의 40년치 소설을 모델에 넣어 학습 시켜 가지고 그 작가의 문체적 특징을 파악할 수 있지 않을까&lt;em&gt;?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;원리: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;1개 episodic내에서 문장(문단)의 sequence 분석 시행&lt;/li&gt;
&lt;li&gt;LSTM layer에 입력&lt;/li&gt;
&lt;li&gt;chatbot의 경우: 대화 내용의 흐름이 존재. 이걸 학습하기 위해 (내가 챗봇과 대화하는 하나의 세션(대화주제) 개념으로의)episodic story 활용 &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이때, time step을 굳이 지정해서 길이를 통일시키지 않아도 된다. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;x.shape = (None, 문장 개수, vector_size)&lt;/p&gt;
&lt;p&gt;xInput = Input(batch&lt;em&gt;shape = (None, &lt;strong&gt;None&lt;/strong&gt;, vector&lt;/em&gt;size))&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;다만, 입력된 문장 개수에 따라 recurrent 횟수가 가변적이다.&lt;/li&gt;
&lt;li&gt;추후 학습 시: model.fit(x,y, batch_size=1) 필요.&lt;/li&gt;
&lt;li&gt;단점: 학습 시간이 오래 걸린다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;참고: &lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;아마추어 퀀트, blog.naver.com/chunjein&lt;/li&gt;
&lt;li&gt;코드 출처: 전창욱, 최태균, 조중현. 2019.02.15. 텐서플로와 머신러닝으로 시작하는 자연어 처리 - 로지스틱 회귀부터 트랜스포머 챗봇까지. 위키북스&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item></channel></rss>